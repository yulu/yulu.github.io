<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LittleCheeseCake</title>
    <description>I am migrating from a blog service provider to github, still trying out with Jekyll
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 26 Aug 2023 15:21:04 +0800</pubDate>
    <lastBuildDate>Sat, 26 Aug 2023 15:21:04 +0800</lastBuildDate>
    <generator>Jekyll v4.2.2</generator>
    
      <item>
        <title>[Kafka Guide] Chapter 3 - Kafka Producer</title>
        <description>&lt;h3 id=&quot;kafka-producer-components-illustrated&quot;&gt;Kafka Producer Components Illustrated&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-producer-components.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;basic-setup&quot;&gt;Basic Setup&lt;/h3&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nc&quot;&gt;Properties&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kafkaProps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Properties&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= start with a property object&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kafkaProps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bootstrap.servers&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;broker1:9092,broker2:9092&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= server uri&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;kafkaProps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;key.serializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;org.apache.kafka.common.serialization.StringSerializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= use default serializer&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kafkaProps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value.serializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;org.apache.kafka.common.serialization.StringSerializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;producer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;KafkaProducer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kafkaProps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;sending-messages&quot;&gt;Sending Messages&lt;/h3&gt;

&lt;p&gt;Three primary methods of sending messages&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;fire-and-forget: Send and do not wait for response&lt;/li&gt;
  &lt;li&gt;synchronous send: use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Future&lt;/code&gt; object and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get()&lt;/code&gt; to wait on the Future and see if the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;send()&lt;/code&gt; was successful or not before sending the next record&lt;/li&gt;
  &lt;li&gt;asynchronous send: A producer object can be used by multiple threads to send messages. Product object is thread-safe&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;the-simplest-way-fire-and-forget&quot;&gt;The Simplest Way (fire-and-forget)&lt;/h5&gt;
&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 
	&lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;CustomerCountry&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Precision Products&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;France&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= there&apos;re different constructors, here we use a simple one requires: topic name, key and value&lt;/span&gt;
	
&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// this returns a future object with RecordMetadata, but we ignore it&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Exception&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;printStackTrace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// exceptions before msg is sent can be captured e.g. SerializationException, BufferExhaustedException and InterruptException&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;synchronous-send&quot;&gt;Synchronous Send&lt;/h5&gt;
&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;CustomerCountry&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Precision Products&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;France&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; 
	
&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= use future.get() to wait for the reply&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Exception&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;printStackTrace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= if there&apos;s any errors before or while sending to Kafka&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;asynchronous-send-with-a-callback&quot;&gt;Asynchronous Send with a Callback&lt;/h5&gt;
&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DemoProducerCallback&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Callback&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= need to implements org.apacke.kafka.clients.producer.callback&lt;/span&gt;
	&lt;span class=&quot;nd&quot;&gt;@Override&lt;/span&gt;
	&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;onCompletion&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;RecordMetadata&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;recordMetadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Exception&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;printStackTrace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= if kafka returns an error, onCompletion will have a nonnull exception&lt;/span&gt;
		&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;CustomerCountry&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Precision Products&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;France&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DemoProducerCallback&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;());&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= we pass a Callback object along when sending the record, callback is executed in the main thread, so it should be reasonably fast&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;configurations&quot;&gt;Configurations&lt;/h3&gt;

&lt;h5 id=&quot;core-configurations&quot;&gt;Core Configurations&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;client.id&lt;/code&gt;: logical identifier for the client and the application it is used in.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;acks&lt;/code&gt;: controls how many partition replicas must receive the record before the producer can consider the write successful.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;You will see that with lower and less reliable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;acks&lt;/code&gt; configuration, the producer will be able to send records faster. This means that you trade off reliability for producer latency. However, end-to-end latency is measured from the time a record was produced until it is available for consumers to read and is identical for all three options. The reason is that, in order to maintain consistency, Kafka will not allow consumers to read records until they are written to all in sync replicas. **Therefore, if you care about end-to-end latency, rather than just the producer latency, there is no trade-off to make: you will get the same end-to-end latency if you choose the most reliable option.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h5 id=&quot;message-delivery-time&quot;&gt;Message Delivery Time&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-producer-delivery-time.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max.block.ms&lt;/code&gt;: how long the producer may block when calling &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;send()&lt;/code&gt; and when explicitly requesting metadata via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;partitionsFor()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;delivery.timeout.ms&lt;/code&gt;: limit the amount of time spent from the point a record is ready for sending until either the broker responds or the client gives up, including time spent on retries, &lt;strong&gt;use this one and leave the retry settings as default&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;request.timeout.ms&lt;/code&gt;: controls how long the producer will wait for a reply from the server when sending data&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;retries&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;retry.backoff.ms&lt;/code&gt;: &lt;strong&gt;not recommended to use&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linger.ms&lt;/code&gt;: controls the amount of time to wait for additional messages before sending the current batch.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h5 id=&quot;others&quot;&gt;Others&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;buffer.memory&lt;/code&gt;: sets the amount of memory the producer will use to buffer messages waiting to be sent to brokers.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compression.type&lt;/code&gt;: e.g. snappy, gzip, lz4, zstd&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch.size&lt;/code&gt;: controls the amount of memory in bytes that will be used for each batch (when multiple records are sent to the same partition, the producer will batch them together)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max.in.flight.requests.per.connection&lt;/code&gt;: controls how many message batches the producer will send to the server without receiving response&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max.request.size&lt;/code&gt;: controls the size of a produce request sent by the producer&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;receive.buffer.bytes&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;send.buffer.bytes&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;enable.idempotence&lt;/code&gt;: when idempotent producer is enabled, the producer will attach a sequence number to each record it sends. If the broker receives records with the same sequence number, it will reject the second copy and the producer will receive the harmless &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DuplicateSequenceException&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;serializers&quot;&gt;Serializers&lt;/h3&gt;
&lt;p&gt;Data serialization strategies detail can be ref to &lt;a href=&quot;/blog1/2022/09/18/ddia-4.html&quot;&gt;DDIA-Chapter4 encoding (serialisation mechanism)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Schema Regsitry&lt;/strong&gt; Pattern is used: the idea is to store all the schemas used to write data to Kafka in the registry. Then we simply store the identifier for the schema in the record we produce to Kafka. The consumers can then use the identifier to pull the record out of the Schema Registry and deserialize the data. The key is that all this work is done in the serializers and deserializers:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-chapter2-1.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is an example of how to produce generated Avro objects to Kafka&lt;/p&gt;
&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nc&quot;&gt;Properties&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;props&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Properties&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;boostrap.servers&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;localhost:9092&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;key.serializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;io.confluent.kafka.serializers.KafkaAvroSerializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= use kafkaAvroSerializer to serialize our objects with Avro&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value.serializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;io.confluent.kafka.serializers.kafkaAvroSerializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;schema.registry.url&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;schemaUrl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;topic&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;customerContacts&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;nc&quot;&gt;Producer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Customer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;producer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;KafkaProducer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// We keep producing new events until someone ctrl-c&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;nc&quot;&gt;Customer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;customer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;CustomerGenerator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getNext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
	&lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Generated customer &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;customer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;());&lt;/span&gt;
	&lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Customer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;topic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;customer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;customer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;partitioners&quot;&gt;Partitioners&lt;/h3&gt;

&lt;p&gt;The importance of keys: all messages with the same key will go to the same partition (for the same topic).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;When the key is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;null&lt;/code&gt; and the default partitioner is used, the record will be sent to one of the available partitions of the topic at random. A round-robin algorithm will be used to balance the messages among the partitions.&lt;/li&gt;
  &lt;li&gt;If a key exists and the default partitioner is used, Kafka will hash the key and use the result to map the message to a specific partition.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RoundRobinPartitioner&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UniformStickyPartitioner&lt;/code&gt; can be used to replace the default partitioner.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can implement custom partitioning strategy, e.g. code example below&lt;/p&gt;
&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;BananaPartitioner&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Partitioner&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;configure&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;configs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
	
	&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;partition&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;topic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Object&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;byte&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyBytes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
						 &lt;span class=&quot;nc&quot;&gt;Object&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;byte&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valueBytes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
						 &lt;span class=&quot;nc&quot;&gt;Cluster&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;nc&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;PartitionInfo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partitions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;partitionsForTopic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;topic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numPartitions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partitions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;

		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keyBytes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(!(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;instanceOf&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)))&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;InvalidRecordException&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;We expect all messages to have customer name as key&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
		
		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(((&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;equals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Banana&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numPartitions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// Banana will always go to last partition&lt;/span&gt;

		&lt;span class=&quot;c1&quot;&gt;// Other records will get hashed to the rest of the partitions&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;murmur2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keyBytes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numPartitions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

	&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;headers-interceptors-quotas-and-throttling&quot;&gt;Headers, Interceptors, Quotas and Throttling&lt;/h3&gt;

&lt;h5 id=&quot;headers&quot;&gt;Headers&lt;/h5&gt;

&lt;p&gt;Additional metadata information about Kafka record, e.g.indicate the source of data, information for routing or tracing)&lt;/p&gt;

&lt;h5 id=&quot;interceptors&quot;&gt;Interceptors&lt;/h5&gt;

&lt;p&gt;Capturing monitoring and tracing information, enhancing the message with standard headers, redacting sensitive information. Example code snippet&lt;/p&gt;
&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;CountingProducerInterceptor&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ProducerInterceptor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;nc&quot;&gt;ScheduledExecutorService&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;executorService&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 
		&lt;span class=&quot;nc&quot;&gt;Executors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;newSingleThreadScheduledExecutor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
	&lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;AtomicLong&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numSent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;AtomicLong&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;AtomicLong&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numAcked&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;AtomicLong&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

	&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;configure&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;nc&quot;&gt;Long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;windowSize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Long&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;valueOf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
			&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;counting.interceptor.window.size.ms&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;executorService&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;scheduleAtFixedRate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;CountingProducerInterceptor:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;windowSize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;windowSize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TimeUnit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;MILLISECONDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

	&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;onSend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;producerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;numSent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;incrementAndGet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;producerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

	&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;onAcknowledgement&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;RecordMetadata&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;recordMetadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Exception&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;numAcked&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;incrementAndGet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

	&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;executorService&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;shutdownNow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

	&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;// just print out the sent and ack counts in a separate thread&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;// reset the counts in each time window&lt;/span&gt;
		&lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numSent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getAndSet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
		&lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numAcked&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getAndSet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Producer interceptors can be applied without any changes to the client code (need to have deployment config changes). To use the preceding interceptor:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Add your jar to the classpath
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export CLASSPATH=$CLASSPATH:~./target/CountProducerInterceptor-1.0-SNAHPSHOT.jar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Create a config file (producer.config) that includes:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;interceptor.classes=com.shapira.examples.interceptors.CountProducerInterceptor counting.interceptor.window.size.ms=100000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Run the application as you normally would but make sure include the configuration that you created
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bin/kafka-console-producer.sh --broker-list localhost:9092 --topic interceptor-test --producer.config producer.config
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;quotas-and-throttling&quot;&gt;Quotas and Throttling&lt;/h5&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;quota.producer.default=2M
quota.producer.override=&quot;clientA:4M,clientB:10M&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;summary-as-ankicard&quot;&gt;Summary as Ankicard&lt;/h3&gt;

&lt;p&gt;💡 What are the three ways to send message from producer to Kafka broker?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;send-and-forget: producer.send(record);&lt;/li&gt;
  &lt;li&gt;synchronous send: producer.send(record).get(); // &amp;lt;= use future.get() to wait for the reply&lt;/li&gt;
  &lt;li&gt;asynchronous send: producer.send(record, new DemoProducerCallback());&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 Can a &lt;strong&gt;producer&lt;/strong&gt; object be used by multiple threads to send messages in Kafka producer application?&lt;/p&gt;

&lt;p&gt;Yes, product object is thread-safe&lt;/p&gt;

&lt;p&gt;💡 What is the recommended Kafka producer timeout and retry configuration, and why?&lt;/p&gt;

&lt;p&gt;Configure the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;delivery.timeout.ms&lt;/code&gt; and leave the retries config as default. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;delivery.timeout.ms&lt;/code&gt;` includes the time when record is ready to be sent to the response is received from the broker, including the retries. In this case we limit the total preparation + in-flight time and let the producer retries as many times as possible within the limited timeout constraint.&lt;/p&gt;

&lt;p&gt;💡 What are the five steps in the send() process in the Kafka producer?&lt;/p&gt;

&lt;p&gt;send() → batching → await send → retries → inflight&lt;/p&gt;

&lt;p&gt;💡 Which component in the Kafka producer is responsible for the serialization and how to config?&lt;/p&gt;

&lt;p&gt;Serializer. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;props.put(&quot;value.serializer&quot;, &quot;io.confluent.kafka.serializers.kafkaAvroSerializer&quot;);&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;💡 What are the commonly used serialization strategies in Kafka producer?&lt;/p&gt;

&lt;p&gt;Avro, protobuff, json&lt;/p&gt;

&lt;p&gt;💡 The key in the message is used to select the partition. If a key is null, what strategy will be used to select the partition?&lt;/p&gt;

&lt;p&gt;A round-robin strategy will be used to select the partition&lt;/p&gt;

&lt;p&gt;💡 What some ready-to-use partitioner to replace the default partitioner?&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RoundRobinPartitioner&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UniformStickyPartitioner&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;💡 How can you write your own partitioner?&lt;/p&gt;

&lt;p&gt;Implement the ‘Partitioner’ interface&lt;/p&gt;

&lt;p&gt;💡 How can we add more metadata information in the message in Kafka producer?&lt;/p&gt;

&lt;p&gt;Use header&lt;/p&gt;

&lt;p&gt;💡 How can we limit the Kafka producer quota?&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;quota.producer.default=2M
quota.producer.override=&quot;clientA:4M,clientB:10M&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Tue, 01 Aug 2023 03:00:00 +0800</pubDate>
        <link>http://localhost:4000/blog1/2023/08/01/kafka-notes-chapter3.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog1/2023/08/01/kafka-notes-chapter3.html</guid>
        
        <category>reading_notes</category>
        
        <category>Kafka</category>
        
        
        <category>blog1</category>
        
      </item>
    
      <item>
        <title>[Kafka Guide] Chapter 2 - Install Kafka</title>
        <description>&lt;h3 id=&quot;hands-on&quot;&gt;Hands-on&lt;/h3&gt;

&lt;h5 id=&quot;running-on-aws-ec2-ubuntu-t2micro&quot;&gt;Running on AWS EC2 (ubuntu, t2.micro)&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;ssh to server
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; yulu-mac.pem ubuntu@xx.xx.xx.xx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;install java 11
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get update
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;openjdk-11-jre-headless
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;install zookeeper
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://dlcdn.apache.org/zookeeper/zookeeper-3.5.10/apache-zookeeper-3.5.10-bin.tar.gz
tar -zxf apache-zookeeper-3.5.10-bin.tar.gz
mv apache-zookeeper-3.5.10-bin /usr/local/zookeeper
mkdir -p /var/lib/zookeeper
cp &amp;gt; /usr/local/zookeeper/conf/zoo.cfg &amp;lt;&amp;lt; EOF
&amp;gt; tickTime=2000
&amp;gt; dataDir=/var/lib/zookeeper
&amp;gt; clientPort=2181
&amp;gt; EOF
export JAVA_HOME=/usr/java/jdk-11.0.10
/usr/local/zookeeper/bin/zkServer.sh start
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;install kafka and config the heap (since t2.micro has only 1G memory)
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://downloads.apache.org/kafka/3.4.0/kafka_2.13-3.4.0.tgz
tar -zxf kafka_2.13-3.4.0.tgz
mv kafka_2.13-3.4.0 /usr/local/kafka
mkdir /tmp/kafka-logs
export JAVA_HOME=/usr/java/jdk-11.0.10
export KAFKA_HEAP_OPTS=&quot;-Xmx256M -Xms128M&quot;
/usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;running-on-mac&quot;&gt;Running on Mac&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;install
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;brew &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;kafka
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;start zookeeper
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/opt/homebrew/bin/zookeeper-server-start /opt/homebrew/etc/zookeeper/zoo.cfg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;start kafka
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/opt/homebrew/bin/kafka-server-start /opt/homebrew/etc/kafka/server.properties
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;test-the-kafka-broker-is-working&quot;&gt;Test the Kafka broker is working&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;send from producer console
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kafka-console-producer &lt;span class=&quot;nt&quot;&gt;--broker-list&lt;/span&gt; localhost:9092 &lt;span class=&quot;nt&quot;&gt;--topic&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; send first message
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; send second message
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; wow it is working
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;consume by the consumer console
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kafka-console-consumer &lt;span class=&quot;nt&quot;&gt;--bootstrap-server&lt;/span&gt; localhost:9092 &lt;span class=&quot;nt&quot;&gt;--topic&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--from-beginning&lt;/span&gt;
send first message
send second message
wow it is working
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;configuring-the-broker&quot;&gt;Configuring the Broker&lt;/h3&gt;

&lt;h5 id=&quot;general-broker-parameters&quot;&gt;General Broker Parameters&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broker.id&lt;/code&gt;: every Kafka broker must have an integer identifier, which is set using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broker.id&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;listener&lt;/code&gt;: A listener is defined as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;protocol&amp;gt;://&amp;lt;hostname&amp;gt;:&amp;lt;port&amp;gt;&lt;/code&gt; e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PLAINTEXT://localhost:9092,SSL://:9091&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zookeeper.connect&lt;/code&gt;: The location of the ZooKeeper used for storing the broker metadata is set using the zookeeper.connect configuration parameter. The format for this parameter is a semicolon-separated list of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hostname:port/path&lt;/code&gt; strings
    &lt;ul&gt;
      &lt;li&gt;hostname: the hostname or IP address of the ZooKeeper server&lt;/li&gt;
      &lt;li&gt;port: the client port number for the server&lt;/li&gt;
      &lt;li&gt;/path: an optional ZooKeeper path to use as a chroot environment for the Kafka cluster. If it is omitted, the root path is used&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Why use a Chroot path?&lt;/p&gt;

  &lt;p&gt;It is generally considered to be good practice to use chroot path for the Kafka cluster. This allows the ZooKeeper ensemble to be shared with other applications, including other Kafka clusters, without a conflict. It is also best to specify multiple ZooKeeper servers in this configuration. This allows the Kafka broker to connect to another member of the ZooKeeper ensemble in the event of server failure&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.dirs&lt;/code&gt;: log segments are stored in the directory specified in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.dir&lt;/code&gt;. For multiple directories, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.dirs&lt;/code&gt; is preferable. Kafka broker will store partitions on them in a “least-used” fashion, with one partition’s log segments stored within the same path.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num.recovery.threads.per.data.dir&lt;/code&gt;: kafka uses a configurable pool of threads for handling log segments. By default, only one thread per log directory is used. The configurable parameter sets the number of threads per directory&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto.create.topics.enable&lt;/code&gt;: the default Kafka configuration specifies that the broker should automatically create a topic under some circumstances. However if you are managing topic creation explicitly, you can set this parameter to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto.leader.rebalance.enable&lt;/code&gt;: in order to ensure a Kafka cluster doesn’t become unbalanced by having all topic leadership on one broker, this config can be specified to ensure leadership is balanced as much as possible.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;delete.topic.enable&lt;/code&gt;: depending on your environment and data retention guidelines, you may wish to lock down a cluster to prevent arbitrary deletions of topics. Disabling topic deletion can be set by setting this flag to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h5 id=&quot;topic-defaults-parameters&quot;&gt;Topic Defaults Parameters&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num.partitions&lt;/code&gt;: how many partitions a new topic is created with.
    &lt;ul&gt;
      &lt;li&gt;keep in mind that the number of partitions for a topic can only be increased, never decreased.&lt;/li&gt;
      &lt;li&gt;many users will have the partition count for a topic be equal to, or a multiple of, the number of brokers in the cluster.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default.replication.factor&lt;/code&gt;: If auto-topic creation is enabled, this configuration sets what the replication factor should be for new topics&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.retention.ms&lt;/code&gt;: the most common configuration for how long Kafka will retain messages is by time. The default is specified in the configuration file using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.retention.hours&lt;/code&gt; parameter. However, there’re two other parameters allowed: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.retention.minutes&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.retention.ms&lt;/code&gt;. All three of these control the same goal - &lt;strong&gt;the amount of time after which messages may be deleted&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.retention.bytes&lt;/code&gt;: another way to expire messages is based on the total number of bytes of messages retained.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.segment.bytes&lt;/code&gt;: once the log segment has reached to the size specified by this parameter, which defaults to 1GB, the log segment is closed and a new one is opened. Once a log segment has been closed, it can be considered for expiration.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.roll.ms&lt;/code&gt;: another way to control when log segments are closed. This parameter specifies the amount of time after which a log segment should be closed.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;min.insync.replicas&lt;/code&gt;: when configuring your cluster for data durability, setting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;min.insync.replicas&lt;/code&gt; to 2 ensures that at least two replicas are caught up and “in sync” with the producer. Refer to &lt;a href=&quot;/blog1/2023/04/10/ddia-5.html&quot;&gt;DDIA-Chapter5 replication&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;message.max.bytes&lt;/code&gt;: this limits the maximum size of a message that can be produced, defaults to 1 MB.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary-as-ankicard&quot;&gt;Summary as Ankicard&lt;/h3&gt;

&lt;p&gt;💡 What are the two simple commands to test a Kafka server?&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kafka-console-producer &lt;span class=&quot;nt&quot;&gt;--broker-list&lt;/span&gt; localhost:9092 &lt;span class=&quot;nt&quot;&gt;--topic&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;test
&lt;/span&gt;kafka-console-consumer &lt;span class=&quot;nt&quot;&gt;--bootstrap-server&lt;/span&gt; localhost:9092 &lt;span class=&quot;nt&quot;&gt;--topic&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--from-beginning&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;💡 What is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broker.id&lt;/code&gt; config?&lt;/p&gt;

&lt;p&gt;Every broker must have a integer identifier&lt;/p&gt;

&lt;p&gt;💡 What is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.dirs&lt;/code&gt; config?&lt;/p&gt;

&lt;p&gt;Where the log segments are stored in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.dir&lt;/code&gt;, for multiple directories, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.dirs&lt;/code&gt; are preferable.&lt;/p&gt;

&lt;p&gt;💡 How will Kafka broker store the partitions in multiple log directories if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.dirs&lt;/code&gt; is configured?&lt;/p&gt;

&lt;p&gt;Kafka will store the partitions in a “least-used” fashion&lt;/p&gt;

&lt;p&gt;💡 What configuration will prevent Kafka to automatically create topic by sending message to a non-existing topic?&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto.create.topics.enable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;💡 Which parameter configs how many partitions a new topic is created with?&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num.partitions&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;💡 Can the number of partitions for a topic be decreased?&lt;/p&gt;

&lt;p&gt;No, the number of partitions for a topic can only be increased&lt;/p&gt;

&lt;p&gt;💡 What is the replication factor configured by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default.replication.factor&lt;/code&gt;?&lt;/p&gt;

&lt;p&gt;It is the number of copies of data across several brokers. It should be greater than 1 to ensure the reliability&lt;/p&gt;

&lt;p&gt;💡 What is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;min.insync.replicas&lt;/code&gt; config and how does it ensure the reliability?&lt;/p&gt;

&lt;p&gt;You should set this to 2 at least to ensure that at least two replicas are caught up and “in sync” with the producer. This enables the semi-sync replication strategy.&lt;/p&gt;

&lt;p&gt;💡 How does &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.retention.ms&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.retention.bytes&lt;/code&gt; works differently as Kafka’s retention policies?&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.retention.ms&lt;/code&gt; is the most common retention policy - for how long the Kafka will retain the message, it indicates the amount of time after which messages may be deleted.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.retention.bytes&lt;/code&gt; indicates once the log segment has reached to the size specified by this parameter (defaults to 1GB), the log segment is closed and a new one is opened. Once a log segment has been closed, it can be considered expiration.&lt;/p&gt;
</description>
        <pubDate>Tue, 01 Aug 2023 03:00:00 +0800</pubDate>
        <link>http://localhost:4000/blog1/2023/08/01/kafka-notes-chapter2.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog1/2023/08/01/kafka-notes-chapter2.html</guid>
        
        <category>reading_notes</category>
        
        <category>Kafka</category>
        
        
        <category>blog1</category>
        
      </item>
    
      <item>
        <title>[Kafka Guide] Chapter 1 - Meet Kafka</title>
        <description>&lt;h3 id=&quot;what-problems-does-kafka-solve&quot;&gt;What Problems does Kafka Solve&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;h5 id=&quot;how-kafka-comes-in-to-save-the-data-transportation-problems-here&quot;&gt;How Kafka comes in to save the data transportation problems here&lt;/h5&gt;

  &lt;p&gt;Apache Kafka was developed as a publish/subscribe messaging system designed to solve the above problem - a distributing streaming platform.
A filesystem or database commit log is designed to provide a durable record of all transactions so that they can be replayed to consistently build the state of the system. Similarly, data within Kafka is stored durably, in order, and can be read deterministically. In addition, the data can be distributed within the system to provide additional protections against failures, as well as significant opportunities for scaling performance.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;direct-connection-inout-data-communications-between-services-is-difficult-to-trace&quot;&gt;Direct-connection in/out data communications between services is difficult to trace&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-chapter1-1.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;use-a-pubsub-pattern&quot;&gt;Use a Pub/Sub pattern&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-chapter1-2.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;multiple-publishsubscribe-systems-to-support-different-biz-use-cases&quot;&gt;Multiple publish/subscribe systems to support different biz use cases&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-chapter1-3.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;summary-as-ankicard&quot;&gt;Summary as Ankicard&lt;/h3&gt;

&lt;p&gt;💡 What problem does Kafka solve?&lt;/p&gt;

&lt;p&gt;Kafka is a pub/sub messaging system what is designed to decouple the business services by introducing the async communication and group the communication channel using topic. A typical Kafka architecture looks like this&lt;/p&gt;

&lt;p&gt;💡 What are the main 3 characteristics Kafka has?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Data within Kafka is stored durably&lt;/li&gt;
  &lt;li&gt;Data can be read deterministically (in-order)&lt;/li&gt;
  &lt;li&gt;Data can be distributed for reliability and scalability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 What is the use of the ‘key’ in Kafka?&lt;/p&gt;

&lt;p&gt;Key is an optional metadata comes with the message, it will be hashed and used to decide which partition the message goes. Kafka brokers make sure that message comes with the same key goes to the same partition so they can be consumed in order. e.g. for mod-N partition, key is used to generate a consistent hash and used to select the partition number for that message so the same key are always written to the same partition&lt;/p&gt;

&lt;h3 id=&quot;basic-concepts&quot;&gt;Basic Concepts&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;message&lt;/strong&gt;:: the unit of data in Kafka, just an array of bytes as far as Kafka concerned&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;key&lt;/strong&gt;:: an optional piece of metadata of the message. Keys are used when messages are to be written to partitions in a more controlled manner. e.g. for mod-N partition, key is used to generate a consistent hash and used to select the partition number for that message so the same key are always written to the same partition&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;batch&lt;/strong&gt;:: a collection of messages - messages are written into Kafka in batches&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;schemas&lt;/strong&gt;:: JSON, XML or Apache Avro, basically the serialisation protocol&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;topic&lt;/strong&gt;:: analogies for a topic are a database table or a folder in a filesystem - group the same concept data into a channel&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;partitions&lt;/strong&gt;:: topics are broken down into a number of partitions. A partition is a single log. Messages are written to it in an append-only fashion and are read in order from beginning to end. Partitions can be distributed and replicated for scalability and reliability.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-chapter1-4.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;producers&lt;/strong&gt;:: create new messages. By default, the producer will balance messages over all partitions of a topic evenly. In some cases the producer will direct messages to specific partitions, done using the message key.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;consumers&lt;/strong&gt;:: read messages. The consumer read messages in the order in which they were produced to each partition. It keeps track of the offset - a metadata with the message to keep track of which messages it has already consumed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;consumer group&lt;/strong&gt;:: one or more consumers that work together to consume a topic. The group ensures that each partition is only consumed by one member.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-chapter1-6.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;💡 What if there’s larger number of consumers (in a consume group) than the number of partitions?&lt;/p&gt;

  &lt;p&gt;Some of the consumers will be idle. Because one partition cannot be consumed by multiple consumers. 
&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-chapter4-4.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;broker&lt;/strong&gt;:: a single Kafka server. It receives messages from producers, write messages to storage on disk and services consumers. - A single broker can easily handle thousands of partitions and millions of messages per second.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;cluster&lt;/strong&gt;:: a cluster of brokers has a leader broker and followers brokers. All producers must connect to the leader in order to publish messages, but consumers may fetch from either the leader or one of the followers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-chapter1-7.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MirrorMaker&lt;/strong&gt;:: used for replicating data to other clusters in a multi-cluster setting. The replication mechanisms within the Kafka clusters are designed only to work within a single cluster, not between multiple clusters. The MirrorMaker tool helps to fulfil this requirement&lt;/p&gt;

</description>
        <pubDate>Tue, 01 Aug 2023 03:00:00 +0800</pubDate>
        <link>http://localhost:4000/blog1/2023/08/01/kafka-notes-chapter1.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog1/2023/08/01/kafka-notes-chapter1.html</guid>
        
        <category>reading_notes</category>
        
        <category>Kafka</category>
        
        
        <category>blog1</category>
        
      </item>
    
      <item>
        <title>[Kafka Guide] Chapter 4 Kafka Consumer</title>
        <description>&lt;h3 id=&quot;kafka-consumer-concepts&quot;&gt;Kafka Consumer Concepts&lt;/h3&gt;

&lt;h5 id=&quot;consumers-and-consumer-groups&quot;&gt;Consumers and Consumer Groups&lt;/h5&gt;

&lt;p&gt;The main way we scale data consumption from a Kafka topic is by adding more consumers to a consumer group - adding consumers in a consumer groups scales up the process, however adding more consumers (in a group) than the number of partitions does not help since some consumers just become idle - &lt;strong&gt;a partition can only be consumed by one consumer in a group&lt;/strong&gt;. Adding consumer group allows separate processing of the topic for new use cases (independent from the other consumer groups).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-chapter4-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;💡 How should we make use of Kafka consumer and Kafka consumer group?&lt;/p&gt;

&lt;p&gt;You create new consumer group for each application that needs all the messages from one or more topics. You add consumers to an existing consumer group to scale the reading and processing of messages from the topics, so each additional consumer in a group will only get a subset of the messages.&lt;/p&gt;

&lt;h5 id=&quot;consumer-groups-and-partition-rebalance&quot;&gt;Consumer Groups and Partition Rebalance&lt;/h5&gt;

&lt;p&gt;💡 What is Partition Rebalance?&lt;/p&gt;

&lt;p&gt;Moving partition ownership from one consumer to another is called a rebalance&lt;/p&gt;

&lt;p&gt;💡 When will Partition Rebalance happen?&lt;/p&gt;

&lt;p&gt;It happens when new consumers are added or old consumers are removed/crashed or administrator adds new partitions etc.&lt;/p&gt;

&lt;p&gt;💡 What are the two types of Partition Rebalance strategies?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Eager rebalances&lt;/em&gt;: during an eager rebalance, all consumers stop consuming, give up their ownership of all partitions, rejoin the consumer group, and get a brand-new partition assignment&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Cooperative rebalances (incremental rebalances)&lt;/em&gt;: typically involve reassigning only a small subset of the partitions from one consumer to another, and allowing consumers to continue processing records from all the partitions that are not reassigned&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 How does consumer maintain membership in a consumer group and ownership of the assigned partition and make sure it is alive and known to the Kafka broker?&lt;/p&gt;

&lt;p&gt;Consumers maintain membership and ownership by sending &lt;em&gt;heartbeats&lt;/em&gt; to a Kafka broker designated as the &lt;em&gt;group coordinator&lt;/em&gt;. The heartbeats are sent by a background thread of the consumer, and as long as the consumer is sending heartbeats at regular intervals, it is assumed to be alive.&lt;/p&gt;

&lt;p&gt;💡 How does the Process of Assigning Partitions to Consumer Work?&lt;/p&gt;

&lt;p&gt;When a consumer wants to join a group, it sends a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JoinGroup&lt;/code&gt; request to the group coordinator. The first consumer to join the group becomes the group leader. The leader receives a list of all consumers in the group from the group coordinator and is responsible for assigning a subset of partitions to each consumer. It uses an implementation of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PartitionAssignor&lt;/code&gt; to decide which partitions should be handled by which consumer.
Kafka has a few build-in partition assignment polices. After deciding on the partition assignment, the consumer group leader sends the list of assignments to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GroupCoordinator&lt;/code&gt;, which sends this information to all the consumers.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;❓ Questions to be explored&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Why the leader is responsible for the assignment strategy execution, but not the group coordinator?&lt;/li&gt;
    &lt;li&gt;Leader is a client (e.g. java runtime), what is exactly a “group coordinator”?&lt;/li&gt;
    &lt;li&gt;What is the Kafka broker’s architecture? 
See more in Kafka Guide - Chapter 6 Kafka Internals later.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;static-group-membership&quot;&gt;Static Group Membership&lt;/h5&gt;

&lt;p&gt;💡 What is static group membership?&lt;/p&gt;

&lt;p&gt;The consumer is assigned a unique &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;group.instance.id&lt;/code&gt; and it becomes a static member. The same partitions will be assigned to it when it rejoins the group.&lt;/p&gt;

&lt;p&gt;💡 When static group membership is useful?&lt;/p&gt;

&lt;p&gt;Static group membership is useful when your application maintains local state or cache that is populated by the partitions that are assigned to each consumer. When re-creating this cache is time-consuming, you don’t want this process to happen every time a consumer restarts.&lt;/p&gt;

&lt;p&gt;💡 What is the difference of the rebalance strategy for a consumer with static group membership from normal consumer?&lt;/p&gt;

&lt;p&gt;When consumer with static group membership restart, it will not trigger a rebalance. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;session.timeout.ms&lt;/code&gt; is used to detected whether they are really gone and rebalance is required. So we should select a good &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;session.timeout.ms&lt;/code&gt; config to make sure restart of the consumer is within a acceptable delay and the consumer can catch up after restart.&lt;/p&gt;

&lt;h3 id=&quot;creating-a-kafka-consumer-and-subscribing-to-topics&quot;&gt;Creating a Kafka Consumer and Subscribing to Topics&lt;/h3&gt;

&lt;p&gt;All you need is server uri, key and value deserializers&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nc&quot;&gt;Properties&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;props&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Properties&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bootstrap.servers&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;broker1:9092,broder2:9092&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;group.id&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;CountryCounter&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;key.deserializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value.deserializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;nc&quot;&gt;KafkaConsumer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;consumer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;KafkaConsumer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Subscribing to Topics, we can use regular expression here, but need to pay attention to two issues&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the metadata size fetched by the client might be large since client do local matching (signifcant overhead)&lt;/li&gt;
  &lt;li&gt;the client should be grant the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;describe&lt;/code&gt; permission on the entire cluster in order to describe all topics in the cluster (then do local matching)&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;subscribe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Collections&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;singletonList&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;customerCountries&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;subscribe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Pattern&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;test.*&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;the-poll-loop&quot;&gt;The Poll Loop&lt;/h3&gt;

&lt;p&gt;The heart of the Consumer API&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nc&quot;&gt;Duration&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timeout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Duration&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;ofMillis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;nc&quot;&gt;ConsumerRecords&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;records&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;poll&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ConsumerRecords&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;record:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;records&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;topic = %s, partition = %d, offset = %d, &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;customer = %s, country = %s\n&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;topic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;partition&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;());&lt;/span&gt;
		&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;updatedCount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;custCountryMap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;containsKey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;updatedCount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;custCountryMap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
		&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;custCountryMap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;updatedCount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

		&lt;span class=&quot;nc&quot;&gt;JSONObject&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;JSONObject&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;custCountryMap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;());&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;poll()&lt;/code&gt; loop does a lot more than just get data. The first time you call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;poll()&lt;/code&gt; with a new consumer, it is responsible for finding the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GroupCoordinator&lt;/code&gt;, joining the consumer group, and receiving a partition assignment. If a rebalance is triggered, it will be handled inside the poll loop as well, including related callbacks.&lt;/p&gt;

&lt;p&gt;Thread Safety: one consumer per thread is the rule. To run multiple consumers in the same group in one application, you will need to run each in its own thread. To achieve multi-threaded message consumption with the Kafka Consumer, there’re some tricks to be played, see Kafka Guide - Chapter 4.1 Multi-threaded Message Consumption with Kafka Consumer&lt;/p&gt;

&lt;h3 id=&quot;configuring-consumers&quot;&gt;Configuring Consumers&lt;/h3&gt;

&lt;p&gt;// todo&lt;/p&gt;

&lt;h3 id=&quot;commits-and-offsets&quot;&gt;Commits and Offsets&lt;/h3&gt;

&lt;p&gt;One of Kafka’s unique characteristic is that it does not track acknowledgements from consumers the way many JMS queues do. Instead, it allows consumers to use Kafka to track their position (offset) in each partition. We call the action of updating the current position in the partition an &lt;strong&gt;offset commit&lt;/strong&gt;. Unlike traditional message queues, Kafka does not commit records individually. Instead, consumers commit the last message they’ve successfully processed from a partition and implicitly assume that every message before the last was also successfully processed.&lt;/p&gt;

&lt;p&gt;💡 For Kafka consumer, which case there will be duplicated processing?&lt;/p&gt;

&lt;p&gt;During a rebalance, a consumer is assigned a new set of partitions. If the committed offset of the partition is smaller than the offset of the last message the client processed, the messages between the last processed offset and the committed offset will be processed twice&lt;/p&gt;

&lt;p&gt;💡 For Kafka consumer, which case there will be missing data?&lt;/p&gt;

&lt;p&gt;Same condition as above, if the committed offset of the partition is larger than the offset of the last message the client processed, the messages between these two offsets will be missing.&lt;/p&gt;

&lt;h5 id=&quot;automatic-commit&quot;&gt;Automatic Commit&lt;/h5&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;enable.auto.commit=true&lt;/code&gt; - it is critical to always process all the events returned by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;poll()&lt;/code&gt; before calling &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;poll()&lt;/code&gt; again because the next &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;poll()&lt;/code&gt; will commit the last offset returned by the previous &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;poll()&lt;/code&gt; regardless if they are fully processed. This is a convenient approach but it does not give developers enough control to avoid duplicate messages.&lt;/p&gt;

&lt;h5 id=&quot;commit-current-offset&quot;&gt;Commit Current Offset&lt;/h5&gt;

&lt;p&gt;The simplest and most reliable of the commit APIs is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;commitSync()&lt;/code&gt;. This API will commit the latest offset returned by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;poll()&lt;/code&gt; and return once the offset is committed, throwing an exception if hte commit fails for some reason. Noted that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;commitSync()&lt;/code&gt; will commit the latest offset returned by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;poll()&lt;/code&gt;.&lt;/p&gt;

&lt;h5 id=&quot;asynchronous-commit&quot;&gt;Asynchronous Commit&lt;/h5&gt;

&lt;p&gt;Done by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;commitAsync()&lt;/code&gt;: we just send the request and continue without waiting for the response from the broker. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;commitAsync()&lt;/code&gt; gives you an option to pass in a callback that will be triggered when the broker responds. It is common to use the callback to log commit errors or to count them in a metric, &lt;strong&gt;but if you want to use the callback for retries, you need to be aware of the problem with commit order &amp;lt;- use a monotonically increasing sequence number to track in this case&lt;/strong&gt;.&lt;/p&gt;

&lt;h5 id=&quot;combining-synchronous-and-asynchronous-commits&quot;&gt;Combining Synchronous and Asynchronous commits&lt;/h5&gt;

&lt;p&gt;Before close the consumer or rebalance, we want to make extra sure that the commit succeeds. We can use a combination of sync and async commit to do this.&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nc&quot;&gt;Duration&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timeout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Duration&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;ofMillis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;closing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nc&quot;&gt;ConsumerRecords&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;records&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;poll&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ConsumerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;records&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(...);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// just do soemthing with the record&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;commitAsync&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// it is ok to request and forget as next loop will server as a retry&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;commitSync&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// it will block and retry until unrecoverable failure&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Exception&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;unexpected error&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;finally&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;committing-a-specified-offset&quot;&gt;Committing a Specified Offset&lt;/h5&gt;

&lt;p&gt;Give the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;commitAsync()&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;commitSync()&lt;/code&gt; a Map of partition and offset, it can commit specific offsets&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;TopicParitition&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;OffsetAndMetadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;currentOffsets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;HashMap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;gt;();&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;consumer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;commitAsync&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;currentOffsets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Tue, 01 Aug 2023 03:00:00 +0800</pubDate>
        <link>http://localhost:4000/blog1/2023/08/01/kafka-notes-chapter4.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog1/2023/08/01/kafka-notes-chapter4.html</guid>
        
        <category>reading_notes</category>
        
        <category>Kafka</category>
        
        
        <category>blog1</category>
        
      </item>
    
      <item>
        <title>屎壳郎，外卖和马屁精</title>
        <description>&lt;p&gt;早上儿子在沙发看着他最近迷上的蚂蚁动画片，突然大声问我：“妈妈！屎壳郎英文怎么说？”。因为蚂蚁的死对头就是一种黑绿的小甲壳虫，有次瞄到电视里它们在滚黑乎乎的圆球球，我就随口说这是屎壳郎呀，其实自己也是瞎说。结果儿子就记下了。今天不知道他哪里又来了学习英语的热情。我哪知道屎壳郎英文哟，敷衍他不知道不知道。他穷追不舍：“妈妈查呀！拿手机查呀！”这又是他昨晚上新学的小技巧。昨晚和他玩橡皮泥，这小子突然和我飙英语，我们对话着我发现，哎呀，我不会橡皮泥英文呀。我很诚实地给他说：“妈妈不知道橡皮泥英文怎么说，我们来查一下吧。“于是拿出手机搜了一番：plasticine。他和我一起学了发音，还对照着网络图片确认了一下，他表示很满意。今天关于屎壳郎的学习是逃不掉了，打开手机和他学习了一下，忍着恶心看了满屏的屎壳郎的图，和动画片里还是有点像呢，看来我瞎猜得没错。然后这个小人就在沙发上自言自语道：where is bung beetle, where is bung beetle, bung beetle on fower(flower)。&lt;/p&gt;

&lt;p&gt;早饭时间和太石讨论，今天晚上有没有时间做饭，要不要点外买。突然在身后吃着早饭的儿子大叫：我要吃外卖！！我想这小子耳朵这么尖，又这么能耐，还知道外卖好吃要吃外卖。转头看他，他指着我盘子里的早餐继续喊：我要吃外卖，我要吃外卖！哎，那是烧麦好不好！他怎么一直分不清“烧麦”和“外卖”。喊了几嗓子之后，他意识到自己说错了，尴尬地一笑，小声说：呵呵，是烧麦。&lt;/p&gt;

&lt;p&gt;有天儿子和我在床上疯打。他突然咯咯地坏笑，听这个笑声就知道他又做了他认为的“坏事”。他边笑边说：“妈妈，马屁精，你的马屁精，嘿嘿嘿。”我一头雾水，说的啥呀，听不懂。他见我没什么回应，扬起小手，继续道：“妈妈你看，你的马屁精！”我这才发现他学我的样子把我扎头发的皮筋套在了手腕上。我才想起来，我们平时偶尔说他人小鬼大嘴巴甜，是个小马屁精。也不知道他怎么就把“皮筋”和“马屁精”弄混了。之后他一直叫我的皮筋为马屁精，因为觉得他那样太好玩儿了，我一直没有纠正他，要扎头发了就指使他去帮我把“马屁精”找出来，嘿嘿。&lt;/p&gt;

&lt;p&gt;太石去幼儿园接儿子，老师给他说儿子不会用嘴巴吹泡泡，让我们回家教教他。于是太石很积极地给他买了肥皂水和吹泡泡工具。回家教了好久，不停地告诉他要嘟嘟嘴吹气，嘟嘟嘴吹气，他终于开窍学会了。然后开启不停吹不停吹模式，我们都担心他要吹断气了。有天中午午睡，他怎么都不愿意睡下，磨磨蹭蹭。最后我生气了，说你去玩吧！他说他吹一会儿泡泡就睡。我让他自己去客厅了，量他也搞不出什么大动作。十分钟过去了，没有听到什么动静。我轻手轻脚地出去看他，结果看到一个孤独的小背影在阳台上卖力地吹泡泡，偷偷看了好久，他一直吹一直吹，一点没懈怠。吹了好久啊，他过来说困了，然后马上床倒头就睡了。&lt;/p&gt;

&lt;p&gt;带儿子打车，帮他系好安全，他有点不情不愿的。于是给他说，不系安全带可能会飞出去呢！他还有点不相信，我于是拿出手机搜出油管上的车祸视频给他看。后来又一次打车，刚上车坐好，我正给自己系安全，他突然在旁边抓狂起来，着急地说：“妈妈，给我系安全带呀，快呀！”我不慌不忙，叫他不要着急，我先系好了来。结果他小嘴瘪瘪，着急得快哭出来了：“你不给我系安全带我就要飞出去呢！！”。后来还有一次，外公给他说带他回国可以坐外公的车，他一脸严肃地问：“外公，你的车有安全带吗？”&lt;/p&gt;

&lt;p&gt;小孩子的模仿能力，观察力和想象力真的是很强大，看着一个小小人用他强大倔强的力量去认识这个世界，也是奇妙的经历。&lt;/p&gt;
</description>
        <pubDate>Thu, 15 Jun 2023 08:00:00 +0800</pubDate>
        <link>http://localhost:4000/blog2/2023/06/15/growing-up.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog2/2023/06/15/growing-up.html</guid>
        
        <category>生活</category>
        
        
        <category>blog2</category>
        
      </item>
    
      <item>
        <title>[DDIA] Chapter 6 - Partition</title>
        <description>&lt;h3 id=&quot;chapter-summary&quot;&gt;Chapter Summary&lt;/h3&gt;

&lt;p&gt;Database partition or sharding is discussed in this chapter. Partitioning is a very basic concept in data storage engine. As when the data volume goes large, a single node will not be enough to host all the data, we have to distribute the data among multiple nodes. Different approaches for partitioning the primary key and the secondary indexes, rebalancing strategies and request routing strategies are discussed in detail in this chapter.&lt;/p&gt;

&lt;h3 id=&quot;mind-map&quot;&gt;Mind Map&lt;/h3&gt;

&lt;div class=&quot;mindmap-container&quot;&gt;
    &lt;iframe src=&quot;https://www.xmind.net/embed/7dePxB&quot; width=&quot;900px&quot; height=&quot;540px&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&quot;question-summary&quot;&gt;Question Summary&lt;/h3&gt;
&lt;h4 id=&quot;approach-for-partitioning&quot;&gt;Approach for partitioning&lt;/h4&gt;

&lt;p&gt;💡 What is “hot spot” in partitioning?&lt;/p&gt;

&lt;p&gt;A node with disproportionally high load&lt;/p&gt;

&lt;p&gt;💡 What is “skewed” in partitioning?&lt;/p&gt;

&lt;p&gt;Some partitions have more data or queries than others&lt;/p&gt;

&lt;p&gt;💡 What is the benefit and downside of key range partitioning?&lt;/p&gt;

&lt;p&gt;It supports the range queries easily, however some access patterns will result in hot spots&lt;/p&gt;

&lt;p&gt;💡 What is the benefit and downside of hash of key partitioning?&lt;/p&gt;

&lt;p&gt;Hash key of partitioning results more evenly distributed data over the partitions to avoid data skew and hot spot. However it makes the range queries challenging. The range queries needs to sent to all the partitions.&lt;/p&gt;

&lt;p&gt;💡 Describe the approach of partitioning secondary indexes by document&lt;/p&gt;

&lt;p&gt;It is also called local index. The secondary index of a particular partition is stored locally with the partition. When querying for the secondary index, the query needs to go to all the partitions&lt;/p&gt;

&lt;p&gt;💡 Describe the term-based partitioning of database with secondary indexes&lt;/p&gt;

&lt;p&gt;The term-based partitioning refer to the approach that the secondary indexes are stored and partitioned independent from the primary key&lt;/p&gt;

&lt;p&gt;💡 Why the document-based secondary index partitioning of database has expensive read queries?&lt;/p&gt;

&lt;p&gt;Document-based partitioning is also termed as local index. Because the index is local in each partition, so query to the index needs to be sent to and processed by all the partitions. That is why it is more expensive&lt;/p&gt;

&lt;p&gt;💡 What is the advantage and disadvantage of term-based secondary index partitioning?&lt;/p&gt;

&lt;p&gt;Term-based partitioning is more efficient in indexing query, as the index are globally partitioned. However the write become more challenging especially to maintain a synchronous update of the secondary index with write process. The distributed transaction is difficult to achieve in this case&lt;/p&gt;

&lt;h4 id=&quot;rebalancing&quot;&gt;Rebalancing&lt;/h4&gt;
&lt;p&gt;💡  What is “rebalancing”?&lt;/p&gt;

&lt;p&gt;The process of moving load of data from one node in the cluster to another&lt;/p&gt;

&lt;p&gt;💡  What are the minimum requirement of a ‘rebalancing’?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;after the rebalancing, the data should be fairly distributed to the partitions&lt;/li&gt;
  &lt;li&gt;during the rebalancing, the read and write should not be interrupted&lt;/li&gt;
  &lt;li&gt;no more than necessary data should be moved during the rebalancing to make the rebalancing fast and minimize the network and disk I/O load&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;💡  In which cases a ‘rebalancing’ needs to be performed?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;the throughput to the data system increases that we need to add more CPUs to handle the requests&lt;/li&gt;
  &lt;li&gt;the dataset size increases that we need to add more disks or RAMs to host the data&lt;/li&gt;
  &lt;li&gt;some machine failed, that we need to move the data to another available machine&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;💡  What is the problem of mod N partitioning strategy?&lt;/p&gt;

&lt;p&gt;When the number of nodes N changes, most of the data needs to be moved from one node to another. The simple solution is to create many more partitions than the number of nodes, when a new node is added, it steels few partitions from other nodes until the partitions are fairly distributed once again.&lt;/p&gt;

&lt;p&gt;💡  Describe the fixed number of partition strategy&lt;/p&gt;

&lt;p&gt;The number of partitions are chosen at a number that is much larger than the number of nodes. When a new node is added, some selected partitions from each node is moved to the new node. This makes sure that only a small amount of data is moved&lt;/p&gt;

&lt;p&gt;💡  In the fixed number of partition approach, what is the drawback of partition volume is too large or too small?&lt;/p&gt;

&lt;p&gt;Since the number of partition is fixed, with the increase of dataset size, the size of each partition increase. If the size of partition become too large, rebalancing and recovery from a node failure become expensive. If the size is too small (the fixed number is chosen too large with small sized dataset), they incur too much overhead&lt;/p&gt;

&lt;p&gt;💡  What is the caveat of dynamic partitioning?&lt;/p&gt;

&lt;p&gt;An empty database starts off with a single partition and with small amount of data, no partition is performed yet. So all the request are handled by a single node while the other nodes sit idle. To mitigate this issue, we can have an initial set of partitions to be configured on an empty database&lt;/p&gt;

&lt;p&gt;💡  What is the advantage of dynamic partitioning?&lt;/p&gt;

&lt;p&gt;The number of partitions adapts to the total data volume. It avoids the partition size too large or too small issue in the fixed number partitioning strategy&lt;/p&gt;

&lt;h4 id=&quot;request-routing&quot;&gt;Request Routing&lt;/h4&gt;
&lt;p&gt;💡  What are the three common approaches of request routing in partitioned data system?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;contact any node and let the node handle the routes&lt;/li&gt;
  &lt;li&gt;add a routing tier that handles the routing strategy&lt;/li&gt;
  &lt;li&gt;let the client be aware of partitions and connect directly to the appropriate node&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;highlighted-topics&quot;&gt;Highlighted Topics&lt;/h3&gt;

&lt;h4 id=&quot;partition-strategy-comparison&quot;&gt;Partition Strategy Comparison&lt;/h4&gt;

&lt;h5 id=&quot;primary-key-partition-strategy-compare&quot;&gt;Primary key partition strategy compare&lt;/h5&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;pros&lt;/th&gt;
      &lt;th&gt;cons&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;key-range&lt;/td&gt;
      &lt;td&gt;more efficient to support range query&lt;/td&gt;
      &lt;td&gt;can result in hot spot or data skews&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;key-hash&lt;/td&gt;
      &lt;td&gt;evenly distribute the data and queries&lt;/td&gt;
      &lt;td&gt;less efficient in supporting range query&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Compound primary key is supported by Cassandra: only the first part of that key is hashed to determine the partition, but the other columns are used as a concatenated index for sorting the data in Cassandra’s SSTable. A query therefore cannot search for a range of values within the first column of a compound key, but if it specifies a fixed value for the first column, it can perform an efficient range scan over the other columns of the key.&lt;/p&gt;

&lt;p&gt;Similar for DynamoDB, a partition key and an optional sort key are required to generate its primary key. For dynamoDB, there’s the recommended pattern to design sort-key as composite sort keys to contain the hierarchical (one-to-many) relationships in your data. For example, you might structure the sort key as&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[country]#[region]#[state]#[county]#[city]#[neighborhood]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This would let you make efficient range queries for a list of locations at any one of these levels of aggregation, from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;country&lt;/code&gt; to a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;neighborhood&lt;/code&gt; and everything in between. (&lt;a href=&quot;https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-sort-keys.html&quot;&gt;ref&lt;/a&gt;)&lt;/p&gt;

&lt;h5 id=&quot;secondary-index-partition-strategy-compare&quot;&gt;Secondary index partition strategy compare&lt;/h5&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;pros&lt;/th&gt;
      &lt;th&gt;cons&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;document-base (local index)&lt;/td&gt;
      &lt;td&gt;write to the secondary index is in a single partition, easy to maintain the transaction&lt;/td&gt;
      &lt;td&gt;query to the secondary index needs to be sent to all partitions, hence less efficient&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;term-based (global index)&lt;/td&gt;
      &lt;td&gt;query to the secondary index is only from a single partition, more efficient&lt;/td&gt;
      &lt;td&gt;write to the secondary index are across partitions, challenging for transaction management&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Most of the databases follow document-base secondary index. DynamoDB supports both Global Secondary Index (GSI) and Local Secondary Index (LSI). DynamoDB GSI supports eventual consistency as stated. (&lt;a href=&quot;https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general.html&quot;&gt;ref&lt;/a&gt;)&lt;/p&gt;

&lt;h4 id=&quot;request-routing-1&quot;&gt;Request Routing&lt;/h4&gt;

&lt;p&gt;Three different request routing strategies:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;allow clients to contact any node (e.g. via a round-robin load balancer). If that node coincidentally owns the partition to which the request applies, it can handle the request directly, otherwise it forwards the request to the appropriate node, receives the reply and passes the reply to the client&lt;/li&gt;
  &lt;li&gt;Send all requests from clients to a routing tier first, which determines the node that should handle each request and forwards it accordingly. This routing tier does not itself handle any requests; it only acts as a partition-aware load balancer.&lt;/li&gt;
  &lt;li&gt;Require that clients to be aware of the partitioning and the assignment of partitions to nodes. In this case, a client can connect directly to the appropriate node, without any intermediary.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-6/ddia_6_partition_request_routing.excalidraw.png&quot; alt=&quot;ddia_6_partition_request_routing.excalidraw&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The challenge of request routing is the consensus among the distributed system about the changes in the assignment of partitions to nodes. Coordination service such as Zookeeper and gossip protocol among nodes are some commons approaches. Below is an illustration of zookeeper as a coordination service to sync among the routing tier and the nodes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-6/ddia_6_request_routing_zookeeper.excalidraw.png&quot; alt=&quot;ddia_6_request_routing_zookeeper.excalidraw&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is a summary of request routing strategies of popular databases&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;data storages&lt;/th&gt;
      &lt;th&gt;Solution&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;HBase, SolrCloud, Kafka&lt;/td&gt;
      &lt;td&gt;zookeeper as the coordination service&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MongoDB&lt;/td&gt;
      &lt;td&gt;its own config server and monogos daemons as the routing tier&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cassandra and Riak&lt;/td&gt;
      &lt;td&gt;use gossip protocol among the nodes, put the complexity int the nodes, avoid external coordination service dependency&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CouchBase&lt;/td&gt;
      &lt;td&gt;routing tier which learns the routing changes from the cluster nodes, not support auto rebalance&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;extended-topics&quot;&gt;Extended Topics&lt;/h3&gt;

&lt;h4 id=&quot;consistent-hashing---dynamo-style-db-partition-strategy&quot;&gt;Consistent Hashing - Dynamo-style DB partition strategy&lt;/h4&gt;

&lt;p&gt;Consistent hashing is useful for&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sharding: minimal rebalancing of partitions while adding or removing nodes&lt;/li&gt;
  &lt;li&gt;load balancing: better cache performance if same requests go to same servers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Overview&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use a hash function and map its range to a ring&lt;/li&gt;
  &lt;li&gt;Map n nodes to the same ring using the same or another hash function&lt;/li&gt;
  &lt;li&gt;Each physical node is split into k slices through multiple hash functions: Num virtual &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nodes = n*k&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;To write data: move clockwise on the ring from hash of data’s key to the nearest virtual node and write data to the physical node represented by that virtual node&lt;/li&gt;
  &lt;li&gt;To add server: when a server &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;si&lt;/code&gt; is added, the affected range starts from server &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;si&lt;/code&gt; and moves anticlockwise around the ring until a server &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sj&lt;/code&gt; is found. Thus, keys located between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;si&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sj&lt;/code&gt; need to be redistributed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;si&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;To remove node: when a server &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;si&lt;/code&gt; is removed, the affected range starts from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;si&lt;/code&gt; and moves anticlockwise around the ring until a server &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sj&lt;/code&gt; is found. Thus keys located between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sj&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;si&lt;/code&gt; needs to be redistributed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;si+1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-6/ddia_6_partition_consistent_hashing.excalidraw.png&quot; alt=&quot;ddia_6_partition_consistent_hashing.excalidraw&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;further-reading&quot;&gt;Further Reading&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html&quot;&gt;Amazon DynamoDB best practices&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Wed, 12 Apr 2023 10:00:00 +0800</pubDate>
        <link>http://localhost:4000/blog1/2023/04/12/ddia-6.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog1/2023/04/12/ddia-6.html</guid>
        
        <category>reading_notes</category>
        
        <category>system_design</category>
        
        
        <category>blog1</category>
        
      </item>
    
      <item>
        <title>[DDIA] Chapter 5 - Replication</title>
        <description>&lt;h3 id=&quot;chapter-summary&quot;&gt;Chapter Summary&lt;/h3&gt;

&lt;p&gt;This chapter is the start of the three related topics regarding storage systems - replication, partition and transaction. Replication is important because of these important properties it provides: 1) keep the data geographically close to the users 2) allow system to continue to work even if some of its parts have failed (redundancy) 3) allows scaling out to serve the increasing throughput.&lt;/p&gt;

&lt;p&gt;Three common patterns, namely single-leader, multi-leader and leaderless are discussed. In the single-leader pattern, the challenge is mainly the follower set-up and replication lag. For multi-leader and leaderless pattern, the main challenge is the write conflict detection and resolving. This chapter discussed the definition of the patterns, the challenges and various solutions in detail.&lt;/p&gt;

&lt;h3 id=&quot;mind-map&quot;&gt;Mind Map&lt;/h3&gt;

&lt;div class=&quot;mindmap-container&quot;&gt;
    &lt;iframe src=&quot;https://www.xmind.net/embed/itczkd&quot; width=&quot;900px&quot; height=&quot;540px&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&quot;question-summary&quot;&gt;Question Summary&lt;/h3&gt;

&lt;h4 id=&quot;single-leader&quot;&gt;Single-leader&lt;/h4&gt;
&lt;p&gt;💡 What are synchronous replication and asynchronous replication?&lt;/p&gt;

&lt;p&gt;Synchronous replication means that when write happens, the successful response to the client will only be made after copying the data to all the followers.
Asynchronous replication means that the successful response is immediately returned to client when a write is saved to the leader. The copying from leader to followers will happen in background asynchronously.&lt;/p&gt;

&lt;p&gt;💡 What is semi-synchronous replication and why it is good?&lt;/p&gt;

&lt;p&gt;In practice that synchronous replication is impractical because one unavailable node will block all the writes. Semi-synchronous refer to the way that the data write to the leader, is synchronously copied to 1 follower, and then copied to other followers asynchronously. This ensures that at any time, at least two nodes have the up-to-date copy of the data.&lt;/p&gt;

&lt;p&gt;💡 What is the trade-off of async replication?&lt;/p&gt;

&lt;p&gt;Weak durability. In the case that a leader fails before all the data is copied to followers, there will be data lost.&lt;/p&gt;

&lt;p&gt;💡 What are the 4 steps to setup a new follower?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;take a consistent snapshot of the leader’s database at some point in time&lt;/li&gt;
  &lt;li&gt;copy the snapshot to the new follower node&lt;/li&gt;
  &lt;li&gt;the follower connects to the leader to request all the data change since the snapshot was taken&lt;/li&gt;
  &lt;li&gt;caught up the change, which is to process the backlog of data changes since the snapshot&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 What is the log sequence number in PostgreSQL and binlog coordinate in MySQL?&lt;/p&gt;

&lt;p&gt;These two are the same concept, refers to the exact position in the leader’s replication log that the snapshot of the database is associated with.&lt;/p&gt;

&lt;p&gt;💡 What are the 3 steps for an automatic failover process?&lt;/p&gt;

&lt;p&gt;Step1: determine the leader has failed, normally use a ping timeout
Step2: choose a new leader through an election process
Step3: reconfiguring the system to use the new leader&lt;/p&gt;

&lt;p&gt;💡 Why manual operation to handle leader failover is preferred?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;automatically handle unreplicated writes in failed leader is tricky. Simple discarding might cause serious consequences&lt;/li&gt;
  &lt;li&gt;split brain, which means that multiple leaders running at the same time may occur if the failover mechanism is not carefully designed&lt;/li&gt;
  &lt;li&gt;if the timeout is not properly decided, false alarm may occur and could result in an unnecessary failover&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 What is split brain?&lt;/p&gt;

&lt;p&gt;More than one leader is running in a single leader replication architecture. If there’s no process to handle write conflict, it will result in data lost or corruption.&lt;/p&gt;

&lt;p&gt;💡 What are the four leader-to-follower replication methods?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;statement-based&lt;/li&gt;
  &lt;li&gt;write-ahead log shipping&lt;/li&gt;
  &lt;li&gt;logical (row-based) log&lt;/li&gt;
  &lt;li&gt;trigger-based&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 In a statement-based replication approach, what conditions may break the replication?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Any statement use nondeterministic function (NOW(), RAND())&lt;/li&gt;
  &lt;li&gt;Statement use auto-incrementing column&lt;/li&gt;
  &lt;li&gt;Statements that have side effects (e.g. triggers, stored procedures, user-defined functions)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;💡 What is the disadvantage of write-ahead log shipping replication approach?&lt;/p&gt;

&lt;p&gt;The log data is stored in a very low level, which makes the replication closely coupled to the storage engine. Normally the log cannot be consumed by different versions of the engine, which makes the zero-down time version upgrade challenging.&lt;/p&gt;

&lt;p&gt;💡 What are the different ways to solve replication lag issue?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;reading your own writes (or read-after-write consistency): 1) serve the often-modified entry by leader only 2) serve by follower only after some time passed the write (e.g. 1 mins later after update) 3) record the update timestamp and use the timestamp to check if value exists&lt;/li&gt;
  &lt;li&gt;monotonic reads: the same user always reads from the same replica&lt;/li&gt;
  &lt;li&gt;consistent prefix reads: if any writes happens in a certain order, then anyone reading those writes will see them appear in the same order&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;multi-leader&quot;&gt;Multi-leader&lt;/h4&gt;
&lt;p&gt;💡 What are the ways (name 4) of achieving convergent conflict resolution?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;last write wins approach:&lt;/li&gt;
  &lt;li&gt;higher-numbered replica wins:&lt;/li&gt;
  &lt;li&gt;somehow merge the values (e.g. concatenate strings):&lt;/li&gt;
  &lt;li&gt;record the conflict in an explicit data structure that preserves all the information and ask for user to resolve at some later time:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 What are the commonly seen multi-leader replication topologies?&lt;/p&gt;

&lt;p&gt;Circular topology, star topology and all-to-all topology&lt;/p&gt;

&lt;p&gt;💡 Why all-to-all topology is better than circular and star topology?&lt;/p&gt;

&lt;p&gt;Avoid the single point of failure&lt;/p&gt;

&lt;p&gt;💡 What is the replication topology in multi-leader replication architecture?&lt;/p&gt;

&lt;p&gt;A replication topology describe the communication paths along which writes are propagated from one node to another&lt;/p&gt;

&lt;p&gt;💡 What is the causality issue of all-to-all topology and what is the solution to that?&lt;/p&gt;

&lt;p&gt;Causality issue refer to the problem that some operations does not arrive to the system in the order expected due to the network latency. So one operation that depends on the other might arrive earlier (e.g. an update statement comes before an insert of the item). Version vector is the solution to the causality issue&lt;/p&gt;

&lt;h4 id=&quot;leaderless&quot;&gt;Leaderless&lt;/h4&gt;
&lt;p&gt;💡 How does an unavailable node catch up on the writes when it comes back online?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;read repair: the client fetches the data during read and detect the stale value from the nodes, it then write the update-to-date back to the nodes&lt;/li&gt;
  &lt;li&gt;anti-entropy process: a background process that constantly looks for differences in the data between replicas and copies any missing data from one replica to another&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 What is the benefit of choosing r and w to be majority (more than n/2) of nodes?&lt;/p&gt;

&lt;p&gt;This choice ensures w+r&amp;gt;n (quorum condition) while still tolerating up to n/2 node failures&lt;/p&gt;

&lt;p&gt;💡 If a workload pattern is few writes and many reads, what is better config for the quorum and why?&lt;/p&gt;

&lt;p&gt;Reduce the r, say to 1, increase the w, say to n (so r + w &amp;gt; n holds). In this case, we can reduce the load of read. However increase w to n has the disadvantage that a single unavailable nodes will cause write fall.&lt;/p&gt;

&lt;p&gt;💡 Define strict quorum reads and writes&lt;/p&gt;

&lt;p&gt;Define the number of nodes that confirms writes are w, the number of nodes required for read is r, and the total number of nodes is n. When w + r &amp;gt; n, it guarantees that at least one of the nodes for read contains the latest updated writes.&lt;/p&gt;

&lt;p&gt;💡 Define sloppy quorum&lt;/p&gt;

&lt;p&gt;In a large cluster with significantly more than n nodes, writes and reads still require w and r successful responses, but those may include nodes that are not among the designated n “home” nodes for a value.&lt;/p&gt;

&lt;h3 id=&quot;highlighted-topics&quot;&gt;Highlighted Topics&lt;/h3&gt;

&lt;h4 id=&quot;quorums-condition&quot;&gt;Quorums Condition&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;Leaderless architecture becomes popular after Amazon used it for its in-house &lt;em&gt;Dynamo&lt;/em&gt; system (the paper is written in 2007). &lt;a href=&quot;https://github.com/basho/riak&quot;&gt;Riak&lt;/a&gt;, &lt;a href=&quot;https://github.com/apache/cassandra&quot;&gt;Cassandra&lt;/a&gt; and &lt;a href=&quot;https://github.com/voldemort/voldemort&quot;&gt;Voldemort&lt;/a&gt; are open source datastores with leaderless replication models inspired by Dynamo, so this kind of database is also known as &lt;em&gt;Dynamo-style&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Quorums is the essential concept to guarantee the data consistency in Dynamo-style data storages. It is defined as this:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;If there are n replicas, every write must be confirmed by w nodes to be considered successful, and we must query at least r nodes fro each read. As long as w + r &amp;gt; n, we expect to get an up-to-date value when reading, because at least one of the r nodes we’re reading from must be up to date. Reads and writes that obey these r and w values are called quorum reads and writes&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-5/ddia-quorum.excalidraw.png&quot; alt=&quot;ddia-quorum.excalidraw&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can smartly choose w and r in different use cases:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;to a write intensive use case, we can choose a smaller w (but a larger r) to reduce the write latencies. Vice versa for read intensive use case&lt;/li&gt;
  &lt;li&gt;generally we can choose w &amp;gt; n/2 and r &amp;gt; n/2 so the system can tolerate up to n/2 nodes failures&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However even with w + r &amp;gt; n, there are likely to be edge cases where stale values are returned. Dynamo-style databases are generally optimized for use cases that can tolerate eventual consistency (BASE vs ACID).&lt;/p&gt;

&lt;h3 id=&quot;extended-topics&quot;&gt;Extended Topics&lt;/h3&gt;
&lt;h4 id=&quot;version-vector&quot;&gt;Version Vector&lt;/h4&gt;

&lt;p&gt;Consider two operations A and B happens at very similar time, there’s only three different possibilities in terms of the which happens first:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A happens before B and B depends on A’s occurrence&lt;/li&gt;
  &lt;li&gt;B happens before A and A depends on B’s occurrence&lt;/li&gt;
  &lt;li&gt;They happen at the same time and there’s no causality relationship between them&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we can identify the causality between A and B, then we can say that A and B are not concurrent and we can safely keep the result from the most recent operation. Otherwise, we want to keep the conflicted result from the concurrent operations A and B, and let the client to resolve the conflict (In simpler cases, we can take the “later” operation based on the timestamp or replica number order, but it comes with the risk of data loss).&lt;/p&gt;

&lt;p&gt;To detect the concurrency, we can take the approach to detect the causality, if no causality, means that the operations are concurrent.&lt;/p&gt;

&lt;p&gt;A simple algorithm to capture the happens-before relationship&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The server maintains a version number for every key, increments the version number every time that key is written, and stores the new version number along with the value written&lt;/li&gt;
  &lt;li&gt;When a client reads a key, the server returns all values that have not been overwritten, as well as the latest version number. A client must read a key before writing&lt;/li&gt;
  &lt;li&gt;When a client writes a key, it must include the version number from the prior read, and it must merge together all values that it received in the prior read&lt;/li&gt;
  &lt;li&gt;When the server receives a write with a particular version number, it can overwrite all values with the that version number or below, but it must keep all values with a higher version number (because those values are concurrent with the incoming write)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Important aspect to consider when reasoning about the scalability of these version vector is the existence of three different orders of magnitude at play:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;a &lt;strong&gt;small number&lt;/strong&gt; of replica nodes for each key&lt;/li&gt;
  &lt;li&gt;a &lt;strong&gt;large number&lt;/strong&gt; of server nodes&lt;/li&gt;
  &lt;li&gt;a &lt;strong&gt;huge number&lt;/strong&gt; of clients, keys and issued operations. 
Thus a scalable solution should avoid mechanisms that are linear with the highest magnitude and, if possible, even try to match the lowest scale&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here’s a summary and comparison of different version vector approaches&lt;/p&gt;

&lt;h5 id=&quot;version-vector-with-causal-histories&quot;&gt;Version vector with causal histories&lt;/h5&gt;
&lt;p&gt;causal histories ({a1, a2}) are maintained in this version vector solution, but this is a &lt;strong&gt;not useful in practical systems&lt;/strong&gt; because they scale linearly with the number of updates.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-5/version-vector_1.excalidraw.png&quot; alt=&quot;version-vector_1.excalidraw&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;causally-compliant-total-order-lww&quot;&gt;Causally compliant total order (LWW)&lt;/h5&gt;
&lt;p&gt;Assuming that client clocks are well synchronised and applying real time clock order, in this approach, replica nodes never store multiple versions and writes do not need to provide a get context. Cassandra is using this simplest approach.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-5/version-vector-2.excalidraw.png&quot; alt=&quot;version-vector-2.excalidraw&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;version-vectors-with-per-server-entry&quot;&gt;Version vectors with per-server entry&lt;/h5&gt;
&lt;p&gt;A concise version vector can be achieved by storing all the update-to-date server+version information with each replica. In this case, it is possible to track the causality among updates that were received in different servers. However, this approach cannot track causality among updates submitted to the same server. Dynamo system uses one entry per replica node and thus falls into this category.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-5/version-vector-3.excalidraw.png&quot; alt=&quot;version-vector-3.excalidraw&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;version-vector-with-per-client-entry&quot;&gt;Version vector with per-client entry&lt;/h5&gt;
&lt;p&gt;The version vectors with one entry per replica node are not enough to track causality among concurrent clients. One natural approach is to track causality by using version vectors with one entry per client. In the implementation, each client needs to maintain a counter increment it and provide it in each PUT, otherwise we will encountered the problem illustrated below. This approach is not very practical mainly because it is a big challenge to maintain the clients stability of the storage system - the application can run concurrent and dynamic threads, it is difficult to the identify and track them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-5/version-vector-4.excalidraw.png&quot; alt=&quot;version-vector-4.excalidraw&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;dotted-version-vectors&quot;&gt;Dotted version vectors&lt;/h5&gt;
&lt;p&gt;Dotted version vectors use a concise and accurate representation for the clocks to be used as a substitute for the classic version vectors. It uses only server-based ids and only a component per replica node, thus avoiding the space consumption explosion. The formal definition of the version vector is&lt;/p&gt;

\[\begin{align*}
&amp;amp;C[(r, m)] = \{r_i | 1 \leq i \leq m\}, \\
\\
&amp;amp;C[(r,m,n)] = \{r_i| 1 \leq i \leq m\} \cup \{r_n\}, \\
\\
&amp;amp;C[X] = \bigcup_{x\in X}C[x] \\
\\
&amp;amp;\text{In a component (r,m,n) we will always have n &amp;gt; m}
\end{align*}\]

&lt;p&gt;The example shows how the version vectors are updated for each update:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-5/version-vector-5.excalidraw.png&quot; alt=&quot;version-vector-5.excalidraw&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we see, the popular databases like dynamoDB and Casandra are using simpler version of the version vectors in practice. This is very common. In reality, production-level engineering system comprises complexities for performance and engineering simplicity.&lt;/p&gt;

&lt;h3 id=&quot;further-reading&quot;&gt;Further Reading&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.purdue.edu/homes/bb/cs542-11Spr/Parker_TSE83.pdf&quot;&gt;Detection of Mutual Inconsistency in Distributed Systems - 1983 classic paper of version vector&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1011.5808.pdf&quot;&gt;Dotted Version Vectors: Logical Clocks for Optimistic Replication&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf&quot;&gt;Dynamo: Amazon’s Highly Available Key-value Store - 2007 classic paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 10 Apr 2023 10:00:00 +0800</pubDate>
        <link>http://localhost:4000/blog1/2023/04/10/ddia-5.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog1/2023/04/10/ddia-5.html</guid>
        
        <category>reading_notes</category>
        
        <category>system_design</category>
        
        
        <category>blog1</category>
        
      </item>
    
      <item>
        <title>[DDIA] Chapter 4 - Encoding and Evolution</title>
        <description>&lt;h3 id=&quot;chapter-summary&quot;&gt;Chapter Summary&lt;/h3&gt;
&lt;p&gt;Services communicate to each other by protocols (pre-defined interfaces and data models). As application developers, we are very often making API design choices by choosing between HTTP or gPRC, or REST vs RPC vs GraphQL, or Web socket vs HTTP etc. The definitions of these terms are sometimes misunderstood a lot (at least by me for a long time). These terms are actually not mutual-exclusive.&lt;/p&gt;

&lt;p&gt;This chapter provides some fundamental knowledge on data encoding (serialisation): different mechanisms of turning data structures into bytes for for communicating over the network or storing on disk. The focus is on the compatibility properties of the different encoding strategies: how easily the backward and forward compatibility can each solution provide. At the later part of the chapter, how data encoding plays a role in the three common dataflow in application design namely data store, sync API communication and async message queue is discussed. Understanding the underlie mechanism of data encoding helps us to make better tech decisions when designing and implementing application softwares.&lt;/p&gt;

&lt;h3 id=&quot;mind-map&quot;&gt;Mind Map&lt;/h3&gt;

&lt;div class=&quot;mindmap-container&quot;&gt;
    &lt;iframe src=&quot;https://www.xmind.net/embed/fUAvvp&quot; width=&quot;900px&quot; height=&quot;540px&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&quot;question-summary&quot;&gt;Question Summary&lt;/h3&gt;

&lt;p&gt;💡 1. Why we need to encode the data (translation between the in-memory data representation to a byte sequence that is sent over the network or sent to a file)?&lt;/p&gt;

&lt;p&gt;The data stored in memory (normally as list, object, hash tables, trees) is designed for efficient access by the CPU. While the data transferred through the network or written to a file is in a format of self-contained sequence of bytes, which is quite different from the data structure used in memory.&lt;/p&gt;

&lt;p&gt;💡 2. What are the problems of language specific encoding method? (e.g. java’s kryo, ruby’s marshal, python’s pickle)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;they are language specified and has to tie with the chosen language&lt;/li&gt;
  &lt;li&gt;most of the time the performance is not good enough&lt;/li&gt;
  &lt;li&gt;there’s no versioning control, backward compatibility and forward compatibility is not easy to maintain&lt;/li&gt;
  &lt;li&gt;security concerns: in order to restore the data in the same object type, the decoding process needs to be able to instantiate arbitrary class, which can be used by attackers to inject executable code&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 3. What are the pros and cons of textual format encoding method? (e.g. xml, json, csv)&lt;/p&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;human readable, easy to understand and debug&lt;/li&gt;
  &lt;li&gt;can be easily made to support backward and forward compatibility&lt;/li&gt;
  &lt;li&gt;language-independent&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;verbose&lt;/li&gt;
  &lt;li&gt;ambiguity around the encoding of numbers. Specific flaws for specific format. E.g.:   integer large than 2^53 cannot be parsed correctly by language uses floating-point numbers. JSON and XML does not support binary string&lt;/li&gt;
  &lt;li&gt;weak support for schema definition&lt;/li&gt;
  &lt;li&gt;CSV does not have schema and confusion to handle value with comma or newline&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 4. What are the benefits of binary encoding?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;clearly defined backward and forward compatibility semantics&lt;/li&gt;
  &lt;li&gt;efficient computation and result in compact size&lt;/li&gt;
  &lt;li&gt;type and schema definition, used as good documentation and code generation for statically typed languages&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 5. How do Thrift and Protocol Buffers handle schema changes while keeping backward and forward compatibility?&lt;/p&gt;

&lt;p&gt;The field tag is used to refer to each field and it is never changed. There will be new fields with new field tags added. To maintain the forward compatibility, the old code will simply ignore the unrecognized field tag. For backward compatibility, the new code can always read old data because the tag numbers still have the same meaning. The only detail is that you cannot make the new field as required, as the old code will not have written the new field you added.&lt;/p&gt;

&lt;p&gt;💡 6. What is the benefit of ‘repeat’ field type in Protocol Buffers?&lt;/p&gt;

&lt;p&gt;The encoding of ‘repeat’ field type says that the same field tag simply appears multiple times in the record. This has the nice effect that it is okay to change an optional field into a repeated field. New code reading old data sees a list with zero or one elements while old code reading new data sees only the last element of the list.&lt;/p&gt;

&lt;p&gt;💡 7. What are the three ways to communicate the writer’s schema to the reader when using Avro encoding?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;When the dataset volume is large, it is ok to contain the writer’s schema in the file itself and sent.&lt;/li&gt;
  &lt;li&gt;When there’s upgrading of versions frequently and each entry of data might be written with different versions of schema. It is helpful to maintain a database of version number of the writer’s schema. It is helpful to maintain a database of version number of the writer’s schema. Then send only the version number with the data to the reader. When the reader reads the data, it will retrieve the schema by the version number.&lt;/li&gt;
  &lt;li&gt;When two processes are communicating over a bidirectional network connection, they can negotiate the schema version on connection setup and then use the schema for the lifetime of the connection.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 8. How does Avro maintain the forward and backward compatibility?&lt;/p&gt;

&lt;p&gt;Avro maintains two set of schema - writer’s schema and reader’s schema. The schema resolution logic will check the compatibility and resolve the difference of the two. Adding or removing fields that has default value, we can maintain the compatibility.&lt;/p&gt;

&lt;p&gt;💡 9. Considering the dataflow through database, in which scenarios that we need to handle the backward compatibility and the forward compatibility?&lt;/p&gt;

&lt;p&gt;We need to consider both backward compatibility and forward compatibility. Backward compatibility needs to be handled by default, as the data written at a time, will be read at a later time. In the scenarios that a database is read and written by multiple instances, during rolling upgrade of the instances, the database may be written by a newer version of the code, and subsequently read by an older version of the code that is still running. Thus, forward compatibility is also often required for database.&lt;/p&gt;

&lt;p&gt;💡 10. What is data outlives code?&lt;/p&gt;

&lt;p&gt;Data in the database was written some time ago and might become incompatible with the newest version of code that reads and writes data.&lt;/p&gt;

&lt;p&gt;💡 11. What can schema evolution bring to the database dataflow?&lt;/p&gt;

&lt;p&gt;Schema evolution allows the entire database to appear as if it was encoded with a single schema, even though the underlying storage may contain records encoded with various historical versions of the schema&lt;/p&gt;

&lt;h3 id=&quot;highlighted-topics&quot;&gt;Highlighted Topics&lt;/h3&gt;

&lt;p&gt;Understand the four encoding algorithms with an example&lt;/p&gt;

&lt;p&gt;Sample data&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
	&quot;username&quot;: &quot;Martin&quot;,
	&quot;favoriteNumber&quot;: 1337,
	&quot;interest&quot;: [&quot;daydreaming&quot;, &quot;hacking&quot;]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;messagepack&quot;&gt;MessagePack&lt;/h4&gt;
&lt;p&gt;MessagePack has no schema, it is a binary encoding for JSON. The schema (keys) needs to be encoded in the message. So, the binary encoding is 66 bytes for this example, which is not much size reduction achieved here (original json is 81 bytes). It is not clear whether such a small space reduction is worth the loss of human-readability.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-4/encoding_messagepack.JPG&quot; alt=&quot;image_1&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;thrift-and-protocol-buffers&quot;&gt;Thrift and Protocol Buffers&lt;/h4&gt;
&lt;p&gt;Thrift and protocol buffers are similar in the sense that schema is defined and fixed with tag number. With the re-defined schema, the message size is significant reduced since only the schema tag is required in the message. An obvious difference between thrift and protocol buffers is how the array type data is defined: thrift support list data type while protocol buffers use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;repeated&lt;/code&gt; marker to indicate that a field can be a list: this has the nice effect that it’s okay to change an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;optional&lt;/code&gt; field into a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;repeated&lt;/code&gt; field. New code reading old data sees a list with zero or one elements; old code reading new data sees only the last element of the list.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-4/encoding_thrift.JPG&quot; alt=&quot;image_2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-4/encoding_protobuff.JPG&quot; alt=&quot;image_3&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;avro&quot;&gt;Avro&lt;/h4&gt;
&lt;p&gt;Avro is a subproject of Hadoop. It does not contains field tag in the schema. To parse the binary data, you go through the fields in the order that they appear in the schema and use the schema to tell you the datatype of each field. For Avro, the writer and reader maintains its own copy of schema, Avro library resolves the differences by looking at the writer’s schema and the reader’s schema side by side and translating the data from the writer’s schema into the reader’s schema.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-4/encoding_avro.JPG&quot; alt=&quot;image_4&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;extended-topics&quot;&gt;Extended Topics&lt;/h3&gt;

&lt;p&gt;A better understanding of the concepts:&lt;/p&gt;

&lt;h4 id=&quot;rest-vs-rpc&quot;&gt;REST vs RPC&lt;/h4&gt;
&lt;p&gt;REST: a philosophy that builds upon the principles of HTTP. It emphases on “resource-centric”. An API designed according to the principles of REST is called RESTful.&lt;/p&gt;

&lt;p&gt;RPC: first we shall differentiate the definition of RPC used in different context. RPC (remote procedure calls), the idea has been around since 1970s. The RPC model tries to make a request to a remote network service look the same as calling a function or method in your programming language, within the same process. Nowadays, we use RPC often to describe:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;protocols
    &lt;blockquote&gt;
      &lt;p&gt;Remote Procedure Call (RPC) is a protocol that provides the high-level communications paradigm used in the operating system. RPC presumes the existence of a low-level transport protocol, such as Transmission Control Protocol/Internet Protocol (TCP/IP) or User Datagram Protocol (UDP), for carrying the message data between communicating programs. RPC implements a logical client-to-server communications system designed specifically for the support of network applications. When RPC is made, the procedure identifier and parameters serialized into a request message then sent to the remote process which serves the call. On the remote process the message got deserialized back, the process run, and the results returned in a similar way.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;frameworks
    &lt;blockquote&gt;
      &lt;p&gt;such as &lt;a href=&quot;https://grpc.io/&quot;&gt;gRPC&lt;/a&gt;, &lt;a href=&quot;https://thrift.apache.org/&quot;&gt;Apache Thrift&lt;/a&gt;, &lt;a href=&quot;https://avro.apache.org/&quot;&gt;Apache Avro&lt;/a&gt;, &lt;a href=&quot;https://dubbo.apache.org/en/index.html&quot;&gt;Apache Dubbo&lt;/a&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;Philosophy of API pattern
    &lt;blockquote&gt;
      &lt;p&gt;The RPC API thinks in terms of “verbs”, exposing the restaurant functionality as function calls that accept parameters, and invokes these functions via the HTTP verb that seems most appropriate - a ‘get’ for a query, and so on, but the name of the verb is purely incidental and has no real bearing on the actual functionality, since you’re calling a different URL each time. Return codes are hand-coded, and part of the service contract.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When we compare REST and RPC, we could be comparing HTTP API design pattern - use RESTful and RPC-style design pattern (another pattern faded out is SOAP). Or we could be evaluating tech decision on protocols, whether we should use HTTP (json) or RPC (gRPC or other frameworks build on top of the protocols).&lt;/p&gt;

&lt;h4 id=&quot;http-vs-grpc&quot;&gt;HTTP vs gRPC&lt;/h4&gt;
&lt;p&gt;Explained above, we are comparing the json encoding and protocol buffers encoding&lt;/p&gt;

&lt;h4 id=&quot;graphql-vs-rest&quot;&gt;GraphQL vs REST&lt;/h4&gt;
&lt;p&gt;GraphQL is an application layer server-side technology that is used for executing queries with existing data. It is deployed over HTTP using a single endpoint that provides the full capabilities of the exposed service. It promotes a difference API design philosophy from the REST, it requires strong typed schema definition and supports schema stitching.&lt;/p&gt;

&lt;h4 id=&quot;websocket&quot;&gt;WebSocket&lt;/h4&gt;
&lt;p&gt;WebSocket connection is initiated by the client. It is bi-directional and persistent. It starts its life as a HTTP connection and could be “upgraded” via some well-defined handshake to a WebSocket connection. Through this persistent connection, a server could send updates to a client. WebSocket connections generally work even if a firewall is in place. This is because they use port 80 or 443 which are also used by HTTP/HTTPS connections.&lt;/p&gt;

&lt;p&gt;WebSocket is the default choice for the online messaging service. We could just skip the discussion of HTTP vs WebSocket in supporting peer-to-peer messaging and opt for WebSocket directly. It is not some fancy technology, as Keith Adams said in his slack presentation, it could be some 40 lines of java code wrapping a standard WebSocket library with a for loop over the people that are connected.&lt;/p&gt;

&lt;h4 id=&quot;further-reading&quot;&gt;Further reading&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://thrift.apache.org/static/files/thrift-20070401.pdf&quot;&gt;Thrift paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html&quot;&gt;A short and concise version of this chapter written by the same author 10 years ago&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 18 Sep 2022 10:00:00 +0800</pubDate>
        <link>http://localhost:4000/blog1/2022/09/18/ddia-4.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog1/2022/09/18/ddia-4.html</guid>
        
        <category>reading_notes</category>
        
        <category>system_design</category>
        
        
        <category>blog1</category>
        
      </item>
    
      <item>
        <title>一本育儿书</title>
        <description>
</description>
        <pubDate>Thu, 01 Sep 2022 03:00:00 +0800</pubDate>
        <link>http://localhost:4000/blog2/2022/09/01/motherhood.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog2/2022/09/01/motherhood.html</guid>
        
        
        <category>blog2</category>
        
      </item>
    
      <item>
        <title>[DDIA] Chapter 3 - Storage and Retrieval</title>
        <description>&lt;h3 id=&quot;chapter-summary&quot;&gt;Chapter Summary&lt;/h3&gt;

&lt;p&gt;This is an interesting and exciting chapter! Finally it starts to talk about data systems. The discussion starts from the illustration of how data store can be done with a simple text file. Extending from the concept, a simple storage engine with appending-only log is introduced - hash index. The basic knowledge makes the understanding of the classic LSM-tree storage engine easy. Complement to the LSM-tree storage engine, another classic B-tree storage engine is discussed in detail.&lt;/p&gt;

&lt;p&gt;In the second section of the chapter, the difference between OLAP and OLTP databases is discussed. Diving deeper into the OLAP and data warehouse concept, column-oriented data storage engine is introduced.&lt;/p&gt;

&lt;h3 id=&quot;mind-map&quot;&gt;Mind Map&lt;/h3&gt;

&lt;div class=&quot;mindmap-container&quot;&gt;
    &lt;iframe src=&quot;https://www.xmind.net/embed/H5bX6h&quot; width=&quot;900px&quot; height=&quot;540px&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&quot;questions-summary&quot;&gt;Questions Summary&lt;/h3&gt;

&lt;p&gt;💡 1. How does Hash Indexes work?&lt;/p&gt;

&lt;p&gt;Key-value stores are used (similar to the hashmap data structure). The key is the index key and the value is byte offset of the value in the data file. The hash index is stored in memory for fast access. When reading the data from the index, we can read the byte offset of the key from the index, and we only need to load the value data from the disk with just one disk seek.
The data file is append only and compaction and segment merging are performed to reduce the storage&lt;/p&gt;

&lt;p&gt;💡 2. What is compaction (of a key-value update log)?&lt;/p&gt;

&lt;p&gt;For a log-structure database, compaction on the log file segments means throwing away duplicate keys in the log and keeping only the most recent update for each key.&lt;/p&gt;

&lt;p&gt;💡 3. How to handle partially written records in hash index?&lt;/p&gt;

&lt;p&gt;Use checksums to delete the corrupted parts of the log&lt;/p&gt;

&lt;p&gt;💡 4. How to handle the data deleting in hash index?&lt;/p&gt;

&lt;p&gt;Append a special deletion record to the data file (called tombstone). When log segments are merged, the tombstone tells the merging process to discard any previous value for the deleted key&lt;/p&gt;

&lt;p&gt;💡 5. What is checksums?&lt;/p&gt;

&lt;p&gt;A checksum is an alphanumeric value that uniquely represents the contents of a file. Checksums are often used to verify the integrity of files downloaded from external source.&lt;/p&gt;

&lt;p&gt;💡 6. What are the limitations of hash index?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the hash table must fit in memory, it does not work for data that has a very large number of keys&lt;/li&gt;
  &lt;li&gt;range queries are not support, you need to fetch data key one by one for a range of keys&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 7. What are the two ways to solve the issue of a key-value index where the key is not unique?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;making each value stored with the keys a list of matching row identifier&lt;/li&gt;
  &lt;li&gt;by appending a row identifier to the key to make the key unique&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 8. What is the reason that in-memory data storage performs faster than disk-based data storage?&lt;/p&gt;

&lt;p&gt;The overhead resides mainly in the data encoding and decoding process as data stored in very different structure in memory and in disk. Surprisingly the read speed to memory and disk is not the deciding factor although they are in huge difference. (Anyway, the disk-based storage could avoid read from disk given enough memory provided and data cached)&lt;/p&gt;

&lt;p&gt;💡 9. What is LSM tree?&lt;/p&gt;

&lt;p&gt;Log-structured Merge-tree. Storage engines that are based on the principle of &lt;strong&gt;merging and compacting sorted files&lt;/strong&gt; are often called LSM storage engines.&lt;/p&gt;

&lt;p&gt;💡 10. How does the LSM-tree storage engine works (SSTable + Memtable)?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;write to the in-memory sorted balanced tree (red-black tree), also called a memtable&lt;/li&gt;
  &lt;li&gt;when the memtable size is larger than a threshold, write to the disk as SSTable. When copying is ongoing, the newly coming write will go to a new memtable&lt;/li&gt;
  &lt;li&gt;to process a read request, first check the memtable, if does not find, check the latest SSTable and so on.&lt;/li&gt;
  &lt;li&gt;on the background, a process will periodically performing merging and compaction on the SSTables to delete data and remove duplicates&lt;/li&gt;
  &lt;li&gt;bloom filter is added as a first layer to check the existence of the key before it hits to the memtable and SSTables&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 11. What is a memtable?&lt;/p&gt;

&lt;p&gt;It is a in-memory cache of the latest set of record writes applied to the database. It is normally in a data structure (e.g. red-black tree) that supports log(n) time for insert and lookup.
The memtable will be write to disk as an SSTable after the size passes some threshold.&lt;/p&gt;

&lt;p&gt;💡 12. How does the LSM-tree storage engine improve the read efficiency for non-existing key?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use a bloom filter to check if key exists before searching for the keys in memtable and SSTables&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 13. How does a LSM-tree storage engine handle server crash?&lt;/p&gt;

&lt;p&gt;The server crash will cause the lost of memtable. In this case, the service will keep a separate append-only log file in disk to record each operation write to the memtable. This is only used to recover the memtable from crash.&lt;/p&gt;

&lt;p&gt;💡 14. How does a B-tree storage engine work?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;B-trees break the database down into fixed size blocks or pages (usually 4kb in size) and read or write one page at a time&lt;/li&gt;
  &lt;li&gt;Each page can be identified using an address or location, which allows one page to refer to another, we can use these page reference to construct a tree of pages&lt;/li&gt;
  &lt;li&gt;when performing read, start from the root of the b-tree and look for the child page the key resides until reaching the leaf page&lt;/li&gt;
  &lt;li&gt;when performing write, search for the leaf key, then caching the value in that page and writing it back to disk after update. If there isn’t enough space accommodate the new key, it creates a new page and updates the parent’s pointer to point to the new page&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 15. What is a heap file?&lt;/p&gt;

&lt;p&gt;When the db index stores the value’s identifier, the actual data of the value is stored in a place called heap file. Often it stores data in no particular order. It is common because it avoids duplicating data when multiple secondary indexes are present.&lt;/p&gt;

&lt;p&gt;💡 16. What is a clustered index?&lt;/p&gt;

&lt;p&gt;It is refer to the type of index that the value stored directly with the key in the index, instead of an identifier of the value stored. This will ensure a better performance than the later. One example is that MySQL InnoDB storage engine, the primary key of a table is always a clustered index, and the secondary indexes refer to the primary key (not using heap file)&lt;/p&gt;

&lt;p&gt;💡 17. Compare B-trees and LSM-trees?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;B-tree is used to support page-oriented storage engine (used in MySQL, MongoDB etc). It stores the key and value (location to the value) and reference to other pages as small chucked pages in a tree-structure to allow efficient access to the keys&lt;/li&gt;
  &lt;li&gt;LSM-trees is used to support log-structured storage engines (e.g. LevelDB). It consists of a in-memory memtable and in-disk SSTables. The memtable provides fast write and read and keeps the key in sorted order. The data in memtable is periodically loaded to SSTable for persistence. Merging and compacting is performed on SSTables to remove duplications&lt;/li&gt;
  &lt;li&gt;B-tree enabled data storage is better suitable for read-heavy applications while LSM-trees database is more suitable for write-heavy applications. B-tree is more mature compared to LSM-trees.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 18. Why some wide-column database e.g. BigTable, Cassandra, Hbase are not column-oriented database?&lt;/p&gt;

&lt;p&gt;These database are not column stores in the original sense of the term, since their two-level structures do not use a columnar data layout. They introduces the column-family concept. Within each column family, they store all columns from a row together, along with a row key, and they do not use column compression.&lt;/p&gt;

&lt;p&gt;💡 19. What are OLTP and OLAP? What are the main differences?&lt;/p&gt;

&lt;p&gt;Online transactional processing vs online analytical processing.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;read pattern: OLTP is for a small number of record per query, OLAP aggregate over large number of records&lt;/li&gt;
  &lt;li&gt;write pattern: OLTP is for random access with low latency, OLAP is for bulk import or event stream&lt;/li&gt;
  &lt;li&gt;end users: OLTP for web app and end users, OLAP for data analytics and decision maker&lt;/li&gt;
  &lt;li&gt;data representation: OLTP represents the latest state of the data, OLAP represents the historical events&lt;/li&gt;
  &lt;li&gt;data size: OLTP - GB to TB, OLAP - TB to PB&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;highlighted-topics&quot;&gt;Highlighted Topics&lt;/h3&gt;

&lt;p&gt;I find the discussion about the LSM-tree storage engine is really interesting and inspiring in this chapter. Here I use illustrations to describe how a simplest storage solution (a plain file) can be extend and evolved to an efficient and full-functional storage engine.&lt;/p&gt;

&lt;h4 id=&quot;simple-storage&quot;&gt;Simple Storage&lt;/h4&gt;

&lt;p&gt;A plain text file as a simple storage engine is illustrated below. The write operations (insert/update/delete) is always append to the end of the log file. The read operation is always start from the beginning and scan to the end. This results to a simple structure, extremely terrible read performance and unbeatable efficient write performance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-3/ddia-3-simple-storage.excalidraw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;hash-index-storage&quot;&gt;Hash Index Storage&lt;/h4&gt;

&lt;p&gt;To improve the read efficiency, the concept of hash index is introduced. It is a in-memory hash map to store the key and the offset to the log file where the value is appended. Use the offset information, one disk seek is required to fetch the value. Compaction and merging strategy is used to reduce the log file size resulting from the append-only write strategy&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-3/ddia-3-hash-index.excalidraw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;lsm-tree-storage&quot;&gt;LSM Tree storage&lt;/h4&gt;

&lt;p&gt;One of the problem of hash index storage engine is the hash index has to be fit into the memory. Memtable and SSTable are introduced in LSM-tree storage to solve the problem. Sorted key concept here is very important. It makes the searching and merging efficient with the sorted nature of the key. Bloom filter and write-ahead log are some common technics to improve the performance and reliability.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-3/ddia-3-lsm-tree.excalidraw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;extended-topics&quot;&gt;Extended Topics&lt;/h3&gt;

&lt;p&gt;B-tree and B*-tree storage engines&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;TODO; just want to publish this post sooner to motivate myself to continue to write&lt;/p&gt;
&lt;/blockquote&gt;

</description>
        <pubDate>Wed, 27 Jul 2022 10:00:00 +0800</pubDate>
        <link>http://localhost:4000/blog1/2022/07/27/ddia-3.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog1/2022/07/27/ddia-3.html</guid>
        
        <category>reading_notes</category>
        
        <category>system_design</category>
        
        
        <category>blog1</category>
        
      </item>
    
  </channel>
</rss>
