<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LittleCheeseCake</title>
    <description>I am migrating from a blog service provider to github, still trying out with Jekyll
</description>
    <link>http://yulu.github.io/</link>
    <atom:link href="http://yulu.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 26 Aug 2023 15:45:12 +0800</pubDate>
    <lastBuildDate>Sat, 26 Aug 2023 15:45:12 +0800</lastBuildDate>
    <generator>Jekyll v4.2.2</generator>
    
      <item>
        <title>[Kafka Guide] Chapter 3 - Kafka Producer</title>
        <description>&lt;h3 id=&quot;kafka-producer-components-illustrated&quot;&gt;Kafka Producer Components Illustrated&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-producer-components.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;basic-setup&quot;&gt;Basic Setup&lt;/h3&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nc&quot;&gt;Properties&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kafkaProps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Properties&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= start with a property object&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kafkaProps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bootstrap.servers&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;broker1:9092,broker2:9092&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= server uri&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;kafkaProps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;key.serializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;org.apache.kafka.common.serialization.StringSerializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= use default serializer&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kafkaProps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value.serializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;org.apache.kafka.common.serialization.StringSerializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;producer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;KafkaProducer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kafkaProps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;sending-messages&quot;&gt;Sending Messages&lt;/h3&gt;

&lt;p&gt;Three primary methods of sending messages&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;fire-and-forget: Send and do not wait for response&lt;/li&gt;
  &lt;li&gt;synchronous send: use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Future&lt;/code&gt; object and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get()&lt;/code&gt; to wait on the Future and see if the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;send()&lt;/code&gt; was successful or not before sending the next record&lt;/li&gt;
  &lt;li&gt;asynchronous send: A producer object can be used by multiple threads to send messages. Product object is thread-safe&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;the-simplest-way-fire-and-forget&quot;&gt;The Simplest Way (fire-and-forget)&lt;/h5&gt;
&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 
	&lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;CustomerCountry&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Precision Products&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;France&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= there&apos;re different constructors, here we use a simple one requires: topic name, key and value&lt;/span&gt;
	
&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// this returns a future object with RecordMetadata, but we ignore it&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Exception&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;printStackTrace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// exceptions before msg is sent can be captured e.g. SerializationException, BufferExhaustedException and InterruptException&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;synchronous-send&quot;&gt;Synchronous Send&lt;/h5&gt;
&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;CustomerCountry&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Precision Products&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;France&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; 
	
&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= use future.get() to wait for the reply&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Exception&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;printStackTrace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= if there&apos;s any errors before or while sending to Kafka&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;asynchronous-send-with-a-callback&quot;&gt;Asynchronous Send with a Callback&lt;/h5&gt;
&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DemoProducerCallback&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Callback&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= need to implements org.apacke.kafka.clients.producer.callback&lt;/span&gt;
	&lt;span class=&quot;nd&quot;&gt;@Override&lt;/span&gt;
	&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;onCompletion&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;RecordMetadata&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;recordMetadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Exception&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;printStackTrace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= if kafka returns an error, onCompletion will have a nonnull exception&lt;/span&gt;
		&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;CustomerCountry&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Precision Products&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;France&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DemoProducerCallback&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;());&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= we pass a Callback object along when sending the record, callback is executed in the main thread, so it should be reasonably fast&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;configurations&quot;&gt;Configurations&lt;/h3&gt;

&lt;h5 id=&quot;core-configurations&quot;&gt;Core Configurations&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;client.id&lt;/code&gt;: logical identifier for the client and the application it is used in.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;acks&lt;/code&gt;: controls how many partition replicas must receive the record before the producer can consider the write successful.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;You will see that with lower and less reliable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;acks&lt;/code&gt; configuration, the producer will be able to send records faster. This means that you trade off reliability for producer latency. However, end-to-end latency is measured from the time a record was produced until it is available for consumers to read and is identical for all three options. The reason is that, in order to maintain consistency, Kafka will not allow consumers to read records until they are written to all in sync replicas. **Therefore, if you care about end-to-end latency, rather than just the producer latency, there is no trade-off to make: you will get the same end-to-end latency if you choose the most reliable option.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h5 id=&quot;message-delivery-time&quot;&gt;Message Delivery Time&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-producer-delivery-time.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max.block.ms&lt;/code&gt;: how long the producer may block when calling &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;send()&lt;/code&gt; and when explicitly requesting metadata via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;partitionsFor()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;delivery.timeout.ms&lt;/code&gt;: limit the amount of time spent from the point a record is ready for sending until either the broker responds or the client gives up, including time spent on retries, &lt;strong&gt;use this one and leave the retry settings as default&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;request.timeout.ms&lt;/code&gt;: controls how long the producer will wait for a reply from the server when sending data&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;retries&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;retry.backoff.ms&lt;/code&gt;: &lt;strong&gt;not recommended to use&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linger.ms&lt;/code&gt;: controls the amount of time to wait for additional messages before sending the current batch.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h5 id=&quot;others&quot;&gt;Others&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;buffer.memory&lt;/code&gt;: sets the amount of memory the producer will use to buffer messages waiting to be sent to brokers.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compression.type&lt;/code&gt;: e.g. snappy, gzip, lz4, zstd&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch.size&lt;/code&gt;: controls the amount of memory in bytes that will be used for each batch (when multiple records are sent to the same partition, the producer will batch them together)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max.in.flight.requests.per.connection&lt;/code&gt;: controls how many message batches the producer will send to the server without receiving response&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max.request.size&lt;/code&gt;: controls the size of a produce request sent by the producer&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;receive.buffer.bytes&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;send.buffer.bytes&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;enable.idempotence&lt;/code&gt;: when idempotent producer is enabled, the producer will attach a sequence number to each record it sends. If the broker receives records with the same sequence number, it will reject the second copy and the producer will receive the harmless &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DuplicateSequenceException&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;serializers&quot;&gt;Serializers&lt;/h3&gt;
&lt;p&gt;Data serialization strategies detail can be ref to &lt;a href=&quot;/blog1/2022/09/18/ddia-4.html&quot;&gt;DDIA-Chapter4 encoding (serialisation mechanism)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Schema Regsitry&lt;/strong&gt; Pattern is used: the idea is to store all the schemas used to write data to Kafka in the registry. Then we simply store the identifier for the schema in the record we produce to Kafka. The consumers can then use the identifier to pull the record out of the Schema Registry and deserialize the data. The key is that all this work is done in the serializers and deserializers:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-chapter2-1.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is an example of how to produce generated Avro objects to Kafka&lt;/p&gt;
&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nc&quot;&gt;Properties&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;props&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Properties&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;boostrap.servers&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;localhost:9092&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;key.serializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;io.confluent.kafka.serializers.KafkaAvroSerializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// &amp;lt;= use kafkaAvroSerializer to serialize our objects with Avro&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value.serializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;io.confluent.kafka.serializers.kafkaAvroSerializer&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;schema.registry.url&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;schemaUrl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;topic&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;customerContacts&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;nc&quot;&gt;Producer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Customer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;producer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;KafkaProducer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// We keep producing new events until someone ctrl-c&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;nc&quot;&gt;Customer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;customer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;CustomerGenerator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getNext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
	&lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Generated customer &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;customer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;());&lt;/span&gt;
	&lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Customer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;topic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;customer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;customer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;partitioners&quot;&gt;Partitioners&lt;/h3&gt;

&lt;p&gt;The importance of keys: all messages with the same key will go to the same partition (for the same topic).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;When the key is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;null&lt;/code&gt; and the default partitioner is used, the record will be sent to one of the available partitions of the topic at random. A round-robin algorithm will be used to balance the messages among the partitions.&lt;/li&gt;
  &lt;li&gt;If a key exists and the default partitioner is used, Kafka will hash the key and use the result to map the message to a specific partition.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RoundRobinPartitioner&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UniformStickyPartitioner&lt;/code&gt; can be used to replace the default partitioner.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can implement custom partitioning strategy, e.g. code example below&lt;/p&gt;
&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;BananaPartitioner&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Partitioner&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;configure&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;configs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
	
	&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;partition&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;topic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Object&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;byte&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keyBytes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
						 &lt;span class=&quot;nc&quot;&gt;Object&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;byte&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valueBytes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
						 &lt;span class=&quot;nc&quot;&gt;Cluster&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;nc&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;PartitionInfo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partitions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;partitionsForTopic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;topic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numPartitions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partitions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;

		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keyBytes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(!(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;instanceOf&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)))&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;InvalidRecordException&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;We expect all messages to have customer name as key&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
		
		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(((&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;equals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Banana&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numPartitions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// Banana will always go to last partition&lt;/span&gt;

		&lt;span class=&quot;c1&quot;&gt;// Other records will get hashed to the rest of the partitions&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;murmur2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keyBytes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numPartitions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

	&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;headers-interceptors-quotas-and-throttling&quot;&gt;Headers, Interceptors, Quotas and Throttling&lt;/h3&gt;

&lt;h5 id=&quot;headers&quot;&gt;Headers&lt;/h5&gt;

&lt;p&gt;Additional metadata information about Kafka record, e.g.indicate the source of data, information for routing or tracing)&lt;/p&gt;

&lt;h5 id=&quot;interceptors&quot;&gt;Interceptors&lt;/h5&gt;

&lt;p&gt;Capturing monitoring and tracing information, enhancing the message with standard headers, redacting sensitive information. Example code snippet&lt;/p&gt;
&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;CountingProducerInterceptor&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ProducerInterceptor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;nc&quot;&gt;ScheduledExecutorService&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;executorService&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 
		&lt;span class=&quot;nc&quot;&gt;Executors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;newSingleThreadScheduledExecutor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
	&lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;AtomicLong&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numSent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;AtomicLong&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;AtomicLong&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numAcked&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;AtomicLong&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

	&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;configure&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;nc&quot;&gt;Long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;windowSize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Long&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;valueOf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
			&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;counting.interceptor.window.size.ms&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;executorService&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;scheduleAtFixedRate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;CountingProducerInterceptor:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;windowSize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;windowSize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TimeUnit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;MILLISECONDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

	&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;onSend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;ProducerRecord&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;producerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;numSent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;incrementAndGet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;producerRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

	&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;onAcknowledgement&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;RecordMetadata&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;recordMetadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Exception&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;numAcked&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;incrementAndGet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

	&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;executorService&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;shutdownNow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

	&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;// just print out the sent and ack counts in a separate thread&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;// reset the counts in each time window&lt;/span&gt;
		&lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numSent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getAndSet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
		&lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numAcked&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getAndSet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Producer interceptors can be applied without any changes to the client code (need to have deployment config changes). To use the preceding interceptor:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Add your jar to the classpath
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export CLASSPATH=$CLASSPATH:~./target/CountProducerInterceptor-1.0-SNAHPSHOT.jar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Create a config file (producer.config) that includes:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;interceptor.classes=com.shapira.examples.interceptors.CountProducerInterceptor counting.interceptor.window.size.ms=100000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Run the application as you normally would but make sure include the configuration that you created
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bin/kafka-console-producer.sh --broker-list localhost:9092 --topic interceptor-test --producer.config producer.config
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;quotas-and-throttling&quot;&gt;Quotas and Throttling&lt;/h5&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;quota.producer.default=2M
quota.producer.override=&quot;clientA:4M,clientB:10M&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;summary-as-ankicard&quot;&gt;Summary as Ankicard&lt;/h3&gt;

&lt;p&gt;💡 What are the three ways to send message from producer to Kafka broker?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;send-and-forget: producer.send(record);&lt;/li&gt;
  &lt;li&gt;synchronous send: producer.send(record).get(); // &amp;lt;= use future.get() to wait for the reply&lt;/li&gt;
  &lt;li&gt;asynchronous send: producer.send(record, new DemoProducerCallback());&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 Can a &lt;strong&gt;producer&lt;/strong&gt; object be used by multiple threads to send messages in Kafka producer application?&lt;/p&gt;

&lt;p&gt;Yes, product object is thread-safe&lt;/p&gt;

&lt;p&gt;💡 What is the recommended Kafka producer timeout and retry configuration, and why?&lt;/p&gt;

&lt;p&gt;Configure the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;delivery.timeout.ms&lt;/code&gt; and leave the retries config as default. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;delivery.timeout.ms&lt;/code&gt;` includes the time when record is ready to be sent to the response is received from the broker, including the retries. In this case we limit the total preparation + in-flight time and let the producer retries as many times as possible within the limited timeout constraint.&lt;/p&gt;

&lt;p&gt;💡 What are the five steps in the send() process in the Kafka producer?&lt;/p&gt;

&lt;p&gt;send() → batching → await send → retries → inflight&lt;/p&gt;

&lt;p&gt;💡 Which component in the Kafka producer is responsible for the serialization and how to config?&lt;/p&gt;

&lt;p&gt;Serializer. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;props.put(&quot;value.serializer&quot;, &quot;io.confluent.kafka.serializers.kafkaAvroSerializer&quot;);&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;💡 What are the commonly used serialization strategies in Kafka producer?&lt;/p&gt;

&lt;p&gt;Avro, protobuff, json&lt;/p&gt;

&lt;p&gt;💡 The key in the message is used to select the partition. If a key is null, what strategy will be used to select the partition?&lt;/p&gt;

&lt;p&gt;A round-robin strategy will be used to select the partition&lt;/p&gt;

&lt;p&gt;💡 What some ready-to-use partitioner to replace the default partitioner?&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RoundRobinPartitioner&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UniformStickyPartitioner&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;💡 How can you write your own partitioner?&lt;/p&gt;

&lt;p&gt;Implement the ‘Partitioner’ interface&lt;/p&gt;

&lt;p&gt;💡 How can we add more metadata information in the message in Kafka producer?&lt;/p&gt;

&lt;p&gt;Use header&lt;/p&gt;

&lt;p&gt;💡 How can we limit the Kafka producer quota?&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;quota.producer.default=2M
quota.producer.override=&quot;clientA:4M,clientB:10M&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Tue, 01 Aug 2023 03:00:00 +0800</pubDate>
        <link>http://yulu.github.io/blog1/2023/08/01/kafka-notes-chapter3.html</link>
        <guid isPermaLink="true">http://yulu.github.io/blog1/2023/08/01/kafka-notes-chapter3.html</guid>
        
        <category>reading_notes</category>
        
        <category>Kafka</category>
        
        
        <category>blog1</category>
        
      </item>
    
      <item>
        <title>[Kafka Guide] Chapter 2 - Install Kafka</title>
        <description>&lt;h3 id=&quot;hands-on&quot;&gt;Hands-on&lt;/h3&gt;

&lt;h5 id=&quot;running-on-aws-ec2-ubuntu-t2micro&quot;&gt;Running on AWS EC2 (ubuntu, t2.micro)&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;ssh to server
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; yulu-mac.pem ubuntu@xx.xx.xx.xx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;install java 11
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get update
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;openjdk-11-jre-headless
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;install zookeeper
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://dlcdn.apache.org/zookeeper/zookeeper-3.5.10/apache-zookeeper-3.5.10-bin.tar.gz
tar -zxf apache-zookeeper-3.5.10-bin.tar.gz
mv apache-zookeeper-3.5.10-bin /usr/local/zookeeper
mkdir -p /var/lib/zookeeper
cp &amp;gt; /usr/local/zookeeper/conf/zoo.cfg &amp;lt;&amp;lt; EOF
&amp;gt; tickTime=2000
&amp;gt; dataDir=/var/lib/zookeeper
&amp;gt; clientPort=2181
&amp;gt; EOF
export JAVA_HOME=/usr/java/jdk-11.0.10
/usr/local/zookeeper/bin/zkServer.sh start
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;install kafka and config the heap (since t2.micro has only 1G memory)
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://downloads.apache.org/kafka/3.4.0/kafka_2.13-3.4.0.tgz
tar -zxf kafka_2.13-3.4.0.tgz
mv kafka_2.13-3.4.0 /usr/local/kafka
mkdir /tmp/kafka-logs
export JAVA_HOME=/usr/java/jdk-11.0.10
export KAFKA_HEAP_OPTS=&quot;-Xmx256M -Xms128M&quot;
/usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;running-on-mac&quot;&gt;Running on Mac&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;install
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;brew &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;kafka
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;start zookeeper
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/opt/homebrew/bin/zookeeper-server-start /opt/homebrew/etc/zookeeper/zoo.cfg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;start kafka
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/opt/homebrew/bin/kafka-server-start /opt/homebrew/etc/kafka/server.properties
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;test-the-kafka-broker-is-working&quot;&gt;Test the Kafka broker is working&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;send from producer console
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kafka-console-producer &lt;span class=&quot;nt&quot;&gt;--broker-list&lt;/span&gt; localhost:9092 &lt;span class=&quot;nt&quot;&gt;--topic&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; send first message
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; send second message
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; wow it is working
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;consume by the consumer console
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kafka-console-consumer &lt;span class=&quot;nt&quot;&gt;--bootstrap-server&lt;/span&gt; localhost:9092 &lt;span class=&quot;nt&quot;&gt;--topic&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--from-beginning&lt;/span&gt;
send first message
send second message
wow it is working
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;configuring-the-broker&quot;&gt;Configuring the Broker&lt;/h3&gt;

&lt;h5 id=&quot;general-broker-parameters&quot;&gt;General Broker Parameters&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broker.id&lt;/code&gt;: every Kafka broker must have an integer identifier, which is set using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broker.id&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;listener&lt;/code&gt;: A listener is defined as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;protocol&amp;gt;://&amp;lt;hostname&amp;gt;:&amp;lt;port&amp;gt;&lt;/code&gt; e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PLAINTEXT://localhost:9092,SSL://:9091&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zookeeper.connect&lt;/code&gt;: The location of the ZooKeeper used for storing the broker metadata is set using the zookeeper.connect configuration parameter. The format for this parameter is a semicolon-separated list of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hostname:port/path&lt;/code&gt; strings
    &lt;ul&gt;
      &lt;li&gt;hostname: the hostname or IP address of the ZooKeeper server&lt;/li&gt;
      &lt;li&gt;port: the client port number for the server&lt;/li&gt;
      &lt;li&gt;/path: an optional ZooKeeper path to use as a chroot environment for the Kafka cluster. If it is omitted, the root path is used&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Why use a Chroot path?&lt;/p&gt;

  &lt;p&gt;It is generally considered to be good practice to use chroot path for the Kafka cluster. This allows the ZooKeeper ensemble to be shared with other applications, including other Kafka clusters, without a conflict. It is also best to specify multiple ZooKeeper servers in this configuration. This allows the Kafka broker to connect to another member of the ZooKeeper ensemble in the event of server failure&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.dirs&lt;/code&gt;: log segments are stored in the directory specified in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.dir&lt;/code&gt;. For multiple directories, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.dirs&lt;/code&gt; is preferable. Kafka broker will store partitions on them in a “least-used” fashion, with one partition’s log segments stored within the same path.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num.recovery.threads.per.data.dir&lt;/code&gt;: kafka uses a configurable pool of threads for handling log segments. By default, only one thread per log directory is used. The configurable parameter sets the number of threads per directory&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto.create.topics.enable&lt;/code&gt;: the default Kafka configuration specifies that the broker should automatically create a topic under some circumstances. However if you are managing topic creation explicitly, you can set this parameter to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto.leader.rebalance.enable&lt;/code&gt;: in order to ensure a Kafka cluster doesn’t become unbalanced by having all topic leadership on one broker, this config can be specified to ensure leadership is balanced as much as possible.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;delete.topic.enable&lt;/code&gt;: depending on your environment and data retention guidelines, you may wish to lock down a cluster to prevent arbitrary deletions of topics. Disabling topic deletion can be set by setting this flag to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h5 id=&quot;topic-defaults-parameters&quot;&gt;Topic Defaults Parameters&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num.partitions&lt;/code&gt;: how many partitions a new topic is created with.
    &lt;ul&gt;
      &lt;li&gt;keep in mind that the number of partitions for a topic can only be increased, never decreased.&lt;/li&gt;
      &lt;li&gt;many users will have the partition count for a topic be equal to, or a multiple of, the number of brokers in the cluster.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default.replication.factor&lt;/code&gt;: If auto-topic creation is enabled, this configuration sets what the replication factor should be for new topics&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.retention.ms&lt;/code&gt;: the most common configuration for how long Kafka will retain messages is by time. The default is specified in the configuration file using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.retention.hours&lt;/code&gt; parameter. However, there’re two other parameters allowed: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.retention.minutes&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.retention.ms&lt;/code&gt;. All three of these control the same goal - &lt;strong&gt;the amount of time after which messages may be deleted&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.retention.bytes&lt;/code&gt;: another way to expire messages is based on the total number of bytes of messages retained.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.segment.bytes&lt;/code&gt;: once the log segment has reached to the size specified by this parameter, which defaults to 1GB, the log segment is closed and a new one is opened. Once a log segment has been closed, it can be considered for expiration.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.roll.ms&lt;/code&gt;: another way to control when log segments are closed. This parameter specifies the amount of time after which a log segment should be closed.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;min.insync.replicas&lt;/code&gt;: when configuring your cluster for data durability, setting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;min.insync.replicas&lt;/code&gt; to 2 ensures that at least two replicas are caught up and “in sync” with the producer. Refer to &lt;a href=&quot;/blog1/2023/04/10/ddia-5.html&quot;&gt;DDIA-Chapter5 replication&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;message.max.bytes&lt;/code&gt;: this limits the maximum size of a message that can be produced, defaults to 1 MB.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary-as-ankicard&quot;&gt;Summary as Ankicard&lt;/h3&gt;

&lt;p&gt;💡 What are the two simple commands to test a Kafka server?&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kafka-console-producer &lt;span class=&quot;nt&quot;&gt;--broker-list&lt;/span&gt; localhost:9092 &lt;span class=&quot;nt&quot;&gt;--topic&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;test
&lt;/span&gt;kafka-console-consumer &lt;span class=&quot;nt&quot;&gt;--bootstrap-server&lt;/span&gt; localhost:9092 &lt;span class=&quot;nt&quot;&gt;--topic&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--from-beginning&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;💡 What is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broker.id&lt;/code&gt; config?&lt;/p&gt;

&lt;p&gt;Every broker must have a integer identifier&lt;/p&gt;

&lt;p&gt;💡 What is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.dirs&lt;/code&gt; config?&lt;/p&gt;

&lt;p&gt;Where the log segments are stored in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.dir&lt;/code&gt;, for multiple directories, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.dirs&lt;/code&gt; are preferable.&lt;/p&gt;

&lt;p&gt;💡 How will Kafka broker store the partitions in multiple log directories if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.dirs&lt;/code&gt; is configured?&lt;/p&gt;

&lt;p&gt;Kafka will store the partitions in a “least-used” fashion&lt;/p&gt;

&lt;p&gt;💡 What configuration will prevent Kafka to automatically create topic by sending message to a non-existing topic?&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto.create.topics.enable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;💡 Which parameter configs how many partitions a new topic is created with?&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num.partitions&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;💡 Can the number of partitions for a topic be decreased?&lt;/p&gt;

&lt;p&gt;No, the number of partitions for a topic can only be increased&lt;/p&gt;

&lt;p&gt;💡 What is the replication factor configured by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default.replication.factor&lt;/code&gt;?&lt;/p&gt;

&lt;p&gt;It is the number of copies of data across several brokers. It should be greater than 1 to ensure the reliability&lt;/p&gt;

&lt;p&gt;💡 What is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;min.insync.replicas&lt;/code&gt; config and how does it ensure the reliability?&lt;/p&gt;

&lt;p&gt;You should set this to 2 at least to ensure that at least two replicas are caught up and “in sync” with the producer. This enables the semi-sync replication strategy.&lt;/p&gt;

&lt;p&gt;💡 How does &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.retention.ms&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.retention.bytes&lt;/code&gt; works differently as Kafka’s retention policies?&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.retention.ms&lt;/code&gt; is the most common retention policy - for how long the Kafka will retain the message, it indicates the amount of time after which messages may be deleted.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log.retention.bytes&lt;/code&gt; indicates once the log segment has reached to the size specified by this parameter (defaults to 1GB), the log segment is closed and a new one is opened. Once a log segment has been closed, it can be considered expiration.&lt;/p&gt;
</description>
        <pubDate>Tue, 01 Aug 2023 03:00:00 +0800</pubDate>
        <link>http://yulu.github.io/blog1/2023/08/01/kafka-notes-chapter2.html</link>
        <guid isPermaLink="true">http://yulu.github.io/blog1/2023/08/01/kafka-notes-chapter2.html</guid>
        
        <category>reading_notes</category>
        
        <category>Kafka</category>
        
        
        <category>blog1</category>
        
      </item>
    
      <item>
        <title>[Kafka Guide] Chapter 1 - Meet Kafka</title>
        <description>&lt;h3 id=&quot;what-problems-does-kafka-solve&quot;&gt;What Problems does Kafka Solve&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;h5 id=&quot;how-kafka-comes-in-to-save-the-data-transportation-problems-here&quot;&gt;How Kafka comes in to save the data transportation problems here&lt;/h5&gt;

  &lt;p&gt;Apache Kafka was developed as a publish/subscribe messaging system designed to solve the above problem - a distributing streaming platform.
A filesystem or database commit log is designed to provide a durable record of all transactions so that they can be replayed to consistently build the state of the system. Similarly, data within Kafka is stored durably, in order, and can be read deterministically. In addition, the data can be distributed within the system to provide additional protections against failures, as well as significant opportunities for scaling performance.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;direct-connection-inout-data-communications-between-services-is-difficult-to-trace&quot;&gt;Direct-connection in/out data communications between services is difficult to trace&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-chapter1-1.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;use-a-pubsub-pattern&quot;&gt;Use a Pub/Sub pattern&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-chapter1-2.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;multiple-publishsubscribe-systems-to-support-different-biz-use-cases&quot;&gt;Multiple publish/subscribe systems to support different biz use cases&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-chapter1-3.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;summary-as-ankicard&quot;&gt;Summary as Ankicard&lt;/h3&gt;

&lt;p&gt;💡 What problem does Kafka solve?&lt;/p&gt;

&lt;p&gt;Kafka is a pub/sub messaging system what is designed to decouple the business services by introducing the async communication and group the communication channel using topic. A typical Kafka architecture looks like this&lt;/p&gt;

&lt;p&gt;💡 What are the main 3 characteristics Kafka has?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Data within Kafka is stored durably&lt;/li&gt;
  &lt;li&gt;Data can be read deterministically (in-order)&lt;/li&gt;
  &lt;li&gt;Data can be distributed for reliability and scalability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 What is the use of the ‘key’ in Kafka?&lt;/p&gt;

&lt;p&gt;Key is an optional metadata comes with the message, it will be hashed and used to decide which partition the message goes. Kafka brokers make sure that message comes with the same key goes to the same partition so they can be consumed in order. e.g. for mod-N partition, key is used to generate a consistent hash and used to select the partition number for that message so the same key are always written to the same partition&lt;/p&gt;

&lt;h3 id=&quot;basic-concepts&quot;&gt;Basic Concepts&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;message&lt;/strong&gt;:: the unit of data in Kafka, just an array of bytes as far as Kafka concerned&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;key&lt;/strong&gt;:: an optional piece of metadata of the message. Keys are used when messages are to be written to partitions in a more controlled manner. e.g. for mod-N partition, key is used to generate a consistent hash and used to select the partition number for that message so the same key are always written to the same partition&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;batch&lt;/strong&gt;:: a collection of messages - messages are written into Kafka in batches&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;schemas&lt;/strong&gt;:: JSON, XML or Apache Avro, basically the serialisation protocol&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;topic&lt;/strong&gt;:: analogies for a topic are a database table or a folder in a filesystem - group the same concept data into a channel&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;partitions&lt;/strong&gt;:: topics are broken down into a number of partitions. A partition is a single log. Messages are written to it in an append-only fashion and are read in order from beginning to end. Partitions can be distributed and replicated for scalability and reliability.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-chapter1-4.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;producers&lt;/strong&gt;:: create new messages. By default, the producer will balance messages over all partitions of a topic evenly. In some cases the producer will direct messages to specific partitions, done using the message key.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;consumers&lt;/strong&gt;:: read messages. The consumer read messages in the order in which they were produced to each partition. It keeps track of the offset - a metadata with the message to keep track of which messages it has already consumed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;consumer group&lt;/strong&gt;:: one or more consumers that work together to consume a topic. The group ensures that each partition is only consumed by one member.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-chapter1-6.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;💡 What if there’s larger number of consumers (in a consume group) than the number of partitions?&lt;/p&gt;

  &lt;p&gt;Some of the consumers will be idle. Because one partition cannot be consumed by multiple consumers. 
&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-chapter4-4.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;broker&lt;/strong&gt;:: a single Kafka server. It receives messages from producers, write messages to storage on disk and services consumers. - A single broker can easily handle thousands of partitions and millions of messages per second.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;cluster&lt;/strong&gt;:: a cluster of brokers has a leader broker and followers brokers. All producers must connect to the leader in order to publish messages, but consumers may fetch from either the leader or one of the followers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/kafka/kafka-chapter1-7.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MirrorMaker&lt;/strong&gt;:: used for replicating data to other clusters in a multi-cluster setting. The replication mechanisms within the Kafka clusters are designed only to work within a single cluster, not between multiple clusters. The MirrorMaker tool helps to fulfil this requirement&lt;/p&gt;

</description>
        <pubDate>Tue, 01 Aug 2023 03:00:00 +0800</pubDate>
        <link>http://yulu.github.io/blog1/2023/08/01/kafka-notes-chapter1.html</link>
        <guid isPermaLink="true">http://yulu.github.io/blog1/2023/08/01/kafka-notes-chapter1.html</guid>
        
        <category>reading_notes</category>
        
        <category>Kafka</category>
        
        
        <category>blog1</category>
        
      </item>
    
      <item>
        <title>屎壳郎，外卖和马屁精</title>
        <description>&lt;p&gt;早上儿子在沙发看着他最近迷上的蚂蚁动画片，突然大声问我：“妈妈！屎壳郎英文怎么说？”。因为蚂蚁的死对头就是一种黑绿的小甲壳虫，有次瞄到电视里它们在滚黑乎乎的圆球球，我就随口说这是屎壳郎呀，其实自己也是瞎说。结果儿子就记下了。今天不知道他哪里又来了学习英语的热情。我哪知道屎壳郎英文哟，敷衍他不知道不知道。他穷追不舍：“妈妈查呀！拿手机查呀！”这又是他昨晚上新学的小技巧。昨晚和他玩橡皮泥，这小子突然和我飙英语，我们对话着我发现，哎呀，我不会橡皮泥英文呀。我很诚实地给他说：“妈妈不知道橡皮泥英文怎么说，我们来查一下吧。“于是拿出手机搜了一番：plasticine。他和我一起学了发音，还对照着网络图片确认了一下，他表示很满意。今天关于屎壳郎的学习是逃不掉了，打开手机和他学习了一下，忍着恶心看了满屏的屎壳郎的图，和动画片里还是有点像呢，看来我瞎猜得没错。然后这个小人就在沙发上自言自语道：where is bung beetle, where is bung beetle, bung beetle on fower(flower)。&lt;/p&gt;

&lt;p&gt;早饭时间和太石讨论，今天晚上有没有时间做饭，要不要点外买。突然在身后吃着早饭的儿子大叫：我要吃外卖！！我想这小子耳朵这么尖，又这么能耐，还知道外卖好吃要吃外卖。转头看他，他指着我盘子里的早餐继续喊：我要吃外卖，我要吃外卖！哎，那是烧麦好不好！他怎么一直分不清“烧麦”和“外卖”。喊了几嗓子之后，他意识到自己说错了，尴尬地一笑，小声说：呵呵，是烧麦。&lt;/p&gt;

&lt;p&gt;有天儿子和我在床上疯打。他突然咯咯地坏笑，听这个笑声就知道他又做了他认为的“坏事”。他边笑边说：“妈妈，马屁精，你的马屁精，嘿嘿嘿。”我一头雾水，说的啥呀，听不懂。他见我没什么回应，扬起小手，继续道：“妈妈你看，你的马屁精！”我这才发现他学我的样子把我扎头发的皮筋套在了手腕上。我才想起来，我们平时偶尔说他人小鬼大嘴巴甜，是个小马屁精。也不知道他怎么就把“皮筋”和“马屁精”弄混了。之后他一直叫我的皮筋为马屁精，因为觉得他那样太好玩儿了，我一直没有纠正他，要扎头发了就指使他去帮我把“马屁精”找出来，嘿嘿。&lt;/p&gt;

&lt;p&gt;太石去幼儿园接儿子，老师给他说儿子不会用嘴巴吹泡泡，让我们回家教教他。于是太石很积极地给他买了肥皂水和吹泡泡工具。回家教了好久，不停地告诉他要嘟嘟嘴吹气，嘟嘟嘴吹气，他终于开窍学会了。然后开启不停吹不停吹模式，我们都担心他要吹断气了。有天中午午睡，他怎么都不愿意睡下，磨磨蹭蹭。最后我生气了，说你去玩吧！他说他吹一会儿泡泡就睡。我让他自己去客厅了，量他也搞不出什么大动作。十分钟过去了，没有听到什么动静。我轻手轻脚地出去看他，结果看到一个孤独的小背影在阳台上卖力地吹泡泡，偷偷看了好久，他一直吹一直吹，一点没懈怠。吹了好久啊，他过来说困了，然后马上床倒头就睡了。&lt;/p&gt;

&lt;p&gt;带儿子打车，帮他系好安全，他有点不情不愿的。于是给他说，不系安全带可能会飞出去呢！他还有点不相信，我于是拿出手机搜出油管上的车祸视频给他看。后来又一次打车，刚上车坐好，我正给自己系安全，他突然在旁边抓狂起来，着急地说：“妈妈，给我系安全带呀，快呀！”我不慌不忙，叫他不要着急，我先系好了来。结果他小嘴瘪瘪，着急得快哭出来了：“你不给我系安全带我就要飞出去呢！！”。后来还有一次，外公给他说带他回国可以坐外公的车，他一脸严肃地问：“外公，你的车有安全带吗？”&lt;/p&gt;

&lt;p&gt;小孩子的模仿能力，观察力和想象力真的是很强大，看着一个小小人用他强大倔强的力量去认识这个世界，也是奇妙的经历。&lt;/p&gt;
</description>
        <pubDate>Thu, 15 Jun 2023 08:00:00 +0800</pubDate>
        <link>http://yulu.github.io/blog2/2023/06/15/growing-up.html</link>
        <guid isPermaLink="true">http://yulu.github.io/blog2/2023/06/15/growing-up.html</guid>
        
        <category>生活</category>
        
        
        <category>blog2</category>
        
      </item>
    
      <item>
        <title>[DDIA] Chapter 6 - Partition</title>
        <description>&lt;h3 id=&quot;chapter-summary&quot;&gt;Chapter Summary&lt;/h3&gt;

&lt;p&gt;Database partition or sharding is discussed in this chapter. Partitioning is a very basic concept in data storage engine. As when the data volume goes large, a single node will not be enough to host all the data, we have to distribute the data among multiple nodes. Different approaches for partitioning the primary key and the secondary indexes, rebalancing strategies and request routing strategies are discussed in detail in this chapter.&lt;/p&gt;

&lt;h3 id=&quot;mind-map&quot;&gt;Mind Map&lt;/h3&gt;

&lt;div class=&quot;mindmap-container&quot;&gt;
    &lt;iframe src=&quot;https://www.xmind.net/embed/7dePxB&quot; width=&quot;900px&quot; height=&quot;540px&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&quot;question-summary&quot;&gt;Question Summary&lt;/h3&gt;
&lt;h4 id=&quot;approach-for-partitioning&quot;&gt;Approach for partitioning&lt;/h4&gt;

&lt;p&gt;💡 What is “hot spot” in partitioning?&lt;/p&gt;

&lt;p&gt;A node with disproportionally high load&lt;/p&gt;

&lt;p&gt;💡 What is “skewed” in partitioning?&lt;/p&gt;

&lt;p&gt;Some partitions have more data or queries than others&lt;/p&gt;

&lt;p&gt;💡 What is the benefit and downside of key range partitioning?&lt;/p&gt;

&lt;p&gt;It supports the range queries easily, however some access patterns will result in hot spots&lt;/p&gt;

&lt;p&gt;💡 What is the benefit and downside of hash of key partitioning?&lt;/p&gt;

&lt;p&gt;Hash key of partitioning results more evenly distributed data over the partitions to avoid data skew and hot spot. However it makes the range queries challenging. The range queries needs to sent to all the partitions.&lt;/p&gt;

&lt;p&gt;💡 Describe the approach of partitioning secondary indexes by document&lt;/p&gt;

&lt;p&gt;It is also called local index. The secondary index of a particular partition is stored locally with the partition. When querying for the secondary index, the query needs to go to all the partitions&lt;/p&gt;

&lt;p&gt;💡 Describe the term-based partitioning of database with secondary indexes&lt;/p&gt;

&lt;p&gt;The term-based partitioning refer to the approach that the secondary indexes are stored and partitioned independent from the primary key&lt;/p&gt;

&lt;p&gt;💡 Why the document-based secondary index partitioning of database has expensive read queries?&lt;/p&gt;

&lt;p&gt;Document-based partitioning is also termed as local index. Because the index is local in each partition, so query to the index needs to be sent to and processed by all the partitions. That is why it is more expensive&lt;/p&gt;

&lt;p&gt;💡 What is the advantage and disadvantage of term-based secondary index partitioning?&lt;/p&gt;

&lt;p&gt;Term-based partitioning is more efficient in indexing query, as the index are globally partitioned. However the write become more challenging especially to maintain a synchronous update of the secondary index with write process. The distributed transaction is difficult to achieve in this case&lt;/p&gt;

&lt;h4 id=&quot;rebalancing&quot;&gt;Rebalancing&lt;/h4&gt;
&lt;p&gt;💡  What is “rebalancing”?&lt;/p&gt;

&lt;p&gt;The process of moving load of data from one node in the cluster to another&lt;/p&gt;

&lt;p&gt;💡  What are the minimum requirement of a ‘rebalancing’?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;after the rebalancing, the data should be fairly distributed to the partitions&lt;/li&gt;
  &lt;li&gt;during the rebalancing, the read and write should not be interrupted&lt;/li&gt;
  &lt;li&gt;no more than necessary data should be moved during the rebalancing to make the rebalancing fast and minimize the network and disk I/O load&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;💡  In which cases a ‘rebalancing’ needs to be performed?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;the throughput to the data system increases that we need to add more CPUs to handle the requests&lt;/li&gt;
  &lt;li&gt;the dataset size increases that we need to add more disks or RAMs to host the data&lt;/li&gt;
  &lt;li&gt;some machine failed, that we need to move the data to another available machine&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;💡  What is the problem of mod N partitioning strategy?&lt;/p&gt;

&lt;p&gt;When the number of nodes N changes, most of the data needs to be moved from one node to another. The simple solution is to create many more partitions than the number of nodes, when a new node is added, it steels few partitions from other nodes until the partitions are fairly distributed once again.&lt;/p&gt;

&lt;p&gt;💡  Describe the fixed number of partition strategy&lt;/p&gt;

&lt;p&gt;The number of partitions are chosen at a number that is much larger than the number of nodes. When a new node is added, some selected partitions from each node is moved to the new node. This makes sure that only a small amount of data is moved&lt;/p&gt;

&lt;p&gt;💡  In the fixed number of partition approach, what is the drawback of partition volume is too large or too small?&lt;/p&gt;

&lt;p&gt;Since the number of partition is fixed, with the increase of dataset size, the size of each partition increase. If the size of partition become too large, rebalancing and recovery from a node failure become expensive. If the size is too small (the fixed number is chosen too large with small sized dataset), they incur too much overhead&lt;/p&gt;

&lt;p&gt;💡  What is the caveat of dynamic partitioning?&lt;/p&gt;

&lt;p&gt;An empty database starts off with a single partition and with small amount of data, no partition is performed yet. So all the request are handled by a single node while the other nodes sit idle. To mitigate this issue, we can have an initial set of partitions to be configured on an empty database&lt;/p&gt;

&lt;p&gt;💡  What is the advantage of dynamic partitioning?&lt;/p&gt;

&lt;p&gt;The number of partitions adapts to the total data volume. It avoids the partition size too large or too small issue in the fixed number partitioning strategy&lt;/p&gt;

&lt;h4 id=&quot;request-routing&quot;&gt;Request Routing&lt;/h4&gt;
&lt;p&gt;💡  What are the three common approaches of request routing in partitioned data system?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;contact any node and let the node handle the routes&lt;/li&gt;
  &lt;li&gt;add a routing tier that handles the routing strategy&lt;/li&gt;
  &lt;li&gt;let the client be aware of partitions and connect directly to the appropriate node&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;highlighted-topics&quot;&gt;Highlighted Topics&lt;/h3&gt;

&lt;h4 id=&quot;partition-strategy-comparison&quot;&gt;Partition Strategy Comparison&lt;/h4&gt;

&lt;h5 id=&quot;primary-key-partition-strategy-compare&quot;&gt;Primary key partition strategy compare&lt;/h5&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;pros&lt;/th&gt;
      &lt;th&gt;cons&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;key-range&lt;/td&gt;
      &lt;td&gt;more efficient to support range query&lt;/td&gt;
      &lt;td&gt;can result in hot spot or data skews&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;key-hash&lt;/td&gt;
      &lt;td&gt;evenly distribute the data and queries&lt;/td&gt;
      &lt;td&gt;less efficient in supporting range query&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Compound primary key is supported by Cassandra: only the first part of that key is hashed to determine the partition, but the other columns are used as a concatenated index for sorting the data in Cassandra’s SSTable. A query therefore cannot search for a range of values within the first column of a compound key, but if it specifies a fixed value for the first column, it can perform an efficient range scan over the other columns of the key.&lt;/p&gt;

&lt;p&gt;Similar for DynamoDB, a partition key and an optional sort key are required to generate its primary key. For dynamoDB, there’s the recommended pattern to design sort-key as composite sort keys to contain the hierarchical (one-to-many) relationships in your data. For example, you might structure the sort key as&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[country]#[region]#[state]#[county]#[city]#[neighborhood]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This would let you make efficient range queries for a list of locations at any one of these levels of aggregation, from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;country&lt;/code&gt; to a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;neighborhood&lt;/code&gt; and everything in between. (&lt;a href=&quot;https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-sort-keys.html&quot;&gt;ref&lt;/a&gt;)&lt;/p&gt;

&lt;h5 id=&quot;secondary-index-partition-strategy-compare&quot;&gt;Secondary index partition strategy compare&lt;/h5&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;pros&lt;/th&gt;
      &lt;th&gt;cons&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;document-base (local index)&lt;/td&gt;
      &lt;td&gt;write to the secondary index is in a single partition, easy to maintain the transaction&lt;/td&gt;
      &lt;td&gt;query to the secondary index needs to be sent to all partitions, hence less efficient&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;term-based (global index)&lt;/td&gt;
      &lt;td&gt;query to the secondary index is only from a single partition, more efficient&lt;/td&gt;
      &lt;td&gt;write to the secondary index are across partitions, challenging for transaction management&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Most of the databases follow document-base secondary index. DynamoDB supports both Global Secondary Index (GSI) and Local Secondary Index (LSI). DynamoDB GSI supports eventual consistency as stated. (&lt;a href=&quot;https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general.html&quot;&gt;ref&lt;/a&gt;)&lt;/p&gt;

&lt;h4 id=&quot;request-routing-1&quot;&gt;Request Routing&lt;/h4&gt;

&lt;p&gt;Three different request routing strategies:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;allow clients to contact any node (e.g. via a round-robin load balancer). If that node coincidentally owns the partition to which the request applies, it can handle the request directly, otherwise it forwards the request to the appropriate node, receives the reply and passes the reply to the client&lt;/li&gt;
  &lt;li&gt;Send all requests from clients to a routing tier first, which determines the node that should handle each request and forwards it accordingly. This routing tier does not itself handle any requests; it only acts as a partition-aware load balancer.&lt;/li&gt;
  &lt;li&gt;Require that clients to be aware of the partitioning and the assignment of partitions to nodes. In this case, a client can connect directly to the appropriate node, without any intermediary.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-6/ddia_6_partition_request_routing.excalidraw.png&quot; alt=&quot;ddia_6_partition_request_routing.excalidraw&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The challenge of request routing is the consensus among the distributed system about the changes in the assignment of partitions to nodes. Coordination service such as Zookeeper and gossip protocol among nodes are some commons approaches. Below is an illustration of zookeeper as a coordination service to sync among the routing tier and the nodes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-6/ddia_6_request_routing_zookeeper.excalidraw.png&quot; alt=&quot;ddia_6_request_routing_zookeeper.excalidraw&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is a summary of request routing strategies of popular databases&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;data storages&lt;/th&gt;
      &lt;th&gt;Solution&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;HBase, SolrCloud, Kafka&lt;/td&gt;
      &lt;td&gt;zookeeper as the coordination service&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MongoDB&lt;/td&gt;
      &lt;td&gt;its own config server and monogos daemons as the routing tier&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cassandra and Riak&lt;/td&gt;
      &lt;td&gt;use gossip protocol among the nodes, put the complexity int the nodes, avoid external coordination service dependency&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CouchBase&lt;/td&gt;
      &lt;td&gt;routing tier which learns the routing changes from the cluster nodes, not support auto rebalance&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;extended-topics&quot;&gt;Extended Topics&lt;/h3&gt;

&lt;h4 id=&quot;consistent-hashing---dynamo-style-db-partition-strategy&quot;&gt;Consistent Hashing - Dynamo-style DB partition strategy&lt;/h4&gt;

&lt;p&gt;Consistent hashing is useful for&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sharding: minimal rebalancing of partitions while adding or removing nodes&lt;/li&gt;
  &lt;li&gt;load balancing: better cache performance if same requests go to same servers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Overview&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use a hash function and map its range to a ring&lt;/li&gt;
  &lt;li&gt;Map n nodes to the same ring using the same or another hash function&lt;/li&gt;
  &lt;li&gt;Each physical node is split into k slices through multiple hash functions: Num virtual &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nodes = n*k&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;To write data: move clockwise on the ring from hash of data’s key to the nearest virtual node and write data to the physical node represented by that virtual node&lt;/li&gt;
  &lt;li&gt;To add server: when a server &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;si&lt;/code&gt; is added, the affected range starts from server &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;si&lt;/code&gt; and moves anticlockwise around the ring until a server &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sj&lt;/code&gt; is found. Thus, keys located between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;si&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sj&lt;/code&gt; need to be redistributed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;si&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;To remove node: when a server &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;si&lt;/code&gt; is removed, the affected range starts from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;si&lt;/code&gt; and moves anticlockwise around the ring until a server &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sj&lt;/code&gt; is found. Thus keys located between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sj&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;si&lt;/code&gt; needs to be redistributed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;si+1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-6/ddia_6_partition_consistent_hashing.excalidraw.png&quot; alt=&quot;ddia_6_partition_consistent_hashing.excalidraw&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;further-reading&quot;&gt;Further Reading&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html&quot;&gt;Amazon DynamoDB best practices&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Wed, 12 Apr 2023 10:00:00 +0800</pubDate>
        <link>http://yulu.github.io/blog1/2023/04/12/ddia-6.html</link>
        <guid isPermaLink="true">http://yulu.github.io/blog1/2023/04/12/ddia-6.html</guid>
        
        <category>reading_notes</category>
        
        <category>system_design</category>
        
        
        <category>blog1</category>
        
      </item>
    
      <item>
        <title>[DDIA] Chapter 5 - Replication</title>
        <description>&lt;h3 id=&quot;chapter-summary&quot;&gt;Chapter Summary&lt;/h3&gt;

&lt;p&gt;This chapter is the start of the three related topics regarding storage systems - replication, partition and transaction. Replication is important because of these important properties it provides: 1) keep the data geographically close to the users 2) allow system to continue to work even if some of its parts have failed (redundancy) 3) allows scaling out to serve the increasing throughput.&lt;/p&gt;

&lt;p&gt;Three common patterns, namely single-leader, multi-leader and leaderless are discussed. In the single-leader pattern, the challenge is mainly the follower set-up and replication lag. For multi-leader and leaderless pattern, the main challenge is the write conflict detection and resolving. This chapter discussed the definition of the patterns, the challenges and various solutions in detail.&lt;/p&gt;

&lt;h3 id=&quot;mind-map&quot;&gt;Mind Map&lt;/h3&gt;

&lt;div class=&quot;mindmap-container&quot;&gt;
    &lt;iframe src=&quot;https://www.xmind.net/embed/itczkd&quot; width=&quot;900px&quot; height=&quot;540px&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&quot;question-summary&quot;&gt;Question Summary&lt;/h3&gt;

&lt;h4 id=&quot;single-leader&quot;&gt;Single-leader&lt;/h4&gt;
&lt;p&gt;💡 What are synchronous replication and asynchronous replication?&lt;/p&gt;

&lt;p&gt;Synchronous replication means that when write happens, the successful response to the client will only be made after copying the data to all the followers.
Asynchronous replication means that the successful response is immediately returned to client when a write is saved to the leader. The copying from leader to followers will happen in background asynchronously.&lt;/p&gt;

&lt;p&gt;💡 What is semi-synchronous replication and why it is good?&lt;/p&gt;

&lt;p&gt;In practice that synchronous replication is impractical because one unavailable node will block all the writes. Semi-synchronous refer to the way that the data write to the leader, is synchronously copied to 1 follower, and then copied to other followers asynchronously. This ensures that at any time, at least two nodes have the up-to-date copy of the data.&lt;/p&gt;

&lt;p&gt;💡 What is the trade-off of async replication?&lt;/p&gt;

&lt;p&gt;Weak durability. In the case that a leader fails before all the data is copied to followers, there will be data lost.&lt;/p&gt;

&lt;p&gt;💡 What are the 4 steps to setup a new follower?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;take a consistent snapshot of the leader’s database at some point in time&lt;/li&gt;
  &lt;li&gt;copy the snapshot to the new follower node&lt;/li&gt;
  &lt;li&gt;the follower connects to the leader to request all the data change since the snapshot was taken&lt;/li&gt;
  &lt;li&gt;caught up the change, which is to process the backlog of data changes since the snapshot&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 What is the log sequence number in PostgreSQL and binlog coordinate in MySQL?&lt;/p&gt;

&lt;p&gt;These two are the same concept, refers to the exact position in the leader’s replication log that the snapshot of the database is associated with.&lt;/p&gt;

&lt;p&gt;💡 What are the 3 steps for an automatic failover process?&lt;/p&gt;

&lt;p&gt;Step1: determine the leader has failed, normally use a ping timeout
Step2: choose a new leader through an election process
Step3: reconfiguring the system to use the new leader&lt;/p&gt;

&lt;p&gt;💡 Why manual operation to handle leader failover is preferred?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;automatically handle unreplicated writes in failed leader is tricky. Simple discarding might cause serious consequences&lt;/li&gt;
  &lt;li&gt;split brain, which means that multiple leaders running at the same time may occur if the failover mechanism is not carefully designed&lt;/li&gt;
  &lt;li&gt;if the timeout is not properly decided, false alarm may occur and could result in an unnecessary failover&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 What is split brain?&lt;/p&gt;

&lt;p&gt;More than one leader is running in a single leader replication architecture. If there’s no process to handle write conflict, it will result in data lost or corruption.&lt;/p&gt;

&lt;p&gt;💡 What are the four leader-to-follower replication methods?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;statement-based&lt;/li&gt;
  &lt;li&gt;write-ahead log shipping&lt;/li&gt;
  &lt;li&gt;logical (row-based) log&lt;/li&gt;
  &lt;li&gt;trigger-based&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 In a statement-based replication approach, what conditions may break the replication?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Any statement use nondeterministic function (NOW(), RAND())&lt;/li&gt;
  &lt;li&gt;Statement use auto-incrementing column&lt;/li&gt;
  &lt;li&gt;Statements that have side effects (e.g. triggers, stored procedures, user-defined functions)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;💡 What is the disadvantage of write-ahead log shipping replication approach?&lt;/p&gt;

&lt;p&gt;The log data is stored in a very low level, which makes the replication closely coupled to the storage engine. Normally the log cannot be consumed by different versions of the engine, which makes the zero-down time version upgrade challenging.&lt;/p&gt;

&lt;p&gt;💡 What are the different ways to solve replication lag issue?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;reading your own writes (or read-after-write consistency): 1) serve the often-modified entry by leader only 2) serve by follower only after some time passed the write (e.g. 1 mins later after update) 3) record the update timestamp and use the timestamp to check if value exists&lt;/li&gt;
  &lt;li&gt;monotonic reads: the same user always reads from the same replica&lt;/li&gt;
  &lt;li&gt;consistent prefix reads: if any writes happens in a certain order, then anyone reading those writes will see them appear in the same order&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;multi-leader&quot;&gt;Multi-leader&lt;/h4&gt;
&lt;p&gt;💡 What are the ways (name 4) of achieving convergent conflict resolution?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;last write wins approach:&lt;/li&gt;
  &lt;li&gt;higher-numbered replica wins:&lt;/li&gt;
  &lt;li&gt;somehow merge the values (e.g. concatenate strings):&lt;/li&gt;
  &lt;li&gt;record the conflict in an explicit data structure that preserves all the information and ask for user to resolve at some later time:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 What are the commonly seen multi-leader replication topologies?&lt;/p&gt;

&lt;p&gt;Circular topology, star topology and all-to-all topology&lt;/p&gt;

&lt;p&gt;💡 Why all-to-all topology is better than circular and star topology?&lt;/p&gt;

&lt;p&gt;Avoid the single point of failure&lt;/p&gt;

&lt;p&gt;💡 What is the replication topology in multi-leader replication architecture?&lt;/p&gt;

&lt;p&gt;A replication topology describe the communication paths along which writes are propagated from one node to another&lt;/p&gt;

&lt;p&gt;💡 What is the causality issue of all-to-all topology and what is the solution to that?&lt;/p&gt;

&lt;p&gt;Causality issue refer to the problem that some operations does not arrive to the system in the order expected due to the network latency. So one operation that depends on the other might arrive earlier (e.g. an update statement comes before an insert of the item). Version vector is the solution to the causality issue&lt;/p&gt;

&lt;h4 id=&quot;leaderless&quot;&gt;Leaderless&lt;/h4&gt;
&lt;p&gt;💡 How does an unavailable node catch up on the writes when it comes back online?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;read repair: the client fetches the data during read and detect the stale value from the nodes, it then write the update-to-date back to the nodes&lt;/li&gt;
  &lt;li&gt;anti-entropy process: a background process that constantly looks for differences in the data between replicas and copies any missing data from one replica to another&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 What is the benefit of choosing r and w to be majority (more than n/2) of nodes?&lt;/p&gt;

&lt;p&gt;This choice ensures w+r&amp;gt;n (quorum condition) while still tolerating up to n/2 node failures&lt;/p&gt;

&lt;p&gt;💡 If a workload pattern is few writes and many reads, what is better config for the quorum and why?&lt;/p&gt;

&lt;p&gt;Reduce the r, say to 1, increase the w, say to n (so r + w &amp;gt; n holds). In this case, we can reduce the load of read. However increase w to n has the disadvantage that a single unavailable nodes will cause write fall.&lt;/p&gt;

&lt;p&gt;💡 Define strict quorum reads and writes&lt;/p&gt;

&lt;p&gt;Define the number of nodes that confirms writes are w, the number of nodes required for read is r, and the total number of nodes is n. When w + r &amp;gt; n, it guarantees that at least one of the nodes for read contains the latest updated writes.&lt;/p&gt;

&lt;p&gt;💡 Define sloppy quorum&lt;/p&gt;

&lt;p&gt;In a large cluster with significantly more than n nodes, writes and reads still require w and r successful responses, but those may include nodes that are not among the designated n “home” nodes for a value.&lt;/p&gt;

&lt;h3 id=&quot;highlighted-topics&quot;&gt;Highlighted Topics&lt;/h3&gt;

&lt;h4 id=&quot;quorums-condition&quot;&gt;Quorums Condition&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;Leaderless architecture becomes popular after Amazon used it for its in-house &lt;em&gt;Dynamo&lt;/em&gt; system (the paper is written in 2007). &lt;a href=&quot;https://github.com/basho/riak&quot;&gt;Riak&lt;/a&gt;, &lt;a href=&quot;https://github.com/apache/cassandra&quot;&gt;Cassandra&lt;/a&gt; and &lt;a href=&quot;https://github.com/voldemort/voldemort&quot;&gt;Voldemort&lt;/a&gt; are open source datastores with leaderless replication models inspired by Dynamo, so this kind of database is also known as &lt;em&gt;Dynamo-style&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Quorums is the essential concept to guarantee the data consistency in Dynamo-style data storages. It is defined as this:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;If there are n replicas, every write must be confirmed by w nodes to be considered successful, and we must query at least r nodes fro each read. As long as w + r &amp;gt; n, we expect to get an up-to-date value when reading, because at least one of the r nodes we’re reading from must be up to date. Reads and writes that obey these r and w values are called quorum reads and writes&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-5/ddia-quorum.excalidraw.png&quot; alt=&quot;ddia-quorum.excalidraw&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can smartly choose w and r in different use cases:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;to a write intensive use case, we can choose a smaller w (but a larger r) to reduce the write latencies. Vice versa for read intensive use case&lt;/li&gt;
  &lt;li&gt;generally we can choose w &amp;gt; n/2 and r &amp;gt; n/2 so the system can tolerate up to n/2 nodes failures&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However even with w + r &amp;gt; n, there are likely to be edge cases where stale values are returned. Dynamo-style databases are generally optimized for use cases that can tolerate eventual consistency (BASE vs ACID).&lt;/p&gt;

&lt;h3 id=&quot;extended-topics&quot;&gt;Extended Topics&lt;/h3&gt;
&lt;h4 id=&quot;version-vector&quot;&gt;Version Vector&lt;/h4&gt;

&lt;p&gt;Consider two operations A and B happens at very similar time, there’s only three different possibilities in terms of the which happens first:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A happens before B and B depends on A’s occurrence&lt;/li&gt;
  &lt;li&gt;B happens before A and A depends on B’s occurrence&lt;/li&gt;
  &lt;li&gt;They happen at the same time and there’s no causality relationship between them&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we can identify the causality between A and B, then we can say that A and B are not concurrent and we can safely keep the result from the most recent operation. Otherwise, we want to keep the conflicted result from the concurrent operations A and B, and let the client to resolve the conflict (In simpler cases, we can take the “later” operation based on the timestamp or replica number order, but it comes with the risk of data loss).&lt;/p&gt;

&lt;p&gt;To detect the concurrency, we can take the approach to detect the causality, if no causality, means that the operations are concurrent.&lt;/p&gt;

&lt;p&gt;A simple algorithm to capture the happens-before relationship&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The server maintains a version number for every key, increments the version number every time that key is written, and stores the new version number along with the value written&lt;/li&gt;
  &lt;li&gt;When a client reads a key, the server returns all values that have not been overwritten, as well as the latest version number. A client must read a key before writing&lt;/li&gt;
  &lt;li&gt;When a client writes a key, it must include the version number from the prior read, and it must merge together all values that it received in the prior read&lt;/li&gt;
  &lt;li&gt;When the server receives a write with a particular version number, it can overwrite all values with the that version number or below, but it must keep all values with a higher version number (because those values are concurrent with the incoming write)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Important aspect to consider when reasoning about the scalability of these version vector is the existence of three different orders of magnitude at play:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;a &lt;strong&gt;small number&lt;/strong&gt; of replica nodes for each key&lt;/li&gt;
  &lt;li&gt;a &lt;strong&gt;large number&lt;/strong&gt; of server nodes&lt;/li&gt;
  &lt;li&gt;a &lt;strong&gt;huge number&lt;/strong&gt; of clients, keys and issued operations. 
Thus a scalable solution should avoid mechanisms that are linear with the highest magnitude and, if possible, even try to match the lowest scale&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here’s a summary and comparison of different version vector approaches&lt;/p&gt;

&lt;h5 id=&quot;version-vector-with-causal-histories&quot;&gt;Version vector with causal histories&lt;/h5&gt;
&lt;p&gt;causal histories ({a1, a2}) are maintained in this version vector solution, but this is a &lt;strong&gt;not useful in practical systems&lt;/strong&gt; because they scale linearly with the number of updates.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-5/version-vector_1.excalidraw.png&quot; alt=&quot;version-vector_1.excalidraw&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;causally-compliant-total-order-lww&quot;&gt;Causally compliant total order (LWW)&lt;/h5&gt;
&lt;p&gt;Assuming that client clocks are well synchronised and applying real time clock order, in this approach, replica nodes never store multiple versions and writes do not need to provide a get context. Cassandra is using this simplest approach.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-5/version-vector-2.excalidraw.png&quot; alt=&quot;version-vector-2.excalidraw&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;version-vectors-with-per-server-entry&quot;&gt;Version vectors with per-server entry&lt;/h5&gt;
&lt;p&gt;A concise version vector can be achieved by storing all the update-to-date server+version information with each replica. In this case, it is possible to track the causality among updates that were received in different servers. However, this approach cannot track causality among updates submitted to the same server. Dynamo system uses one entry per replica node and thus falls into this category.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-5/version-vector-3.excalidraw.png&quot; alt=&quot;version-vector-3.excalidraw&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;version-vector-with-per-client-entry&quot;&gt;Version vector with per-client entry&lt;/h5&gt;
&lt;p&gt;The version vectors with one entry per replica node are not enough to track causality among concurrent clients. One natural approach is to track causality by using version vectors with one entry per client. In the implementation, each client needs to maintain a counter increment it and provide it in each PUT, otherwise we will encountered the problem illustrated below. This approach is not very practical mainly because it is a big challenge to maintain the clients stability of the storage system - the application can run concurrent and dynamic threads, it is difficult to the identify and track them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-5/version-vector-4.excalidraw.png&quot; alt=&quot;version-vector-4.excalidraw&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;dotted-version-vectors&quot;&gt;Dotted version vectors&lt;/h5&gt;
&lt;p&gt;Dotted version vectors use a concise and accurate representation for the clocks to be used as a substitute for the classic version vectors. It uses only server-based ids and only a component per replica node, thus avoiding the space consumption explosion. The formal definition of the version vector is&lt;/p&gt;

\[\begin{align*}
&amp;amp;C[(r, m)] = \{r_i | 1 \leq i \leq m\}, \\
\\
&amp;amp;C[(r,m,n)] = \{r_i| 1 \leq i \leq m\} \cup \{r_n\}, \\
\\
&amp;amp;C[X] = \bigcup_{x\in X}C[x] \\
\\
&amp;amp;\text{In a component (r,m,n) we will always have n &amp;gt; m}
\end{align*}\]

&lt;p&gt;The example shows how the version vectors are updated for each update:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-5/version-vector-5.excalidraw.png&quot; alt=&quot;version-vector-5.excalidraw&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we see, the popular databases like dynamoDB and Casandra are using simpler version of the version vectors in practice. This is very common. In reality, production-level engineering system comprises complexities for performance and engineering simplicity.&lt;/p&gt;

&lt;h3 id=&quot;further-reading&quot;&gt;Further Reading&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.purdue.edu/homes/bb/cs542-11Spr/Parker_TSE83.pdf&quot;&gt;Detection of Mutual Inconsistency in Distributed Systems - 1983 classic paper of version vector&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1011.5808.pdf&quot;&gt;Dotted Version Vectors: Logical Clocks for Optimistic Replication&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf&quot;&gt;Dynamo: Amazon’s Highly Available Key-value Store - 2007 classic paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 10 Apr 2023 10:00:00 +0800</pubDate>
        <link>http://yulu.github.io/blog1/2023/04/10/ddia-5.html</link>
        <guid isPermaLink="true">http://yulu.github.io/blog1/2023/04/10/ddia-5.html</guid>
        
        <category>reading_notes</category>
        
        <category>system_design</category>
        
        
        <category>blog1</category>
        
      </item>
    
      <item>
        <title>[DDIA] Chapter 4 - Encoding and Evolution</title>
        <description>&lt;h3 id=&quot;chapter-summary&quot;&gt;Chapter Summary&lt;/h3&gt;
&lt;p&gt;Services communicate to each other by protocols (pre-defined interfaces and data models). As application developers, we are very often making API design choices by choosing between HTTP or gPRC, or REST vs RPC vs GraphQL, or Web socket vs HTTP etc. The definitions of these terms are sometimes misunderstood a lot (at least by me for a long time). These terms are actually not mutual-exclusive.&lt;/p&gt;

&lt;p&gt;This chapter provides some fundamental knowledge on data encoding (serialisation): different mechanisms of turning data structures into bytes for for communicating over the network or storing on disk. The focus is on the compatibility properties of the different encoding strategies: how easily the backward and forward compatibility can each solution provide. At the later part of the chapter, how data encoding plays a role in the three common dataflow in application design namely data store, sync API communication and async message queue is discussed. Understanding the underlie mechanism of data encoding helps us to make better tech decisions when designing and implementing application softwares.&lt;/p&gt;

&lt;h3 id=&quot;mind-map&quot;&gt;Mind Map&lt;/h3&gt;

&lt;div class=&quot;mindmap-container&quot;&gt;
    &lt;iframe src=&quot;https://www.xmind.net/embed/fUAvvp&quot; width=&quot;900px&quot; height=&quot;540px&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&quot;question-summary&quot;&gt;Question Summary&lt;/h3&gt;

&lt;p&gt;💡 1. Why we need to encode the data (translation between the in-memory data representation to a byte sequence that is sent over the network or sent to a file)?&lt;/p&gt;

&lt;p&gt;The data stored in memory (normally as list, object, hash tables, trees) is designed for efficient access by the CPU. While the data transferred through the network or written to a file is in a format of self-contained sequence of bytes, which is quite different from the data structure used in memory.&lt;/p&gt;

&lt;p&gt;💡 2. What are the problems of language specific encoding method? (e.g. java’s kryo, ruby’s marshal, python’s pickle)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;they are language specified and has to tie with the chosen language&lt;/li&gt;
  &lt;li&gt;most of the time the performance is not good enough&lt;/li&gt;
  &lt;li&gt;there’s no versioning control, backward compatibility and forward compatibility is not easy to maintain&lt;/li&gt;
  &lt;li&gt;security concerns: in order to restore the data in the same object type, the decoding process needs to be able to instantiate arbitrary class, which can be used by attackers to inject executable code&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 3. What are the pros and cons of textual format encoding method? (e.g. xml, json, csv)&lt;/p&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;human readable, easy to understand and debug&lt;/li&gt;
  &lt;li&gt;can be easily made to support backward and forward compatibility&lt;/li&gt;
  &lt;li&gt;language-independent&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;verbose&lt;/li&gt;
  &lt;li&gt;ambiguity around the encoding of numbers. Specific flaws for specific format. E.g.:   integer large than 2^53 cannot be parsed correctly by language uses floating-point numbers. JSON and XML does not support binary string&lt;/li&gt;
  &lt;li&gt;weak support for schema definition&lt;/li&gt;
  &lt;li&gt;CSV does not have schema and confusion to handle value with comma or newline&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 4. What are the benefits of binary encoding?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;clearly defined backward and forward compatibility semantics&lt;/li&gt;
  &lt;li&gt;efficient computation and result in compact size&lt;/li&gt;
  &lt;li&gt;type and schema definition, used as good documentation and code generation for statically typed languages&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 5. How do Thrift and Protocol Buffers handle schema changes while keeping backward and forward compatibility?&lt;/p&gt;

&lt;p&gt;The field tag is used to refer to each field and it is never changed. There will be new fields with new field tags added. To maintain the forward compatibility, the old code will simply ignore the unrecognized field tag. For backward compatibility, the new code can always read old data because the tag numbers still have the same meaning. The only detail is that you cannot make the new field as required, as the old code will not have written the new field you added.&lt;/p&gt;

&lt;p&gt;💡 6. What is the benefit of ‘repeat’ field type in Protocol Buffers?&lt;/p&gt;

&lt;p&gt;The encoding of ‘repeat’ field type says that the same field tag simply appears multiple times in the record. This has the nice effect that it is okay to change an optional field into a repeated field. New code reading old data sees a list with zero or one elements while old code reading new data sees only the last element of the list.&lt;/p&gt;

&lt;p&gt;💡 7. What are the three ways to communicate the writer’s schema to the reader when using Avro encoding?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;When the dataset volume is large, it is ok to contain the writer’s schema in the file itself and sent.&lt;/li&gt;
  &lt;li&gt;When there’s upgrading of versions frequently and each entry of data might be written with different versions of schema. It is helpful to maintain a database of version number of the writer’s schema. It is helpful to maintain a database of version number of the writer’s schema. Then send only the version number with the data to the reader. When the reader reads the data, it will retrieve the schema by the version number.&lt;/li&gt;
  &lt;li&gt;When two processes are communicating over a bidirectional network connection, they can negotiate the schema version on connection setup and then use the schema for the lifetime of the connection.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 8. How does Avro maintain the forward and backward compatibility?&lt;/p&gt;

&lt;p&gt;Avro maintains two set of schema - writer’s schema and reader’s schema. The schema resolution logic will check the compatibility and resolve the difference of the two. Adding or removing fields that has default value, we can maintain the compatibility.&lt;/p&gt;

&lt;p&gt;💡 9. Considering the dataflow through database, in which scenarios that we need to handle the backward compatibility and the forward compatibility?&lt;/p&gt;

&lt;p&gt;We need to consider both backward compatibility and forward compatibility. Backward compatibility needs to be handled by default, as the data written at a time, will be read at a later time. In the scenarios that a database is read and written by multiple instances, during rolling upgrade of the instances, the database may be written by a newer version of the code, and subsequently read by an older version of the code that is still running. Thus, forward compatibility is also often required for database.&lt;/p&gt;

&lt;p&gt;💡 10. What is data outlives code?&lt;/p&gt;

&lt;p&gt;Data in the database was written some time ago and might become incompatible with the newest version of code that reads and writes data.&lt;/p&gt;

&lt;p&gt;💡 11. What can schema evolution bring to the database dataflow?&lt;/p&gt;

&lt;p&gt;Schema evolution allows the entire database to appear as if it was encoded with a single schema, even though the underlying storage may contain records encoded with various historical versions of the schema&lt;/p&gt;

&lt;h3 id=&quot;highlighted-topics&quot;&gt;Highlighted Topics&lt;/h3&gt;

&lt;p&gt;Understand the four encoding algorithms with an example&lt;/p&gt;

&lt;p&gt;Sample data&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
	&quot;username&quot;: &quot;Martin&quot;,
	&quot;favoriteNumber&quot;: 1337,
	&quot;interest&quot;: [&quot;daydreaming&quot;, &quot;hacking&quot;]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;messagepack&quot;&gt;MessagePack&lt;/h4&gt;
&lt;p&gt;MessagePack has no schema, it is a binary encoding for JSON. The schema (keys) needs to be encoded in the message. So, the binary encoding is 66 bytes for this example, which is not much size reduction achieved here (original json is 81 bytes). It is not clear whether such a small space reduction is worth the loss of human-readability.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-4/encoding_messagepack.JPG&quot; alt=&quot;image_1&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;thrift-and-protocol-buffers&quot;&gt;Thrift and Protocol Buffers&lt;/h4&gt;
&lt;p&gt;Thrift and protocol buffers are similar in the sense that schema is defined and fixed with tag number. With the re-defined schema, the message size is significant reduced since only the schema tag is required in the message. An obvious difference between thrift and protocol buffers is how the array type data is defined: thrift support list data type while protocol buffers use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;repeated&lt;/code&gt; marker to indicate that a field can be a list: this has the nice effect that it’s okay to change an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;optional&lt;/code&gt; field into a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;repeated&lt;/code&gt; field. New code reading old data sees a list with zero or one elements; old code reading new data sees only the last element of the list.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-4/encoding_thrift.JPG&quot; alt=&quot;image_2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-4/encoding_protobuff.JPG&quot; alt=&quot;image_3&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;avro&quot;&gt;Avro&lt;/h4&gt;
&lt;p&gt;Avro is a subproject of Hadoop. It does not contains field tag in the schema. To parse the binary data, you go through the fields in the order that they appear in the schema and use the schema to tell you the datatype of each field. For Avro, the writer and reader maintains its own copy of schema, Avro library resolves the differences by looking at the writer’s schema and the reader’s schema side by side and translating the data from the writer’s schema into the reader’s schema.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-4/encoding_avro.JPG&quot; alt=&quot;image_4&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;extended-topics&quot;&gt;Extended Topics&lt;/h3&gt;

&lt;p&gt;A better understanding of the concepts:&lt;/p&gt;

&lt;h4 id=&quot;rest-vs-rpc&quot;&gt;REST vs RPC&lt;/h4&gt;
&lt;p&gt;REST: a philosophy that builds upon the principles of HTTP. It emphases on “resource-centric”. An API designed according to the principles of REST is called RESTful.&lt;/p&gt;

&lt;p&gt;RPC: first we shall differentiate the definition of RPC used in different context. RPC (remote procedure calls), the idea has been around since 1970s. The RPC model tries to make a request to a remote network service look the same as calling a function or method in your programming language, within the same process. Nowadays, we use RPC often to describe:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;protocols
    &lt;blockquote&gt;
      &lt;p&gt;Remote Procedure Call (RPC) is a protocol that provides the high-level communications paradigm used in the operating system. RPC presumes the existence of a low-level transport protocol, such as Transmission Control Protocol/Internet Protocol (TCP/IP) or User Datagram Protocol (UDP), for carrying the message data between communicating programs. RPC implements a logical client-to-server communications system designed specifically for the support of network applications. When RPC is made, the procedure identifier and parameters serialized into a request message then sent to the remote process which serves the call. On the remote process the message got deserialized back, the process run, and the results returned in a similar way.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;frameworks
    &lt;blockquote&gt;
      &lt;p&gt;such as &lt;a href=&quot;https://grpc.io/&quot;&gt;gRPC&lt;/a&gt;, &lt;a href=&quot;https://thrift.apache.org/&quot;&gt;Apache Thrift&lt;/a&gt;, &lt;a href=&quot;https://avro.apache.org/&quot;&gt;Apache Avro&lt;/a&gt;, &lt;a href=&quot;https://dubbo.apache.org/en/index.html&quot;&gt;Apache Dubbo&lt;/a&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;Philosophy of API pattern
    &lt;blockquote&gt;
      &lt;p&gt;The RPC API thinks in terms of “verbs”, exposing the restaurant functionality as function calls that accept parameters, and invokes these functions via the HTTP verb that seems most appropriate - a ‘get’ for a query, and so on, but the name of the verb is purely incidental and has no real bearing on the actual functionality, since you’re calling a different URL each time. Return codes are hand-coded, and part of the service contract.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When we compare REST and RPC, we could be comparing HTTP API design pattern - use RESTful and RPC-style design pattern (another pattern faded out is SOAP). Or we could be evaluating tech decision on protocols, whether we should use HTTP (json) or RPC (gRPC or other frameworks build on top of the protocols).&lt;/p&gt;

&lt;h4 id=&quot;http-vs-grpc&quot;&gt;HTTP vs gRPC&lt;/h4&gt;
&lt;p&gt;Explained above, we are comparing the json encoding and protocol buffers encoding&lt;/p&gt;

&lt;h4 id=&quot;graphql-vs-rest&quot;&gt;GraphQL vs REST&lt;/h4&gt;
&lt;p&gt;GraphQL is an application layer server-side technology that is used for executing queries with existing data. It is deployed over HTTP using a single endpoint that provides the full capabilities of the exposed service. It promotes a difference API design philosophy from the REST, it requires strong typed schema definition and supports schema stitching.&lt;/p&gt;

&lt;h4 id=&quot;websocket&quot;&gt;WebSocket&lt;/h4&gt;
&lt;p&gt;WebSocket connection is initiated by the client. It is bi-directional and persistent. It starts its life as a HTTP connection and could be “upgraded” via some well-defined handshake to a WebSocket connection. Through this persistent connection, a server could send updates to a client. WebSocket connections generally work even if a firewall is in place. This is because they use port 80 or 443 which are also used by HTTP/HTTPS connections.&lt;/p&gt;

&lt;p&gt;WebSocket is the default choice for the online messaging service. We could just skip the discussion of HTTP vs WebSocket in supporting peer-to-peer messaging and opt for WebSocket directly. It is not some fancy technology, as Keith Adams said in his slack presentation, it could be some 40 lines of java code wrapping a standard WebSocket library with a for loop over the people that are connected.&lt;/p&gt;

&lt;h4 id=&quot;further-reading&quot;&gt;Further reading&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://thrift.apache.org/static/files/thrift-20070401.pdf&quot;&gt;Thrift paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html&quot;&gt;A short and concise version of this chapter written by the same author 10 years ago&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 18 Sep 2022 10:00:00 +0800</pubDate>
        <link>http://yulu.github.io/blog1/2022/09/18/ddia-4.html</link>
        <guid isPermaLink="true">http://yulu.github.io/blog1/2022/09/18/ddia-4.html</guid>
        
        <category>reading_notes</category>
        
        <category>system_design</category>
        
        
        <category>blog1</category>
        
      </item>
    
      <item>
        <title>[DDIA] Chapter 3 - Storage and Retrieval</title>
        <description>&lt;h3 id=&quot;chapter-summary&quot;&gt;Chapter Summary&lt;/h3&gt;

&lt;p&gt;This is an interesting and exciting chapter! Finally it starts to talk about data systems. The discussion starts from the illustration of how data store can be done with a simple text file. Extending from the concept, a simple storage engine with appending-only log is introduced - hash index. The basic knowledge makes the understanding of the classic LSM-tree storage engine easy. Complement to the LSM-tree storage engine, another classic B-tree storage engine is discussed in detail.&lt;/p&gt;

&lt;p&gt;In the second section of the chapter, the difference between OLAP and OLTP databases is discussed. Diving deeper into the OLAP and data warehouse concept, column-oriented data storage engine is introduced.&lt;/p&gt;

&lt;h3 id=&quot;mind-map&quot;&gt;Mind Map&lt;/h3&gt;

&lt;div class=&quot;mindmap-container&quot;&gt;
    &lt;iframe src=&quot;https://www.xmind.net/embed/H5bX6h&quot; width=&quot;900px&quot; height=&quot;540px&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&quot;questions-summary&quot;&gt;Questions Summary&lt;/h3&gt;

&lt;p&gt;💡 1. How does Hash Indexes work?&lt;/p&gt;

&lt;p&gt;Key-value stores are used (similar to the hashmap data structure). The key is the index key and the value is byte offset of the value in the data file. The hash index is stored in memory for fast access. When reading the data from the index, we can read the byte offset of the key from the index, and we only need to load the value data from the disk with just one disk seek.
The data file is append only and compaction and segment merging are performed to reduce the storage&lt;/p&gt;

&lt;p&gt;💡 2. What is compaction (of a key-value update log)?&lt;/p&gt;

&lt;p&gt;For a log-structure database, compaction on the log file segments means throwing away duplicate keys in the log and keeping only the most recent update for each key.&lt;/p&gt;

&lt;p&gt;💡 3. How to handle partially written records in hash index?&lt;/p&gt;

&lt;p&gt;Use checksums to delete the corrupted parts of the log&lt;/p&gt;

&lt;p&gt;💡 4. How to handle the data deleting in hash index?&lt;/p&gt;

&lt;p&gt;Append a special deletion record to the data file (called tombstone). When log segments are merged, the tombstone tells the merging process to discard any previous value for the deleted key&lt;/p&gt;

&lt;p&gt;💡 5. What is checksums?&lt;/p&gt;

&lt;p&gt;A checksum is an alphanumeric value that uniquely represents the contents of a file. Checksums are often used to verify the integrity of files downloaded from external source.&lt;/p&gt;

&lt;p&gt;💡 6. What are the limitations of hash index?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the hash table must fit in memory, it does not work for data that has a very large number of keys&lt;/li&gt;
  &lt;li&gt;range queries are not support, you need to fetch data key one by one for a range of keys&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 7. What are the two ways to solve the issue of a key-value index where the key is not unique?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;making each value stored with the keys a list of matching row identifier&lt;/li&gt;
  &lt;li&gt;by appending a row identifier to the key to make the key unique&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 8. What is the reason that in-memory data storage performs faster than disk-based data storage?&lt;/p&gt;

&lt;p&gt;The overhead resides mainly in the data encoding and decoding process as data stored in very different structure in memory and in disk. Surprisingly the read speed to memory and disk is not the deciding factor although they are in huge difference. (Anyway, the disk-based storage could avoid read from disk given enough memory provided and data cached)&lt;/p&gt;

&lt;p&gt;💡 9. What is LSM tree?&lt;/p&gt;

&lt;p&gt;Log-structured Merge-tree. Storage engines that are based on the principle of &lt;strong&gt;merging and compacting sorted files&lt;/strong&gt; are often called LSM storage engines.&lt;/p&gt;

&lt;p&gt;💡 10. How does the LSM-tree storage engine works (SSTable + Memtable)?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;write to the in-memory sorted balanced tree (red-black tree), also called a memtable&lt;/li&gt;
  &lt;li&gt;when the memtable size is larger than a threshold, write to the disk as SSTable. When copying is ongoing, the newly coming write will go to a new memtable&lt;/li&gt;
  &lt;li&gt;to process a read request, first check the memtable, if does not find, check the latest SSTable and so on.&lt;/li&gt;
  &lt;li&gt;on the background, a process will periodically performing merging and compaction on the SSTables to delete data and remove duplicates&lt;/li&gt;
  &lt;li&gt;bloom filter is added as a first layer to check the existence of the key before it hits to the memtable and SSTables&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 11. What is a memtable?&lt;/p&gt;

&lt;p&gt;It is a in-memory cache of the latest set of record writes applied to the database. It is normally in a data structure (e.g. red-black tree) that supports log(n) time for insert and lookup.
The memtable will be write to disk as an SSTable after the size passes some threshold.&lt;/p&gt;

&lt;p&gt;💡 12. How does the LSM-tree storage engine improve the read efficiency for non-existing key?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use a bloom filter to check if key exists before searching for the keys in memtable and SSTables&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 13. How does a LSM-tree storage engine handle server crash?&lt;/p&gt;

&lt;p&gt;The server crash will cause the lost of memtable. In this case, the service will keep a separate append-only log file in disk to record each operation write to the memtable. This is only used to recover the memtable from crash.&lt;/p&gt;

&lt;p&gt;💡 14. How does a B-tree storage engine work?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;B-trees break the database down into fixed size blocks or pages (usually 4kb in size) and read or write one page at a time&lt;/li&gt;
  &lt;li&gt;Each page can be identified using an address or location, which allows one page to refer to another, we can use these page reference to construct a tree of pages&lt;/li&gt;
  &lt;li&gt;when performing read, start from the root of the b-tree and look for the child page the key resides until reaching the leaf page&lt;/li&gt;
  &lt;li&gt;when performing write, search for the leaf key, then caching the value in that page and writing it back to disk after update. If there isn’t enough space accommodate the new key, it creates a new page and updates the parent’s pointer to point to the new page&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 15. What is a heap file?&lt;/p&gt;

&lt;p&gt;When the db index stores the value’s identifier, the actual data of the value is stored in a place called heap file. Often it stores data in no particular order. It is common because it avoids duplicating data when multiple secondary indexes are present.&lt;/p&gt;

&lt;p&gt;💡 16. What is a clustered index?&lt;/p&gt;

&lt;p&gt;It is refer to the type of index that the value stored directly with the key in the index, instead of an identifier of the value stored. This will ensure a better performance than the later. One example is that MySQL InnoDB storage engine, the primary key of a table is always a clustered index, and the secondary indexes refer to the primary key (not using heap file)&lt;/p&gt;

&lt;p&gt;💡 17. Compare B-trees and LSM-trees?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;B-tree is used to support page-oriented storage engine (used in MySQL, MongoDB etc). It stores the key and value (location to the value) and reference to other pages as small chucked pages in a tree-structure to allow efficient access to the keys&lt;/li&gt;
  &lt;li&gt;LSM-trees is used to support log-structured storage engines (e.g. LevelDB). It consists of a in-memory memtable and in-disk SSTables. The memtable provides fast write and read and keeps the key in sorted order. The data in memtable is periodically loaded to SSTable for persistence. Merging and compacting is performed on SSTables to remove duplications&lt;/li&gt;
  &lt;li&gt;B-tree enabled data storage is better suitable for read-heavy applications while LSM-trees database is more suitable for write-heavy applications. B-tree is more mature compared to LSM-trees.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 18. Why some wide-column database e.g. BigTable, Cassandra, Hbase are not column-oriented database?&lt;/p&gt;

&lt;p&gt;These database are not column stores in the original sense of the term, since their two-level structures do not use a columnar data layout. They introduces the column-family concept. Within each column family, they store all columns from a row together, along with a row key, and they do not use column compression.&lt;/p&gt;

&lt;p&gt;💡 19. What are OLTP and OLAP? What are the main differences?&lt;/p&gt;

&lt;p&gt;Online transactional processing vs online analytical processing.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;read pattern: OLTP is for a small number of record per query, OLAP aggregate over large number of records&lt;/li&gt;
  &lt;li&gt;write pattern: OLTP is for random access with low latency, OLAP is for bulk import or event stream&lt;/li&gt;
  &lt;li&gt;end users: OLTP for web app and end users, OLAP for data analytics and decision maker&lt;/li&gt;
  &lt;li&gt;data representation: OLTP represents the latest state of the data, OLAP represents the historical events&lt;/li&gt;
  &lt;li&gt;data size: OLTP - GB to TB, OLAP - TB to PB&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;highlighted-topics&quot;&gt;Highlighted Topics&lt;/h3&gt;

&lt;p&gt;I find the discussion about the LSM-tree storage engine is really interesting and inspiring in this chapter. Here I use illustrations to describe how a simplest storage solution (a plain file) can be extend and evolved to an efficient and full-functional storage engine.&lt;/p&gt;

&lt;h4 id=&quot;simple-storage&quot;&gt;Simple Storage&lt;/h4&gt;

&lt;p&gt;A plain text file as a simple storage engine is illustrated below. The write operations (insert/update/delete) is always append to the end of the log file. The read operation is always start from the beginning and scan to the end. This results to a simple structure, extremely terrible read performance and unbeatable efficient write performance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-3/ddia-3-simple-storage.excalidraw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;hash-index-storage&quot;&gt;Hash Index Storage&lt;/h4&gt;

&lt;p&gt;To improve the read efficiency, the concept of hash index is introduced. It is a in-memory hash map to store the key and the offset to the log file where the value is appended. Use the offset information, one disk seek is required to fetch the value. Compaction and merging strategy is used to reduce the log file size resulting from the append-only write strategy&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-3/ddia-3-hash-index.excalidraw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;lsm-tree-storage&quot;&gt;LSM Tree storage&lt;/h4&gt;

&lt;p&gt;One of the problem of hash index storage engine is the hash index has to be fit into the memory. Memtable and SSTable are introduced in LSM-tree storage to solve the problem. Sorted key concept here is very important. It makes the searching and merging efficient with the sorted nature of the key. Bloom filter and write-ahead log are some common technics to improve the performance and reliability.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-3/ddia-3-lsm-tree.excalidraw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;extended-topics&quot;&gt;Extended Topics&lt;/h3&gt;

&lt;p&gt;B-tree and B*-tree storage engines&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;TODO; just want to publish this post sooner to motivate myself to continue to write&lt;/p&gt;
&lt;/blockquote&gt;

</description>
        <pubDate>Wed, 27 Jul 2022 10:00:00 +0800</pubDate>
        <link>http://yulu.github.io/blog1/2022/07/27/ddia-3.html</link>
        <guid isPermaLink="true">http://yulu.github.io/blog1/2022/07/27/ddia-3.html</guid>
        
        <category>reading_notes</category>
        
        <category>system_design</category>
        
        
        <category>blog1</category>
        
      </item>
    
      <item>
        <title>闪电女侠计划</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;2021年底体检之后意识到自己的体重已经超过了健康标准。从怀孕到哺乳，都没有控制饮食，不知不觉竟然涨了这么多，于是决定减肥。看了一圈，减脂基本要领：&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;控制饮食加运动制造热量差&lt;/li&gt;
    &lt;li&gt;减少碳水化合物和不健康脂肪的摄入促使身体消耗囤积的脂肪&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;于是开始了我的闪电女侠计划。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;tldr版本&quot;&gt;TL;DR版本&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;记录自己每天晨重&lt;/li&gt;
  &lt;li&gt;低碳饮食并计划记录自己每顿饭&lt;/li&gt;
  &lt;li&gt;买好看的运动服并坚持运动&lt;/li&gt;
  &lt;li&gt;每天大量喝水&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;减脂第一个月低碳和适度运动&quot;&gt;减脂第一个月——低碳和适度运动&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;初始体重：64.5kg
当前体重：60.1kg
目标体重：55.0kg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;一个月减重4.5公斤，体重掉得比较平缓。对于饮食和运动都做了详细的计划，不过都是在身体和心理可以接受的舒适范围内，没有极端节食或者过度运动。吃好喝好开心蹦蹦跳跳的瘦了4.5公斤。&lt;/p&gt;

&lt;h5 id=&quot;健康的饮食是第一步&quot;&gt;健康的饮食是第一步&lt;/h5&gt;

&lt;p&gt;了解了所谓减脂餐，其实就是WHO官方推荐的日常健康饮食。注意一下几个方面：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1) 减少糖(&amp;lt;50克)和钠(&amp;lt;5克)的摄入
2) 保证每天400克的蔬菜水果摄入(根茎类不算在内)
3) 控制饱和脂肪酸和避免反式脂肪酸的摄入，尽量摄入优质的非饱和脂肪酸
4) 用优质碳水替代精细米面
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;以上不管是否减肥，都是很好的健康饮食参考。在此基础上，通过减少碳水提高优质蛋白质摄入，完全避免糖的摄入，每顿7分饱，来到达健康减脂的效果。&lt;/p&gt;

&lt;h5 id=&quot;运动循序渐进&quot;&gt;运动循序渐进&lt;/h5&gt;

&lt;p&gt;对于没有运动习惯的人，从有氧运动开始建立起运动习惯比较好。慢跑，拉伸，有氧单车，健身操，我都穿插安排了一下。目前最喜欢的是晨跑，五点到六点出门跑一圈，一整天精力充沛。HIIT对于新手如我太猛了，做一次好几天恢复不过来。&lt;/p&gt;

&lt;h5 id=&quot;平常心对待体重波动&quot;&gt;平常心对待体重波动&lt;/h5&gt;

&lt;p&gt;减脂开始前两星期掉秤嗖嗖的。之后就开始各种瓶颈波动。开始有些焦虑，但是几次之后心态就调整过来了。明确一天之内的几斤的浮动都是水，时间线拉长点来看，才看得出变化的那点点肌肉或脂肪。碳水摄入较多或者外食重口味，身体很诚实马上就会反应，但是也很快在调饮食后恢复到正常曲线上。&lt;/p&gt;

&lt;h5 id=&quot;狂喝水&quot;&gt;狂喝水&lt;/h5&gt;

&lt;p&gt;我之前没有喝水的习惯。但是调整饮食结构后真的变得特别渴，在外面逛街都会突然渴到不行到处找水。&lt;/p&gt;

&lt;h5 id=&quot;和自己和解&quot;&gt;和自己和解&lt;/h5&gt;

&lt;p&gt;喜欢美食是人的天性，和自己和解，不要过度压抑自己。压力大时，我一定要吃点零食来缓解。蛋白棒和坚果作为零食压力就没有这么大。碳水于我也是无论如何也戒不掉，于是我满城的找好吃的酸面包，把我的碳水摄入quota消耗在最爱的食物上。外食也不慌张，控制量就好。可以这样想，家人朋友一起聚餐的好处是你可以尝很多不同美食，但是只吃一点点，吃不完剩下有别人帮你解决。&lt;/p&gt;

&lt;h3 id=&quot;减脂第二个月轻断食&quot;&gt;减脂第二个月——轻断食&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;初始体重：64.5kg
月初始体重：60.1kg
当前体重：57.6kg
目标体重：55.0kg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;减脂第二个月，轻断食三周左右，体重下降3kg左右，谈谈身体和心理反应。&lt;/p&gt;

&lt;p&gt;开始轻断食主要是两个原因：一是月初打了疫苗，两个星期内不建议剧烈运动，二是年底外出聚餐的安排多，很难保证低碳干净的饮食，所以打算少吃一餐来降低摄入。开始实践轻断食，我的进食窗口安排在早上7点到下午3点。早餐我安排得很丰盛，碳水纤维蛋白质都有。中午一半时间吃得比较健康，另外一半时间外食吃得较重口，但是总体保证碳水摄入在比较低的水平。&lt;/p&gt;

&lt;p&gt;第一周掉称特别明显，几乎是每天一斤。晚饭时间饥饿感比较强，但是因为之前低碳了一个多月，身体和心理能接受适度的饥饿感。而且睡眠特别的神奇，晚上明明很饿，一觉起来饥饿感就完全消失了。&lt;/p&gt;

&lt;p&gt;第二周开始就陷入了体重瓶颈。我感觉第一周的快速掉称应该是摄入明显减少的原因，因为我减少一顿饭后没有特别增加另外两顿的量。但是身体适应这个摄入量后调整消耗，热量差就不明显了。这让我心态开始不平衡，晚上还是要忍受饥饿感，但是体重不变化。&lt;/p&gt;

&lt;p&gt;到第三周，心态彻底崩溃。近年底，外食很频繁，晚上也不是很严格的断食了，因为妈妈做的饭菜太引起食欲了，我感觉多吃少吃反正体重也不变化，晚上总忍不住吃几口菜。吃得比较重口油腻，而且总外出喝水量也不够，虽然恢复运动了，但体重只增不减。&lt;/p&gt;

&lt;p&gt;三个星期后结束了轻断食计划。恢复三餐，早起规律运动。晚上喝一碗素菜豆腐汤，既抑制了食欲（妈妈的菜真是减肥路上最大的绊脚石），同时保证一定能量摄入支持第二天早上运动。恢复第三天体重就下降了一斤，并且维持了新低几天了，突破了这个小小的瓶颈。&lt;/p&gt;

&lt;p&gt;总结来说要我长期坚持轻断食很困难。我感觉进食窗口和摄入量需要更好的安排才能更持续的坚持，但是我没有找到一个自己比较适应的状态。不过经过这段时间的尝试，身体更好的适应了适度饥饿的状态。离目标体重还有两公斤，这个月的计划是坚持每天运动，保持均衡健康干净的饮食，偶尔放纵大餐。看一下保持正常健康的生活状态能不能减脂，也算是给自己的身体和食欲松弛一下。如果不达标，下个月继续加油。&lt;/p&gt;

&lt;h3 id=&quot;减脂第三个月健康饮食和持续运动&quot;&gt;减脂第三个月——健康饮食和持续运动&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;减肥初始体重: 64.5kg
月初始体重: 57.6kg
当前体重: 55.15kg
目标体重：55.0kg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;关于吃&quot;&gt;关于吃&lt;/h4&gt;

&lt;p&gt;减脂还是老生常谈的两个话题：饮食和运动。先聊聊这三个月我是怎么吃的。关于吃的一些Tips：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. 用坚果和无糖咖啡代替炸鸡和奶茶，健康的食物也可以吃得很开心
2. 外出大餐完全没有问题，只要控制频率。偶尔吃点不健康零食完全没有问题，只要控制量
3. 不需要做太复杂花哨的减脂餐。我在尝试了乱七八糟很多菜式以后，发现自己习惯的几样：烤三文鱼，腰果虾仁，手撕鸡胸肉，然后就循环着做了
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;下面是我的一日三餐菜单。我并不是严格遵守的，偶尔晚上妈妈做了好吃的菜我还是忍不住吃点，周末也和家人朋友外出吃大餐。还是上面说的，注意量和进食时间，没有什么不能吃。平衡食欲，摄入营养和体重的关系，才能持续健康的减脂。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;早餐&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- 酸面包加芝士开放三明治
- 香蕉燕麦蛋挞
- 黑巧克力全麦吐司布丁
- 橄榄油蔬菜坚果沙拉
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;午餐/晚餐&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;蛋白质

- 蒜香鸡胸肉
- 香煎三文鱼
- 腰果炒虾仁
- 蔬菜豆腐汤

碳水

- 蒸玉米，红薯，紫薯
- 鱼片芝士焗饭
- 鸡丝拌荞麦面
- 香菇燕麦粥
- 糙米饭

蔬菜

- 水煮青菜
- 蔬菜蛋花汤
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;饮料&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- 白水
- flat white
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;零食&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- 坚果
- 蛋白棒
- 草莓，蓝莓，柚子
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/diet-workout/diet-workout-3.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/diet-workout/diet-workout-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;关于运动&quot;&gt;关于运动&lt;/h4&gt;

&lt;p&gt;坚持运动三个月，也是坚持无间隔每天运动快两个月。我的一周运动安排：间隔一天晨跑40到50分钟，另一天在家跟视频做HIIT、有氧操或者瑜伽拉伸。这里记录一下让我坚持的几个Tips。&lt;/p&gt;

&lt;h5 id=&quot;选择合适的运动时间&quot;&gt;选择合适的运动时间&lt;/h5&gt;

&lt;p&gt;每天忙碌生活中似乎很难抽出一个小时的运动时间。开始运动习惯要感谢早起习惯的养成。一天中只有这个时间段，没有琐事缠身没有其他人打扰，可以全身心的投入到一件事情上。&lt;/p&gt;

&lt;p&gt;那到底一天中什么时间运动最佳呢？刚开始运动时，研究学习各种运动知识，关于一天中什么时候运动这个问题，我看到一个很好的答案：你能每天坚持下来的那个时间段，就是最好的时间。&lt;/p&gt;

&lt;h5 id=&quot;让开始运动变得非常方便&quot;&gt;让开始运动变得非常方便&lt;/h5&gt;

&lt;p&gt;我们总有这样的感觉：有些困难的事，总是拖着不愿开始，但是一旦真的开始了，反而觉得没有这么大阻力了，自然的进入“心流”，甚至不想停下来。运动肯定是这些“困难的事”之一。那我们要做的，就是尽量让开始做这件事变得非常容易。比如我会在前一天晚上把第二天要穿的运动服运动鞋包括袜子都准备好，手机或者ipad保证充满电，这样尽量排除阻碍开始做的各种困难。我听一个podcast host分享，她为了让第二天早上起床跑步变得更加容易，她就直接穿着运动服睡觉了。&lt;/p&gt;

&lt;p&gt;还有就是选择一个方便开始的运动。比如去健身房对于只能在清晨抽出时间运动的我就不可行，游泳我虽然很喜欢但是准备太繁琐。相比之下，跑步和瑜伽是最没有门槛的运动，从床上弹起来基本就可以马上开始了。&lt;/p&gt;

&lt;h5 id=&quot;让运动变得有所期待&quot;&gt;让运动变得有所期待&lt;/h5&gt;

&lt;p&gt;运动总是困难的事，难免让人心生抵触。我们可以增加一些有意思的事同时做或者稍候做，这样让我们对困难的事也能产生期待。准备晨跑的前一天，我会选好一些有趣的podcast或者有声书，这让前一晚我就对第二天的晨跑充满了期待。我还会每天做丰盛好吃的早饭，也是我早起运动的一大动力。&lt;/p&gt;

&lt;h5 id=&quot;坚持记录增加信心&quot;&gt;坚持记录增加信心&lt;/h5&gt;

&lt;p&gt;人的动机很大程度来自于对及时的正向反馈，但是运动能马上给人的可感知的正向反馈很少，我们的身体和身材积极变化都是在长时间的坚持运动中缓慢发生的。我需要找到方法去可视化这样的变化，把它们转换成增加我们行动力的积极信号。记录身体的数据，看到数据曲线的变化就是一个非常有效的方法。&lt;/p&gt;

&lt;h5 id=&quot;和自己和解-1&quot;&gt;和自己和解&lt;/h5&gt;

&lt;p&gt;人不是机器，不可能每天都按同样的设定运转。我们总会遇到身体或心理倦怠时候，或者一些计划之外的事件。不要太勉强自己了。当我觉得有点疲乏不想跑步，就做个简单的放松瑜伽。当清晨被不睡觉的娃困住没法出门时，就把运动时间调整到中午去健身房。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/diet-workout/diet-workout-2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Fri, 15 Jul 2022 10:00:00 +0800</pubDate>
        <link>http://yulu.github.io/blog2/2022/07/15/diet-workout.html</link>
        <guid isPermaLink="true">http://yulu.github.io/blog2/2022/07/15/diet-workout.html</guid>
        
        <category>生活</category>
        
        <category>美食</category>
        
        <category>运动</category>
        
        
        <category>blog2</category>
        
      </item>
    
      <item>
        <title>[DDIA] Chapter 2 - Data Models and Query Languages</title>
        <description>&lt;h3 id=&quot;chapter-summary&quot;&gt;Chapter Summary&lt;/h3&gt;

&lt;p&gt;This chapter briefly discussed the three types of data models: relational, document and graph database. Some histories about database are introduced to give us a better ideas why these databases are developed and what types of problems they solve.&lt;/p&gt;

&lt;p&gt;Personally I think this chapter is loosely structured and too much detail about the query languages is provided. Advice given to anyone who read this chapter: do not spend much time in understanding the query language syntax here, because the information given in the book will not help anyone to master the particular language (if you want to get into one query language, follow a well designed tutorial a specific book). Instead, you should try to understand the philosophy of each type of data model and identify the scenarios to apply them.&lt;/p&gt;

&lt;h3 id=&quot;mind-map&quot;&gt;Mind Map&lt;/h3&gt;

&lt;div class=&quot;mindmap-container&quot;&gt;
	&lt;iframe src=&quot;https://www.xmind.net/embed/sDETtj&quot; width=&quot;900px&quot; height=&quot;540px&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&quot;questions-summary&quot;&gt;Questions Summary&lt;/h3&gt;

&lt;p&gt;💡 1. What is ORM frameworks?&lt;/p&gt;

&lt;p&gt;Object-relational mapping framework, e.g. Hibernate&lt;/p&gt;

&lt;p&gt;💡 2. What is data locality?&lt;/p&gt;

&lt;p&gt;Data locality is the process of moving computation to the node where that data resides, rather than moving data to save bandwidth. This helps to minimize network congestion and improve computation throughput.&lt;/p&gt;

&lt;p&gt;In computer science, locality of reference, also known as the principle of locality, is the tendency of a processor to access the same set of memory locations repetitively over a short period of time.&lt;/p&gt;

&lt;p&gt;💡 3. What is RDBMS?&lt;/p&gt;

&lt;p&gt;Relational database management system&lt;/p&gt;

&lt;p&gt;💡 4. What are three types of ways to store one-to-many relationship in database?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;normalized representation of the data: to store the data to multiple tables and use foreign key reference to link the data from one source to multiple others&lt;/li&gt;
  &lt;li&gt;store the data in the predefined data structure that supported by the data store engine, e.g. json format supported by MySQL or PostgreSQL. The data is being able to be indexed and queried.&lt;/li&gt;
  &lt;li&gt;store the data as pure text field and allow the application to interpret the data. In this case, the data will not be able to be indexed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 5. What are the typical use cases of graph-like data models?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;social media&lt;/li&gt;
  &lt;li&gt;web graph&lt;/li&gt;
  &lt;li&gt;road or rail networks&lt;/li&gt;
  &lt;li&gt;recommendation engine&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 6. What are the difference between JPA and Hibernate and Spring Data JPA?&lt;/p&gt;

&lt;p&gt;JPA (Java persistence API, nothing to do with Spring) is an interface (specification).&lt;/p&gt;

&lt;p&gt;Hibernate is a JPA provider or the implementation.&lt;/p&gt;

&lt;p&gt;Spring Data JPA is either an interface nor implementation, it is an abstraction used to reduce the amount of boilerplate code required to implement data access layers. Spring Data JPA cannot work without a JPA provider.&lt;/p&gt;

&lt;p&gt;💡 7. What are schema-on-write and schema-on-read?&lt;/p&gt;

&lt;p&gt;Schema-on-write refers to the data systems that requires the explicit definition of data schema. When data is write to the database, it will be verified against the defined schema, any content that does not conforms to the schema will not be allowed to write.&lt;/p&gt;

&lt;p&gt;Schema-on-read refers to the data systems that does not require the explicit definition of data schema. The data schema will be interpreted from the data when it is read.&lt;/p&gt;

&lt;p&gt;💡 8. What are NoSQL databases? What are the different types of NoSQL databases?&lt;/p&gt;

&lt;p&gt;NoSQL database is a non-relational DBMS that does not require a fixed schema, avoid joins and easy to scale. There are four types of NoSQL database: Document databases (MongoDB), key-value stores (DynamoDB, Redis), wide-column database (Bigtable, Cassandra, Hbase) and Graph database (Neo4j).&lt;/p&gt;

&lt;p&gt;💡 9. In a property graph database model, what are the two data concepts? And what does each of them consists of?&lt;/p&gt;

&lt;p&gt;node (vertex) and relationship (edge)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;node consists of labels and properties&lt;/li&gt;
  &lt;li&gt;relationship consists of a source node, a target node, a type and properties&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 10. What does ployglot persistence mean?&lt;/p&gt;

&lt;p&gt;When design an application data storage, choose multiple types of storage based on the different use cases of the components of the application.&lt;/p&gt;

&lt;p&gt;💡 11. What is impedance mismatch?&lt;/p&gt;

&lt;p&gt;This is the term used to refer to the problems that object-oriented programming language’s data model has a mismatch of the SQL database model, that an awkward translation is required between them.&lt;/p&gt;

&lt;p&gt;ORM frameworks like Hibernate reduce the amount of boilerplate code for this translation layer.&lt;/p&gt;

&lt;h3 id=&quot;highlighted-topics&quot;&gt;Highlighted Topics&lt;/h3&gt;

&lt;p&gt;Property Graph model (Neo4j and Cypher) and Triple-store model (SPARQL) are two classic graph data models that is widely used. Here’s a brief summary and comparison of these two data model.&lt;/p&gt;

&lt;div class=&quot;image-banner-2&quot;&gt;
    &lt;img src=&quot;https://s3.ap-southeast-1.amazonaws.com/littlecheesecake.me/blog-post/ddia-2/graphDB.png&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;extended-topics&quot;&gt;Extended Topics&lt;/h3&gt;

&lt;p&gt;I had some hands on with &lt;a href=&quot;https://neo4j.com/developer/get-started/&quot;&gt;Neo4j beginner’s tutorials&lt;/a&gt;, however I felt little I can learn and master without a practical project to work on. Unfortunately I have not got a chance to use graph database in production level projects yet. It is just valuable additions to my toolbox for building applications in the future.&lt;/p&gt;
</description>
        <pubDate>Mon, 23 May 2022 10:00:00 +0800</pubDate>
        <link>http://yulu.github.io/blog1/2022/05/23/ddia-2.html</link>
        <guid isPermaLink="true">http://yulu.github.io/blog1/2022/05/23/ddia-2.html</guid>
        
        <category>reading_notes</category>
        
        <category>system_design</category>
        
        
        <category>blog1</category>
        
      </item>
    
  </channel>
</rss>
